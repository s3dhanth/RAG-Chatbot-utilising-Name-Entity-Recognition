{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import time\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone.exceptions import PineconeApiException\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to split by sections and subsections using regex\n",
    "def regex_split_documents(documents: list[Document]):\n",
    "    # Define regex pattern to capture both section and subsection headers\n",
    "    section_regex = r\"(\\d{1,2}\\s{1,}[A-Za-z]+)\"\n",
    "    \n",
    "    # Regex for subsections (e.g., 1.1, 2.2)\n",
    "    subsection_regex = r\"(\\d{1,2}\\.\\d{1,2})\"\n",
    "    \n",
    "    all_splits = []\n",
    "    all_documents = []\n",
    "    for document in documents:\n",
    "        text = document.page_content\n",
    "        metadata = document.metadata\n",
    "        # First, split by main sections (e.g., 1 Introduction)\n",
    "        sections = re.split(section_regex, text, flags=re.IGNORECASE)\n",
    "        \n",
    "        for section in sections:\n",
    "            # If section is too small, skip it\n",
    "            if len(section.strip()) < 100:\n",
    "                continue\n",
    "            \n",
    "            # Further split by subsections within each section (e.g., 1.1, 1.2)\n",
    "            subsections = re.split(subsection_regex, section, flags=re.IGNORECASE)\n",
    "            for subsection in subsections:\n",
    "                if len(subsection.strip()) < 100:\n",
    "                    continue\n",
    "                \n",
    "                # Split subsections into smaller chunks by characters if needed\n",
    "                text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=600,  # Adjust the chunk size\n",
    "                    chunk_overlap=60,\n",
    "                    length_function=len,\n",
    "                    is_separator_regex=False\n",
    "                )\n",
    "                \n",
    "                # Split the subsection text into smaller chunks\n",
    "                split_chunks = text_splitter.split_text(subsection)\n",
    "                for chunk in split_chunks:\n",
    "\n",
    "                    doc = Document(\n",
    "                        metadata=metadata,  # Keep the original metadata\n",
    "                        page_content=chunk   # Assign the chunked content\n",
    "                        )\n",
    "                all_documents.append(doc)\n",
    "    \n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Function to split by sections and subsections using regex\n",
    "def regex_split_documents(documents: list[Document]):\n",
    "    # Define regex patterns\n",
    "    # Section headers: e.g., 1 Introduction\n",
    "    section_regex = r\"(\\n?\\d{1,2}\\s{1,}[A-Za-z ]+)\"\n",
    "    # Subsection headers: e.g., 1.1 Introduction\n",
    "    subsection_regex = r\"(\\n?\\d{1,2}\\.\\d{1,2}\\s{1,}[A-Za-z ]+)\"\n",
    "\n",
    "    all_documents = []\n",
    "    \n",
    "    for document in documents:\n",
    "        text = document.page_content\n",
    "        metadata = document.metadata\n",
    "        \n",
    "        # Include the title and abstract by keeping the first paragraph intact\n",
    "        title_and_abstract = text.split(\"\\n\", 1)  # Split by the first newline\n",
    "        title = title_and_abstract[0].strip()     # Capture the title\n",
    "        abstract = title_and_abstract[1].strip() if len(title_and_abstract) > 1 else \"\"  # Capture the abstract\n",
    "        \n",
    "        # Create a Document for the title and abstract\n",
    "        all_documents.append(Document(\n",
    "            metadata=metadata,\n",
    "            page_content=title\n",
    "        ))\n",
    "        \n",
    "        if abstract:\n",
    "            all_documents.append(Document(\n",
    "                metadata=metadata,\n",
    "                page_content=abstract\n",
    "            ))\n",
    "        \n",
    "        # Now split the rest of the document by sections\n",
    "        sections = re.split(section_regex, text, flags=re.IGNORECASE)\n",
    "\n",
    "        for section in sections:\n",
    "            # Skip empty sections\n",
    "            if not section.strip():\n",
    "                continue\n",
    "            \n",
    "            # Further split by subsections within each section\n",
    "            subsections = re.split(subsection_regex, section, flags=re.IGNORECASE)\n",
    "            for subsection in subsections:\n",
    "                # Skip if subsection is too small\n",
    "                if len(subsection.strip()) < 100:\n",
    "                    continue\n",
    "                \n",
    "                # Split subsections into smaller chunks if needed\n",
    "                text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=600,  # Adjust the chunk size as necessary\n",
    "                    chunk_overlap=60,\n",
    "                    length_function=len,\n",
    "                    is_separator_regex=False\n",
    "                )\n",
    "                \n",
    "                # Split the subsection text into smaller chunks\n",
    "                split_chunks = text_splitter.split_text(subsection)\n",
    "                for chunk in split_chunks:\n",
    "                    doc = Document(\n",
    "                        metadata=metadata,  # Keep the original metadata\n",
    "                        page_content=chunk   # Assign the chunked content\n",
    "                    )\n",
    "                    all_documents.append(doc)\n",
    "    \n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r'C:\\QpiAi'\n",
    "\n",
    "# Load PDF documents\n",
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = regex_split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 0}, page_content='Multi-View and Multi-Scale Alignment for Contrastive'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 0}, page_content='Language-Image Pre-training in Mammography\\nYuexi Du1, John Onofrey1,2,3, Nicha C. Dvornek1,2\\n1Department of Biomedical Engineering,\\n2Department of Radiology & Biomedical Imaging,3Department of Urology,\\nYale University, New Haven, CT, USA\\nAbstract\\nContrastive Language-Image Pre-training (CLIP) shows promise in medical image\\nanalysis but requires substantial data and computational resources. Due to these\\nrestrictions, existing CLIP applications in medical imaging focus mainly on modal-\\nities like chest X-rays that have abundant image-report data available, leaving many\\nother important modalities under-explored. Here, we propose the first adaptation of\\nthe full CLIP model to mammography, which presents significant challenges due to\\nlabeled data scarcity, high-resolution images with small regions of interest, and data\\nimbalance. We first develop a specialized supervision framework for mammogra-\\nphy that leverages its multi-view nature. Furthermore, we design a symmetric local\\nalignment module to better focus on detailed features in high-resolution images.\\nLastly, we incorporate a parameter-efficient fine-tuning approach for large lan-\\nguage models pre-trained with medical knowledge to address data limitations. Our\\nmulti-view and multi-scale alignment (MaMA) method outperforms state-of-the-art\\nbaselines for three different tasks on two large real-world mammography datasets,\\nEMBED and RSNA-Mammo, with only 52% model size compared with the largest\\nbaseline. The code is available at https://github.com/XYPB/MaMA .1\\n1 Introduction\\nContrastive learning [ 5,17,16] has become one of the most popular self-supervised representation\\nlearning paradigms due to its intuitive concept and robust performance. Contrastive learning removes\\nthe reliance on a supervised signal by optimizing the semantic distance for similar pairs in the\\nrepresentation space in a contrastive manner. More recently, the introduction of natural language\\nsignals to contrastive learning [ 38] has given rise to modern visual-language models [ 26,25,29].\\nContrastive Language-Image Pre-training (CLIP) [ 38] has also been widely applied in the medical\\nimaging domain [ 53,19,51,56,54,55,15] and shows promising improvement in medical image\\nunderstanding when large-scale medical imaging datasets are available [ 22,20,15,55]. However, the\\nCLIP model in the natural image domain usually demands more than hundreds of millions of image-\\ntext pairs to be properly trained [ 38,44,46,45], which is almost impossible in the medical domain\\ndue to privacy and security concerns. Existing medical CLIP methods either build general-purpose\\nCLIP models with multiple anatomical sites and modalities from public online databases [ 15,55]\\nor focus on imaging modalities with large-scale (less than a million) datasets, e.g., chest X-ray or\\npathology images [ 56,19,51,54,53,60,52,50,24]. This means other imaging modalities, such as\\nmammography, have yet to fully benefit from such visual-language pre-trained models.\\nMammography is a critical medical imaging modality for breast cancer screening and diagnosis,\\nas breast cancer is one of the most commonly diagnosed cancers globally and a leading cause\\n1This work is also the basis of the overall best solution for the MICCAI 2024 CXR-LT Challenge.\\nPreprint. Under review.arXiv:2409.18119v1  [cs.CV]  26 Sep 2024'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 0}, page_content='Multi-View and Multi-Scale Alignment for Contrastive\\nLanguage-Image Pre-training in Mammography\\nYuexi Du1, John Onofrey1,2,3, Nicha C. Dvornek1,2\\n1Department of Biomedical Engineering,\\n2Department of Radiology & Biomedical Imaging,3Department of Urology,\\nYale University, New Haven, CT, USA\\nAbstract\\nContrastive Language-Image Pre-training (CLIP) shows promise in medical image\\nanalysis but requires substantial data and computational resources. Due to these\\nrestrictions, existing CLIP applications in medical imaging focus mainly on modal-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 0}, page_content='ities like chest X-rays that have abundant image-report data available, leaving many\\nother important modalities under-explored. Here, we propose the first adaptation of\\nthe full CLIP model to mammography, which presents significant challenges due to\\nlabeled data scarcity, high-resolution images with small regions of interest, and data\\nimbalance. We first develop a specialized supervision framework for mammogra-\\nphy that leverages its multi-view nature. Furthermore, we design a symmetric local\\nalignment module to better focus on detailed features in high-resolution images.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 0}, page_content='Lastly, we incorporate a parameter-efficient fine-tuning approach for large lan-\\nguage models pre-trained with medical knowledge to address data limitations. Our\\nmulti-view and multi-scale alignment (MaMA) method outperforms state-of-the-art\\nbaselines for three different tasks on two large real-world mammography datasets,\\nEMBED and RSNA-Mammo, with only 52% model size compared with the largest\\nbaseline. The code is available at https://github.com/XYPB/MaMA .1'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 0}, page_content='Contrastive learning [ 5,17,16] has become one of the most popular self-supervised representation\\nlearning paradigms due to its intuitive concept and robust performance. Contrastive learning removes\\nthe reliance on a supervised signal by optimizing the semantic distance for similar pairs in the\\nrepresentation space in a contrastive manner. More recently, the introduction of natural language\\nsignals to contrastive learning [ 38] has given rise to modern visual-language models [ 26,25,29].\\nContrastive Language-Image Pre-training (CLIP) [ 38] has also been widely applied in the medical'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 0}, page_content='imaging domain [ 53,19,51,56,54,55,15] and shows promising improvement in medical image\\nunderstanding when large-scale medical imaging datasets are available [ 22,20,15,55]. However, the\\nCLIP model in the natural image domain usually demands more than hundreds of millions of image-\\ntext pairs to be properly trained [ 38,44,46,45], which is almost impossible in the medical domain\\ndue to privacy and security concerns. Existing medical CLIP methods either build general-purpose\\nCLIP models with multiple anatomical sites and modalities from public online databases [ 15,55]'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 0}, page_content='or focus on imaging modalities with large-scale (less than a million) datasets, e.g., chest X-ray or\\npathology images [ 56,19,51,54,53,60,52,50,24]. This means other imaging modalities, such as\\nmammography, have yet to fully benefit from such visual-language pre-trained models.\\nMammography is a critical medical imaging modality for breast cancer screening and diagnosis,\\nas breast cancer is one of the most commonly diagnosed cancers globally and a leading cause\\n1This work is also the basis of the overall best solution for the MICCAI 20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='ℒ𝑉𝑇'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='Image𝑓𝑉\\nText𝑓𝑇𝑣 𝑡\\n𝒯1 𝒯2ℒ𝑉𝑇\\nImage Text𝑓𝑇 𝑓𝑉𝑣2 𝑣 𝑣1 𝑡ℒ𝑉𝑉\\nText𝑓𝑇𝑡\\n𝑓𝑉\\nMulti -view Images𝑣 \\u0de4𝑣ℒ𝑉𝑇 ℒ𝑉𝑉ℒ𝑉𝑇\\n𝒯1 𝒯2\\n(a) CLIP Style (b) SLIP (c) MaMA  (Ours)Figure 1: Comparison of Three Visual-Language Contrastive Learning Frameworks . (a)\\nCLIP [ 38] style; (b) SLIP [ 34] style; (c) Proposed MaMA that aligns image-image and image-text\\nfeatures, exploiting the multi-view nature of mammography and aligning images from the same study.\\nof cancer-related mortality in women [ 47]. While visual-language pre-training (VLP) has the\\npotential to improve mammography interpretation, there are two major obstacles: 1) Limited data\\nand annotation : Recent work has introduced a large-scale mammography image and tabular dataset\\nof more than 110,000 patients, i.e., EMBED [ 21], but no corresponding clinical reports are available.\\n2)Nature of mammography : Different from the single view natural image or chest X-ray, each\\nmammography study usually contains four high-resolution ( ∼2,000-by-2,000 pixels) views of the\\nsame patient: left and right side, each with craniocaudal (CC) and mediolateral oblique (MLO)\\nviews. Such multi-view mammography has the critical properties of bilateral asymmetry [12] and\\nipsilateral correspondence [32]. Bilateral asymmetry means images from different sides of the same\\npatient can contain different information, e.g., density, calcification, and mass findings. Ipsilateral\\ncorrespondence means different views of the same side share similar information from different\\nviewpoints. Clinicians consider both properties and all four images at once as a cross reference\\nwhen reading a study. Meanwhile, lesions of interest are often relatively small compared with\\nhigh-resolution mammograms, which further challenges a model’s ability to focus on local details.\\nThis pixel-level imbalance compounds the problem of image-level imbalance, in which the vast\\nmajority of mammograms will not contain cancer. While one recent work [ 6] attempts to address\\nthese issues by leveraging VLP, they take a fine-tuning approach in which they simply adopt a\\npre-trained CLIP model and perform supervised fine-tuning to a zero-shot classification task for\\nmulti-view mammography, rather than capitalizing on the mammography domain information to\\nperform contrastive language-image pre-training for mammography representation learning [58].\\nTo address these challenges, we propose a novel Multi-view and Multi-scale Alignment i.e., MaMA,\\ncontrastive language-image pre-training framework that exploits the multi-view property of mam-\\nmography and aligns multi-scale features simultaneously. We first propose an intuitive method for\\ntemplate-based report construction from tabular data to resolve the lack of clinical reports and enable\\nVLP. The proposed model then optimizes both multi-view image-image and symmetric text-image\\ncontrastive loss simultaneously (Fig. 1), learning the correspondence between the multi-view images\\nand image-report relationship from the same mammography study. We then propose a novel symmet-\\nric local alignment module that actively learns sentence-patch relationships by computing similarity\\nscores for each image-text pair. We also incorporate parameter-efficient fine-tuned large-language\\nmodels (LLMs) pre-trained with medical domain knowledge to improve the understanding of the\\nreport while addressing data scarcity. We validate our method on two large-scale mammography\\ndatasets, EMBED [ 21] and RNSA-Mammo [ 4], with multiple settings compared with state-of-the-art\\nmedical CLIP methods. The proposed method surpasses all the baselines with a considerable gap\\nwith only 52% model size, showing promise on multiple mammography-related tasks.\\n2 Related Works\\nMedical Visual-Language Pre-training Existing medical VLP methods can be divided into\\ntwo types depending on the training data. The first type is the general-purpose medical CLIP\\nmodel trained with a large-scale medical-image dataset with multiple anatomical sites and imaging\\nmodalities derived from PubMed [ 15,55]. This approach mainly focuses on scaling dataset size\\nwhile using a vanilla CLIP design [ 38]. These models show promising generalization ability on\\nmultiple sites but are often suboptimal compared with modality-specific models due to the lack of a\\nspecific design for the individual image modality. The other type of VLP models mainly focuses on\\n2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='ℒ𝑉𝑇\\nImage𝑓𝑉\\nText𝑓𝑇𝑣 𝑡\\n𝒯1 𝒯2ℒ𝑉𝑇\\nImage Text𝑓𝑇 𝑓𝑉𝑣2 𝑣 𝑣1 𝑡ℒ𝑉𝑉\\nText𝑓𝑇𝑡\\n𝑓𝑉\\nMulti -view Images𝑣 \\u0de4𝑣ℒ𝑉𝑇 ℒ𝑉𝑉ℒ𝑉𝑇\\n𝒯1 𝒯2\\n(a) CLIP Style (b) SLIP (c) MaMA  (Ours)Figure 1: Comparison of Three Visual-Language Contrastive Learning Frameworks . (a)\\nCLIP [ 38] style; (b) SLIP [ 34] style; (c) Proposed MaMA that aligns image-image and image-text\\nfeatures, exploiting the multi-view nature of mammography and aligning images from the same study.\\nof cancer-related mortality in women [ 47]. While visual-language pre-training (VLP) has the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='potential to improve mammography interpretation, there are two major obstacles: 1) Limited data\\nand annotation : Recent work has introduced a large-scale mammography image and tabular dataset\\nof more than 110,0'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content=', i.e., EMBED [ 21], but no corresponding clinical reports are available.\\n2)Nature of mammography : Different from the single view natural image or chest X-ray, each\\nmammography study usually contains four high-resolution ( ∼2,000-by-2,0'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content=') views of the\\nsame patient: left and right side, each with craniocaudal (CC) and mediolateral oblique (MLO)\\nviews. Such multi-view mammography has the critical properties of bilateral asymmetry [12] and\\nipsilateral correspondence [32]. Bilateral asymmetry means images from different sides of the same\\npatient can contain different information, e.g., density, calcification, and mass findings. Ipsilateral\\ncorrespondence means different views of the same side share similar information from different\\nviewpoints. Clinicians consider both properties and all four images at once as a cross reference'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='when reading a study. Meanwhile, lesions of interest are often relatively small compared with\\nhigh-resolution mammograms, which further challenges a model’s ability to focus on local details.\\nThis pixel-level imbalance compounds the problem of image-level imbalance, in which the vast\\nmajority of mammograms will not contain cancer. While one recent work [ 6] attempts to address\\nthese issues by leveraging VLP, they take a fine-tuning approach in which they simply adopt a\\npre-trained CLIP model and perform supervised fine-tuning to a zero-shot classification task for'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='multi-view mammography, rather than capitalizing on the mammography domain information to\\nperform contrastive language-image pre-training for mammography representation learning [58].\\nTo address these challenges, we propose a novel Multi-view and Multi-scale Alignment i.e., MaMA,\\ncontrastive language-image pre-training framework that exploits the multi-view property of mam-\\nmography and aligns multi-scale features simultaneously. We first propose an intuitive method for\\ntemplate-based report construction from tabular data to resolve the lack of clinical reports and enable'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='VLP. The proposed model then optimizes both multi-view image-image and symmetric text-image\\ncontrastive loss simultaneously (Fig. 1), learning the correspondence between the multi-view images\\nand image-report relationship from the same mammography study. We then propose a novel symmet-\\nric local alignment module that actively learns sentence-patch relationships by computing similarity\\nscores for each image-text pair. We also incorporate parameter-efficient fine-tuned large-language\\nmodels (LLMs) pre-trained with medical domain knowledge to improve the understanding of the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='report while addressing data scarcity. We validate our method on two large-scale mammography\\ndatasets, EMBED [ 21] and RNSA-Mammo [ 4], with multiple settings compared with state-of-the-art\\nmedical CLIP methods. The proposed method surpasses all the baselines with a considerable gap\\nwith only 52% model size, showing promise on multiple mammography-related tasks.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='Medical Visual-Language Pre-training Existing medical VLP methods can be divided into\\ntwo types depending on the training data. The first type is the general-purpose medical CLIP\\nmodel trained with a large-scale medical-image dataset with multiple anatomical sites and imaging\\nmodalities derived from PubMed [ 15,55]. This approach mainly focuses on scaling dataset size\\nwhile using a vanilla CLIP design [ 38]. These models show promising generalization ability on\\nmultiple sites but are often suboptimal compared with modality-specific models due to the lack of a'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='specific design for the individual image modality. The other type of VLP models mainly focuses on\\n2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='(b) Symmetric Local Alignment (SLA)#Patches'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='⋯\\n⋯\\n⋯Text \\nlocalization#Sentences⋯\\nVisual \\nlocalization\\nℒ\\u0bdf\\u0be2\\u0bd6\\u0bd4\\u0bdf=1\\n2(ℒ\\u0bdf\\u0be2\\u0bd6\\u0bd4\\u0bdf்+ℒ\\u0bdf\\u0be2\\u0bd6\\u0bd4\\u0bdf\\u0bcf)Avg.Avg.\\n(a) MaMA𝐶\\u0bdc,\\u0bdc\\n𝑐\\u0bdc,\\u0bdc்𝑐\\u0bdc,\\u0bdc\\u0bcf𝑝\\u0bdc𝑠\\u0bdc\\nText\\nEncoder\\n𝑓்Image \\nEncoder\\n𝑓\\u0bcf⋯ ⋯ ⋯ℒ\\u0bcf்ℒ\\u0bcf்ℒ\\u0bcf\\u0bcf\\nIntra-Study\\nRandom SamplingRandom\\nMeta Masking⋯\\n⋯SLA\\nStructured Mammo. \\nReport Constructor\\n‘[BOS]Procedure reported : MG Diagnostic Bilateral \\nw/ CAD [SEP] Reason for procedure: diagnostic \\n[SEP]Patient info : …[SEP] Image info : …[SEP]\\nBreast composition : …[SEP] Findings : …[SEP]\\nImpression : BI-RADS Category 5: highly suggestive \\nof malignancy [SEP] Overall Assessment : Highly \\nsuggestive of malignancy [SEP][EOS]’Study-level\\nTabular Annotation\\nMulti-view Images\\n𝑔\\u0bcf(⋅)\\n𝑔்(⋅)[SEP] Tokensℎ\\u0bcf(⋅)𝑔\\u0bcf(⋅)\\nℎ்(⋅)\\n⋯⋯\\nAvg.Avg.Avg.𝑣\\u0de4\\u0bdc\\n𝑣\\u0bdc\\n𝑡\\u0bdc𝑥\\u0bdc\\n𝑦\\u0bdc\\nData Augmentation𝑥\\u0de4\\u0bdcPairedFigure 2: Proposed Multi-view and Multi-scale (MaMA) VLP Framework . (a) We utilize\\nthe multi-view information of mammography to conduct symmetric image-image and image-text\\ncontrastive learning. (b) We localize the most relevant sentence for each image patch and the most\\nrelevant patch for each sentence and align these matched local features via symmetric local alignment.\\nchest X-ray [ 56,19,51,54,53,60,52,50] due to the availability of large datasets, trained on either\\nMIMIC-CXR [ 22] or CheXpert [ 20] datasets. While these methods show impressive performance\\non chest-specific tasks, they are specially designed for single-view images like regular CLIP [ 38].\\nSome of the methods further require full clinical reports paired with the image [ 51,50,60], which\\nmakes them harder to adopt. Recently, Chen et al. [6] proposed a first attempt to introduce CLIP to\\nmammography. It fine-tunes a pre-trained CLIP model with an added multi-view image aggregation\\nmodule to a zero-shot classification task. However, the method does not perform contrastive pre-\\ntraining, ignores pixel-level data imbalance, and cannot correlate the medical report with fine-grained\\nROIs. Furthermore, they only fine-tuned a pre-trained CLIP model with a few thousand private cases.\\nMulti-view Contrastive Learning To obtain a more robust self-supervised contrastive learning\\nframework, methods like SLIP [ 34] (Fig. 1 (b)) and DeCLIP[ 27] exploit image-image contrastive\\nlearning along with image-text contrastive learning simultaneously. Such ideas have been applied to\\n3D shape recognition [ 9,42] by exploiting the nature of 3D shapes from different viewpoints and also\\nto the action recognition task in the real world [ 40]. These methods all exploit the multi-view nature\\nof the specific image modality, where images of the same object from different viewpoints share the\\nsame semantic meaning while having different appearances. Multi-view contrastive learning has\\nalso been utilized in mammography [ 28,14,43], where the multi-view consistency is leveraged to\\nactively learn high-level shared information within the multi-view mammography. However, to the\\nbest of our knowledge, none of the existing works combine multi-view mammography contrastive\\nlearning with CLIP to fully utilize the supervising signal from the multimodal data.\\nUnsupervised Local Contrastive Learning Correlating a dense visual representation with fine-\\ngrained semantic meaning is not only helpful for image understanding but vital to tasks like semantic\\nsegmentation. Recent work address this problem in the challenging unsupervised scenario [ 19,51,\\n59,52,31,57,40,30]. Some methods rely on a pre-trained object detector or segmentation model to\\nextract the region of interest [ 57]. Other methods either aggregate dense similarity scores and conduct\\nimage-level contrastive learning [ 59,52,30], which may ignore too much visual information during\\ntraining, or exhaustively conduct token-level language-image matching and optimize patch-level\\ncontrastive loss [19, 51, 40], with the cost of additional computation.\\n3'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='(b) Symmetric Local Alignment (SLA)#Patches\\n⋯\\n⋯\\n⋯Text \\nlocalization#Sentences⋯\\nVisual \\nlocalization\\nℒ\\u0bdf\\u0be2\\u0bd6\\u0bd4\\u0bdf=1\\n2(ℒ\\u0bdf\\u0be2\\u0bd6\\u0bd4\\u0bdf்+ℒ\\u0bdf\\u0be2\\u0bd6\\u0bd4\\u0bdf\\u0bcf)Avg.Avg.\\n(a) MaMA𝐶\\u0bdc,\\u0bdc\\n𝑐\\u0bdc,\\u0bdc்𝑐\\u0bdc,\\u0bdc\\u0bcf𝑝\\u0bdc𝑠\\u0bdc\\nText\\nEncoder\\n𝑓்Image \\nEncoder\\n𝑓\\u0bcf⋯ ⋯ ⋯ℒ\\u0bcf்ℒ\\u0bcf்ℒ\\u0bcf\\u0bcf\\nIntra-Study\\nRandom SamplingRandom\\nMeta Masking⋯\\n⋯SLA\\nStructured Mammo. \\nReport Constructor\\n‘[BOS]Procedure reported : MG Diagnostic Bilateral \\nw/ CAD [SEP] Reason for procedure: diagnostic \\n[SEP]Patient info : …[SEP] Image info : …[SEP]\\nBreast composition : …[SEP] Findings : …[SEP]\\nImpression : BI-RADS Category 5: highly suggestive \\nof malignancy [SEP] Overall Assessment : Highly'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='of malignancy [SEP] Overall Assessment : Highly \\nsuggestive of malignancy [SEP][EOS]’Study-level\\nTabular Annotation\\nMulti-view Images\\n𝑔\\u0bcf(⋅)\\n𝑔்(⋅)[SEP] Tokensℎ\\u0bcf(⋅)𝑔\\u0bcf(⋅)\\nℎ்(⋅)\\n⋯⋯\\nAvg.Avg.Avg.𝑣\\u0de4\\u0bdc\\n𝑣\\u0bdc\\n𝑡\\u0bdc𝑥\\u0bdc\\n𝑦\\u0bdc\\nData Augmentation𝑥\\u0de4\\u0bdcPairedFigure 2: Proposed Multi-view and Multi-scale (MaMA) VLP Framework . (a) We utilize\\nthe multi-view information of mammography to conduct symmetric image-image and image-text\\ncontrastive learning. (b) We localize the most relevant sentence for each image patch and the most'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='relevant patch for each sentence and align these matched local features via symmetric local alignment.\\nchest X-ray [ 56,19,51,54,53,60,52,50] due to the availability of large datasets, trained on either\\nMIMIC-CXR [ 22] or CheXpert [ 20] datasets. While these methods show impressive performance\\non chest-specific tasks, they are specially designed for single-view images like regular CLIP [ 38].\\nSome of the methods further require full clinical reports paired with the image [ 51,50,60], which\\nmakes them harder to adopt. Recently, Chen et al. [6] proposed a first attempt to introduce CLIP to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='mammography. It fine-tunes a pre-trained CLIP model with an added multi-view image aggregation\\nmodule to a zero-shot classification task. However, the method does not perform contrastive pre-\\ntraining, ignores pixel-level data imbalance, and cannot correlate the medical report with fine-grained\\nROIs. Furthermore, they only fine-tuned a pre-trained CLIP model with a few thousand private cases.\\nMulti-view Contrastive Learning To obtain a more robust self-supervised contrastive learning\\nframework, methods like SLIP [ 34] (Fig. 1 (b)) and DeCLIP[ 27] exploit image-image contrastive'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='learning along with image-text contrastive learning simultaneously. Such ideas have been applied to\\n3D shape recognition [ 9,42] by exploiting the nature of 3D shapes from different viewpoints and also\\nto the action recognition task in the real world [ 40]. These methods all exploit the multi-view nature\\nof the specific image modality, where images of the same object from different viewpoints share the\\nsame semantic meaning while having different appearances. Multi-view contrastive learning has\\nalso been utilized in mammography [ 28,14,43], where the multi-view consistency is leveraged to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='actively learn high-level shared information within the multi-view mammography. However, to the\\nbest of our knowledge, none of the existing works combine multi-view mammography contrastive\\nlearning with CLIP to fully utilize the supervising signal from the multimodal data.\\nUnsupervised Local Contrastive Learning Correlating a dense visual representation with fine-\\ngrained semantic meaning is not only helpful for image understanding but vital to tasks like semantic\\nsegmentation. Recent work address this problem in the challenging unsupervised scenario [ 19,51,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='59,52,31,57,40,30]. Some methods rely on a pre-trained object detector or segmentation model to\\nextract the region of interest [ 57]. Other methods either aggregate dense similarity scores and conduct\\nimage-level contrastive learning [ 59,52,30], which may ignore too much visual information during\\ntraining, or exhaustively conduct token-level language-image matching and optimize patch-level\\ncontrastive loss [19, 51, 40], with the cost of additional computation.\\n3'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='3 Method'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='In this section, we introduce the proposed MaMA (Fig. 2). We begin with the construction of the\\nstructured mammography report from the tabular data. We then introduce the multi-view contrastive\\nimage-text pre-training framework, followed by the proposed symmetric local alignment (SLA).\\n3.1 Structured Report Construction\\nDifferent from chest X-ray datasets that provide paired images with corresponding clinical reports,\\ne.g., MIMIC-CXR [ 22], large-scale mammography datasets with the full report available are rare.\\nRather, existing datasets in this domain [ 21,4,35] mainly provide a tabular structure annotation\\nincluding both the anonymized meta information as well as the clinical findings, e.g., breast density\\ntype, calcification findings, tumor description, and Breast Imaging Reporting and Data System\\n(BI-RADS) assessment category [ 41]. Clinical findings serve as cross-validation evidence for the\\nfinal diagnosis. Using a CLIP-style [ 58] caption with only the simple class label for cancer will result\\nin a highly simplified caption and limit the model’s understanding of the image due to missing details.\\nWe propose a template-based caption construction method following the standard clinical report\\nstructure [ 36] (Fig. 2 (a)). We first create a report template with segments describing study procedure ,\\npatient meta-information ,image meta-information ,breast composition ,findings ,clinical impression\\nand the final overall assessment in a natural language report style. Each segment contains keywords\\nthat can be replaced with the corresponding meta-information in the tabular data. By replacing\\nthese keywords and concatenating these segments, we can build a complete clinical report for each\\nspecific image, and provide more details for language-image contrastive learning. We provide the full\\ntemplate and a few image-caption examples in the appendix.\\nMeta-Info Masking The increased information from patient and image-specific meta-data may\\nbe memorized by the model during the contrastive training and result in learning shortcuts for the\\nmodel decision. To focus more on the diagnosis and disease-related information, we propose a data\\naugmentation method that randomly masks each patient or image meta-information keyword with a\\nprobability of mwhen constructing the caption.\\n3.2 Multi-view VLP\\nWe introduce the multi-view contrastive VLP framework here. Let D={(xi, yi), i= 0,1, . . . , N }\\nbe a multimodal dataset, where there are Nindividual images xiand corresponding text captions yi.\\nOur framework optimizes both image-to-image and symmetric image-to-text contrastive loss.\\nMulti-view Visual Contrastive Loss We first optimize the contrastive loss within the multi-view\\nimages (Fig. 2 (a)). We define a study to include the data from the same imaging session for a patient,\\nincluding one or more image-text pairs. For a random image-text pair (xi, yi)from the dataset D, we\\nuniformly sample another image ˜xifrom the same study that xibelongs to as the positive sample\\nofxi. Note that ˜xicould be xias the augmented view of the same image is naturally a positive\\nsample. We augment both images with random data augmentation and then feed into the vision\\nencoder fVandd-dimensional global embedding projection head gVfollowed by average pooling to\\nget corresponding visual embedding vi,˜vi∈Rd,i.e.,vi= avg( gV(fV(xi))). We then compute the\\ncosine similarity for each pair of visual embeddings and optimize the InfoNCE [ 5] loss for viin a\\nmini-batch of size B:\\nLV V(vi,˜vi) = logexp(sim(vi,˜vi)/τ1)PB\\nj=1exp(sim(vi, vj)/τ1),where sim(vi, vj) =vT\\nivj\\n∥vi∥∥vj∥, (1)\\nwhere τ1is the visual temperature constant and vjis the j-th visual embedding in the batch. Since\\ntwo views of the same side of a study have ipsilateral correspondence, it is natural to treat them as\\npositive samples of each other, as the features, like tumors, present in one view, are often present\\nin the other view as well. On the other hand, even if considering bilateral asymmetry for images\\nfrom different sides, they still share much high-level information such as patient-level features (e.g.,\\nglobal breast shape similarity, age) and similar breast density. Introducing multi-view mammography\\ncontrastive learning forces the model to learn semantically similar features from images within the\\nsame study. This also provides a stronger self-supervised signal than using random augmented images.\\nOur image-to-image contrastive learning framework follows the design of SimCLR [ 5] for simplicity.\\n4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='In this section, we introduce the proposed MaMA (Fig. 2). We begin with the construction of the\\nstructured mammography report from the tabular data. We then introduce the multi-view contrastive\\nimage-text pre-training framework, followed by the proposed symmetric local alignment (SLA).\\n3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='Different from chest X-ray datasets that provide paired images with corresponding clinical reports,\\ne.g., MIMIC-CXR [ 22], large-scale mammography datasets with the full report available are rare.\\nRather, existing datasets in this domain [ 21,4,35] mainly provide a tabular structure annotation\\nincluding both the anonymized meta information as well as the clinical findings, e.g., breast density\\ntype, calcification findings, tumor description, and Breast Imaging Reporting and Data System\\n(BI-RADS) assessment category [ 41]. Clinical findings serve as cross-validation evidence for the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='final diagnosis. Using a CLIP-style [ 58] caption with only the simple class label for cancer will result\\nin a highly simplified caption and limit the model’s understanding of the image due to missing details.\\nWe propose a template-based caption construction method following the standard clinical report\\nstructure [ 36] (Fig. 2 (a)). We first create a report template with segments describing study procedure ,\\npatient meta-information ,image meta-information ,breast composition ,findings ,clinical impression'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='and the final overall assessment in a natural language report style. Each segment contains keywords\\nthat can be replaced with the corresponding meta-information in the tabular data. By replacing\\nthese keywords and concatenating these segments, we can build a complete clinical report for each\\nspecific image, and provide more details for language-image contrastive learning. We provide the full\\ntemplate and a few image-caption examples in the appendix.\\nMeta-Info Masking The increased information from patient and image-specific meta-data may'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='be memorized by the model during the contrastive training and result in learning shortcuts for the\\nmodel decision. To focus more on the diagnosis and disease-related information, we propose a data\\naugmentation method that randomly masks each patient or image meta-information keyword with a\\nprobability of mwhen constructing the caption.\\n3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='-view VLP\\nWe introduce the multi-view contrastive VLP framework here. Let D={(xi, yi), i= 0,1, . . . , N }\\nbe a multimodal dataset, where there are Nindividual images xiand corresponding text captions yi.\\nOur framework optimizes both image-to-image and symmetric image-to-text contrastive loss.\\nMulti-view Visual Contrastive Loss We first optimize the contrastive loss within the multi-view\\nimages (Fig. 2 (a)). We define a study to include the data from the same imaging session for a patient,\\nincluding one or more image-text pairs. For a random image-text pair (xi, yi)from the dataset D, we'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='uniformly sample another image ˜xifrom the same study that xibelongs to as the positive sample\\nofxi. Note that ˜xicould be xias the augmented view of the same image is naturally a positive\\nsample. We augment both images with random data augmentation and then feed into the vision\\nencoder fVandd-dimensional global embedding projection head gVfollowed by average pooling to\\nget corresponding visual embedding vi,˜vi∈Rd,i.e.,vi= avg( gV(fV(xi))). We then compute the\\ncosine similarity for each pair of visual embeddings and optimize the InfoNCE [ 5] loss for viin a\\nmini-batch of size B:'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='mini-batch of size B:\\nLV V(vi,˜vi) = logexp(sim(vi,˜vi)/τ1)PB\\nj=1exp(sim(vi, vj)/τ1),where sim(vi, vj) =vT\\nivj\\n∥vi∥∥vj∥, (1)\\nwhere τ1is the visual temperature constant and vjis the j-th visual embedding in the batch. Since\\ntwo views of the same side of a study have ipsilateral correspondence, it is natural to treat them as\\npositive samples of each other, as the features, like tumors, present in one view, are often present\\nin the other view as well. On the other hand, even if considering bilateral asymmetry for images'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='from different sides, they still share much high-level information such as patient-level features (e.g.,\\nglobal breast shape similarity, age) and similar breast density. Introducing multi-view mammography\\ncontrastive learning forces the model to learn semantically similar features from images within the\\nsame study. This also provides a stronger self-supervised signal than using random augmented images.\\nOur image-to-image contrastive learning framework follows the design of SimCLR [ 5] for simplicity.\\n4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='Symmetric Visual-Text Contrastive Loss While existing methods like SLIP [ 34] also optimize'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='both image-image and image-text contrastive loss, we note there is a potential contradiction between\\nimage-image and image-text objectives when computed for different examples (Fig. 1 (b)), i.e.,LV V\\nandLV Tare independent and the extra image will introduce unnecessary memory cost. To address\\nthis, we propose re-using viwhen optimizing LV Tand symmetrically optimizing this loss.\\nWe feed caption yito the tokenizer and text encoder fTand then the text global projection head gT\\nwith average pooling to get the text embedding ti∈Rd. We optimize the CLIP [ 38] loss (Fig. 2 (a)):\\nLV T(vi, ti) =−1\\n2(logexp(sim(vi, ti)/τ2)PB\\nj=1exp(sim(vi, tj)/τ2)+ logexp(sim(ti, vi)/τ2)PB\\nj=1exp(sim(ti, vj)/τ2)),(2)\\nwhere τ2is the learnable language temperature constant. We compute LV Tfor both viand˜vifor the\\nsame tisymmetrically. Namely, we minimize the semantic distance between two images from the\\nsame view and the corresponding report simultaneously. We note that even if the information in yiis\\nnot completely matched with ˜xi,e.g., different side and view caption, they still share a large overlap\\nin patient-level information. This encourages the model to mine the high-level shared patient-related\\nfeatures via minimizing LV T(˜vi, ti)while focusing on diagnosis-related information by minimizing\\nLV T(vi, ti).\\n3.3 Symmetric Local Alignment (SLA)\\nMammography usually contains high-frequency details and the region of interest is usually very small.\\nThese properties require a higher image resolution for the deep learning method to work properly. It\\nalso challenges the model’s ability to extract important local information and filter out less meaningful\\nbackground and tissue unrelated to diagnosis. To address these challenges, we propose a symmetric\\nlocal alignment (SLA) module. Specifically, the SLA module allows the model to determine the local\\ncorrespondence relationship between each sentence and image patch (Fig. 2 (b)).\\nWe start with extracting local features from input (xi, yi). We feed the image and caption to the\\nvision encoder fVand text encoder fTrespectively, followed by corresponding local projection head\\nhVandhTwithout pooling to produce output feature sequence vlocal\\ni∈RNV×dandtlocal\\ni∈RNT×d,\\nwhere NVandNTare the length of visual tokens and text tokens, respectively. We then extract\\nsentence-level features by selecting the embedding corresponding to the [SEP] token, which results\\nin a sequence of sentence embeddings si∈RS×d, where Sis the number of sentences. We extract\\nthe image patch-level features by removing the extra functional tokens like [CLS] tokens, resulting\\nin a sequence of patch embeddings pi∈RP×d, where Pis the number of patches. We then compute\\nthe sentence-patch correspondence matrix Ci,i∈RS×Pin the form of cosine similarity, which\\nreveals the relationship between local patches and each sentence in the report. However, we cannot\\ndirectly supervise the learning of this matrix since we have no access to the local correspondence\\nbetween the image and the report. Thus, we aggregate the patch-sentence level correspondence matrix\\nCi,ito an image-report level similarity score. We start by localizing the patch that has the highest\\ncorrespondence for each sentence. Namely, we find the most relevant region in the image for each\\nsentence. We call this process Visual Localization. We then average the similarity score for each\\nsentence to obtain a correspondence score which describes the similarity of the most relevant patch\\nfor the whole report cV\\ni,i=1\\nSP\\njmaxkCi,i(j, k), where Ci,i(j, k)is the similarity between the j-th\\nsentence and the k-th patch. Similarly, we conduct Text Localization by finding the most similar\\nsentence feature for each patch and averaging it to get a score for the similarity of the most relevant\\nsentence for the whole image cT\\ni,i=1\\nPP\\nkmaxjCi,i(j, k). We compute the aggregated visual and\\ntext localization scores for all pandsin the mini-batch and optimize the InfoNCE [17] loss:\\nsLV\\nlocal(i) =−1\\n2(exp(cV\\ni,i/τlocal)\\nPB\\nj=1exp(cV\\ni,j/τlocal)+exp(cV\\ni,i/τlocal)\\nPB\\nj=1exp(cV\\nj,i/τlocal)), (3)\\nandLT\\nlocal is defined similarly, where τlocal is the local temperature constant. The final local loss\\nwill then be Llocal=1\\n2(LV\\nlocal+LT\\nlocal). We note that introducing this local loss from the beginning\\nof the training can lead to unstable behavior as the initial visual and language embeddings are not\\naligned. Thus, we add this loss after ksteps of training.\\nThe intuition behind this design is to mimic the process of radiologic interpretation of a medical\\nimage in the real world. On the one hand, in mammography, the clinician will look for the image\\n5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='Symmetric Visual-Text Contrastive Loss While existing methods like SLIP [ 34] also optimize\\nboth image-image and image-text contrastive loss, we note there is a potential contradiction between\\nimage-image and image-text objectives when computed for different examples (Fig. 1 (b)), i.e.,LV V\\nandLV Tare independent and the extra image will introduce unnecessary memory cost. To address\\nthis, we propose re-using viwhen optimizing LV Tand symmetrically optimizing this loss.\\nWe feed caption yito the tokenizer and text encoder fTand then the text global projection head gT'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='with average pooling to get the text embedding ti∈Rd. We optimize the CLIP [ 38] loss (Fig. 2 (a)):\\nLV T(vi, ti) =−1\\n2(logexp(sim(vi, ti)/τ2)PB\\nj=1exp(sim(vi, tj)/τ2)+ logexp(sim(ti, vi)/τ2)PB\\nj=1exp(sim(ti, vj)/τ2)),(2)\\nwhere τ2is the learnable language temperature constant. We compute LV Tfor both viand˜vifor the\\nsame tisymmetrically. Namely, we minimize the semantic distance between two images from the\\nsame view and the corresponding report simultaneously. We note that even if the information in yiis'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='not completely matched with ˜xi,e.g., different side and view caption, they still share a large overlap\\nin patient-level information. This encourages the model to mine the high-level shared patient-related\\nfeatures via minimizing LV T(˜vi, ti)while focusing on diagnosis-related information by minimizing\\nLV T(vi, ti).\\n3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='(SLA)\\nMammography usually contains high-frequency details and the region of interest is usually very small.\\nThese properties require a higher image resolution for the deep learning method to work properly. It\\nalso challenges the model’s ability to extract important local information and filter out less meaningful\\nbackground and tissue unrelated to diagnosis. To address these challenges, we propose a symmetric\\nlocal alignment (SLA) module. Specifically, the SLA module allows the model to determine the local\\ncorrespondence relationship between each sentence and image patch (Fig. 2 (b)).'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='We start with extracting local features from input (xi, yi). We feed the image and caption to the\\nvision encoder fVand text encoder fTrespectively, followed by corresponding local projection head\\nhVandhTwithout pooling to produce output feature sequence vlocal\\ni∈RNV×dandtlocal\\ni∈RNT×d,\\nwhere NVandNTare the length of visual tokens and text tokens, respectively. We then extract\\nsentence-level features by selecting the embedding corresponding to the [SEP] token, which results\\nin a sequence of sentence embeddings si∈RS×d, where Sis the number of sentences. We extract'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='the image patch-level features by removing the extra functional tokens like [CLS] tokens, resulting\\nin a sequence of patch embeddings pi∈RP×d, where Pis the number of patches. We then compute\\nthe sentence-patch correspondence matrix Ci,i∈RS×Pin the form of cosine similarity, which\\nreveals the relationship between local patches and each sentence in the report. However, we cannot\\ndirectly supervise the learning of this matrix since we have no access to the local correspondence\\nbetween the image and the report. Thus, we aggregate the patch-sentence level correspondence matrix'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='Ci,ito an image-report level similarity score. We start by localizing the patch that has the highest\\ncorrespondence for each sentence. Namely, we find the most relevant region in the image for each\\nsentence. We call this process Visual Localization. We then average the similarity score for each\\nsentence to obtain a correspondence score which describes the similarity of the most relevant patch\\nfor the whole report cV\\ni,i='),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='jmaxkCi,i(j, k), where Ci,i(j, k)is the similarity between the j-th\\nsentence and the k-th patch. Similarly, we conduct Text Localization by finding the most similar\\nsentence feature for each patch and averaging it to get a score for the similarity of the most relevant\\nsentence for the whole image cT\\ni,i='),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='kmaxjCi,i(j, k). We compute the aggregated visual and\\ntext localization scores for all pandsin the mini-batch and optimize the InfoNCE [17] loss:\\nsLV\\nlocal(i) =−1\\n2(exp(cV\\ni,i/τlocal)\\nPB\\nj=1exp(cV\\ni,j/τlocal)+exp(cV\\ni,i/τlocal)\\nPB\\nj=1exp(cV\\nj,i/τlocal)), (3)\\nandLT\\nlocal is defined similarly, where τlocal is the local temperature constant. The final local loss\\nwill then be Llocal=1\\n2(LV\\nlocal+LT\\nlocal). We note that introducing this local loss from the beginning\\nof the training can lead to unstable behavior as the initial visual and language embeddings are not'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='aligned. Thus, we add this loss after ksteps of training.\\nThe intuition behind this design is to mimic the process of radiologic interpretation of a medical\\nimage in the real world. On the one hand, in mammography, the clinician will look for the image\\n5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='Table 1: Linear Classification Results on EMBED [ 21]We evaluate linear classification results'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='with different amounts of fine-tuning data for both BI-RADS and density prediction tasks. We report\\nboth balanced accuracy (bACC) and AUC metrics. The best and second-best results are highlighted\\nin bold and underlined, respectively. Our method is shaded in gray.\\nModelsEMBED BI-RADS [21] EMBED Density [21]\\nbACC (%) AUC (%) bACC (%) AUC (%)\\n1% 10% 100% 1% 10% 100% 1% 10% 100% 1% 10% 100%\\nVision only\\nRandom-ViT [13] 20.84 20.68 22.10 57.15 61.54 61.76 45.81 45.11 47.01 72.83 72.62 72.92\\nDiNOv2-ViT [37] 22.63 25.17 29.33 61.83 66.00 70.11 66.71 70.80 71.20 89.18 90.46 90.47\\nDeiT-based [48]\\nCLIP [38] 19.33 21.97 22.26 55.52 61.02 61.65 48.95 50.33 50.77 75.41 76.31 76.92\\nConVIRT [56] 25.08 27.63 29.56 65.43 70.49 71.54 72.66 73.46 73.53 91.69 92.11 92.10\\nMGCA [51] 24.17 27.28 28.09 65.18 71.08 71.49 74.03 74.49 74.53 91.80 92.25 92.21\\nDiNOv2-based [37]\\nCLIP [38] 26.66 31.65 34.35 70.35 74.98 74.11 74.64 75.00 75.97 91.50 90.62 92.39\\nSLIP [34] 22.94 27.86 30.93 64.43 69.48 71.95 73.24 74.79 75.23 91.56 92.37 92.46\\nMM-MIL [52] 25.85 30.94 35.11 67.16 71.99 76.12 74.23 76.69 75.77 91.96 93.34 91.65\\nConVIRT [56] 24.62 30.38 31.27 65.09 73.33 74.03 74.34 74.95 74.74 92.21 92.56 92.58\\nMGCA [51] 23.66 30.11 30.27 64.19 72.24 72.54 71.43 72.25 72.20 90.83 91.21 91.24\\nMaMA 28.46 35.12 39.75 70.63 75.98 77.50 76.26 78.11 78.09 93.11 93.62 93.65\\nregions and local features that appear most suspicious for cancer. On the other hand, the clinician\\nwill write the radiology report in a few sentences based on the findings across the whole image, while\\nmatching each description with a specific feature of the image. Our proposed SLA gives the model\\nthe ability to perceive fine-grain local image detail with sentence-level description. The derived local\\nsimilarity map could also be used as a guide of the relevance between specific image details and each\\nsentence in the provided report and therefore improve the interpretability of the model.\\n3.4 LLM with PEFT as Text Encoder\\nLastly, we incorporate parameter-efficient fine-tuning (PEFT) of a pre-trained large language model\\n(LLM) as our text encoder ( e.g., BioMedLM [ 3]) rather than use a small pre-trained BERT encoder [ 2].\\nUsing a pre-trained LLM with strong domain knowledge can help improve the model’s understanding\\nof the text caption and provide a more robust supervised signal for the visual-language pre-training.\\nMoreover, PEFT ( e.g., LoRA [18]) can greatly reduce the cost of adapting LLM to scenarios with a\\nshortage of computing resources while maintaining a strong performance after fine-tuning. Adapting\\nan LLM with PEFT thus has the potential to greatly improve performance while reducing trainable\\nparameters and GPU memory costs compared to learning the commonly adopted BERT-style encoder.\\n4 Experiments\\n4.1 Pre-training Settings\\nDataset We pre-trained our model on the Emory EMBED [21] dataset, which is a large-scale\\nmammography dataset with public access and one of the largest open mammography datasets\\navailable. The current release contains 72,768 multi-view mammography studies for 23,356 patients\\ncollected from 4 hospitals. We focus on 2D mammography, which has 364,564 individual images\\nin total. The dataset provides tabular annotation about the patient, imaging meta-information, and\\ncorresponding image-level findings including breast density, BI-RADS assessment, and calcification\\nfindings. We split the dataset by patient into train/validation/test partitions, each with 70%/10%/20%\\nimages. All the images are resized and padded to 518×518without changing the aspect ratio.\\nImplementation Details We choose to use DiNOv2-ViT-B-14 [ 37] and BioMedLM [ 3] as our\\nimage and text encoder respectively. We adapt LoRA [ 18] to the text encoder to fine-tune it efficiently.\\nWe choose DiNOv2 [ 37] ViT as it is pre-trained with a larger image size which is suitable for\\nmammography. Note that our method does not depend on a specific text encoder design. We also\\nreport the performance of our model with a more common BioClincialBERT [ 2] encoder. The meta\\nmasking ratio mis 0.8 during training. We train our model with the AdamW optimizer [ 33] using a\\nlearning rate of 4E-5, weight-decay of 0.1, and cosine annealing scheduler for 40k steps. We also\\nadapt warm-up from 1E-8 for 4k steps. The SLA loss is added after k= 8k steps. We use a batch\\n6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='Table 1: Linear Classification Results on EMBED [ 21]We evaluate linear classification results\\nwith different amounts of fine-tuning data for both BI-RADS and density prediction tasks. We report\\nboth balanced accuracy (bACC) and AUC metrics. The best and second-best results are highlighted\\nin bold and underlined, respectively. Our method is shaded in gray.\\nModelsEMBED BI-RADS [21] EMBED Density [21]\\nbACC (%) AUC (%) bACC (%) AUC (%)\\n1% 10% 100% 1% 10% 100% 1% 10% 100% 1% 10% 100%\\nVision only\\nRandom-ViT [13] 20.84 20.68 22.10 57.15 61.54 61.76 45.81 45.11 47.01 72.83 72.62 72.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='. On the other hand, the clinician\\nwill write the radiology report in a few sentences based on the findings across the whole image, while\\nmatching each description with a specific feature of the image. Our proposed SLA gives the model\\nthe ability to perceive fine-grain local image detail with sentence-level description. The derived local\\nsimilarity map could also be used as a guide of the relevance between specific image details and each\\nsentence in the provided report and therefore improve the interpretability of the model.\\n3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='Lastly, we incorporate parameter-efficient fine-tuning (PEFT) of a pre-trained large language model\\n(LLM) as our text encoder ( e.g., BioMedLM [ 3]) rather than use a small pre-trained BERT encoder [ 2].\\nUsing a pre-trained LLM with strong domain knowledge can help improve the model’s understanding\\nof the text caption and provide a more robust supervised signal for the visual-language pre-training.\\nMoreover, PEFT ( e.g., LoRA [18]) can greatly reduce the cost of adapting LLM to scenarios with a\\nshortage of computing resources while maintaining a strong performance after fine-tuning. Adapting'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='an LLM with PEFT thus has the potential to greatly improve performance while reducing trainable\\nparameters and GPU memory costs compared to learning the commonly adopted BERT-style encoder.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='-training Settings\\nDataset We pre-trained our model on the Emory EMBED [21] dataset, which is a large-scale\\nmammography dataset with public access and one of the largest open mammography datasets\\navailable. The current release contains 72,7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='in total. The dataset provides tabular annotation about the patient, imaging meta-information, and\\ncorresponding image-level findings including breast density, BI-RADS assessment, and calcification\\nfindings. We split the dataset by patient into train/validation/test partitions, each with 70%/10%/20%\\nimages. All the images are resized and padded to 518×518without changing the aspect ratio.\\nImplementation Details We choose to use DiNOv2-ViT-B-14 [ 37] and BioMedLM [ 3] as our\\nimage and text encoder respectively. We adapt LoRA [ 18] to the text encoder to fine-tune it efficiently.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='We choose DiNOv2 [ 37] ViT as it is pre-trained with a larger image size which is suitable for\\nmammography. Note that our method does not depend on a specific text encoder design. We also\\nreport the performance of our model with a more common BioClincialBERT [ 2] encoder. The meta\\nmasking ratio mis 0.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='. We train our model with the AdamW optimizer [ 33] using a\\nlearning rate of 4E-5, weight-decay of 0.1, and cosine annealing scheduler for 40k steps. We also\\nadapt warm-up from 1E-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='Table 2: Zero-shot and Full Fine-tuning Results on EMBED [ 21]We evaluate zero-shot and fully'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='fine-tuned classification results for both BI-RADS and density prediction tasks. We report balanced\\naccuracy (bACC) and AUC. The best and second-best results are highlighted in bold and underlined,\\nrespectively. Our method is shaded in gray.\\nModelsEMBED BI-RADS [21] EMBED Density [21]\\nZero-shot Full Fine-tune Zero-shot Full Fine-tune\\nbACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%)\\nDeiT-based [48]\\nCLIP [38] 23.86 67.08 25.05 63.43 71.72 91.52 71.90 89.74\\nConVIRT [56] 23.72 62.85 31.80 72.82 64.61 86.62 77.07 93.34\\nMGCA [51] 22.73 62.24 33.05 74.20 68.47 87.86 77.29 93.47\\nDiNOv2-based\\nCLIP [38] 23.05 59.81 34.25 71.61 73.56 92.37 77.47 93.69\\nSLIP [34] 24.14 67.47 21.75 61.96 75.45 92.17 64.72 86.37\\nMM-MIL [52] 21.78 62.41 33.05 71.26 69.73 89.07 75.92 92.59\\nConVIRT [56] 25.27 65.13 34.54 74.05 64.85 87.66 77.93 93.60\\nMGCA [51] 26.55 63.76 34.15 73.89 69.00 88.36 77.74 93.64\\nMaMA 31.04 74.83 40.31 77.36 75.40 93.46 78.02 93.65\\nsize of 144 and train the model on 4 RTX A5000 GPUs with BFloat-16 precision. We set d= 512\\nandτ1=τ2=τlocal= 0.07. We provide more details for hyper-parameters in the appendix.\\n4.2 Downstream Evaluation Settings\\nTasks and Datasets We primarily evaluate our method on the EMBED [21] dataset for both\\nBI-RADS assessment category (7 classes) and breast density (4 classes) prediction tasks. Note that\\nthe real-world distribution of labels for both tasks is extremely imbalanced. To demonstrate the\\nbehavior of each model in a more realistic scenario, we sample 7,666 images for BI-RADS prediction\\nand 7,301 images for breast density prediction from the test split following the dataset distribution.\\nTo avoid insufficient test data and possible bias, we use all the images with BIRADS 5 and 6 in\\nthe BIRADS prediction test set. Detailed class distribution is provided in the appendix. We also\\nuse the RSNA-Mammo [4] dataset for out-of-domain evaluation for binary cancer detection, which\\nonly released a training set with 54k images. We split it into a training set of 85% data and used\\nthe remaining as the evaluation. Given the extremely imbalanced distribution of both datasets, we\\nchoose to report balanced accuracy and AUC as our primary metrics. We also report the sensitivity\\nand specificity of the RSNA-Mammo cancer detection task. We do not assess zero-shot classification\\non this dataset since only a binary cancer label is available for all samples.\\nEvaluation Settings We evaluate all methods under zero-shot, linear classification, and full fine-\\ntuning settings. For zero-shot classification, we provide patient and imaging meta-information\\nalong with the class-wise captions, as this meta-information is readily available without a clinician’s\\ndiagnosis. For linear classification, we attach a linear classifier and fine-tune it using 1%, 10%, or\\n100% of the training data. Following [ 56,19,51,53,54,50], we perform this full data efficiency\\nstudy with linear classification and present as our primary results since this experiment mainly focuses\\non the quality of the pre-trained embedding and it can best demonstrate the difference between each\\nVLP method. For full fine-tuning, we again attach a linear classifier and fine-tune the whole model\\nusing 100% of the training data. Our learning rate is set to 5E-4 and weight decay to 1E-3 using the\\nSGD optimizer with cosine annealing scheduler for 8k steps with batch size 36. A warm-up of 100\\nsteps with a minimum learning rate of 1E-5 is applied. The fine-tuning uses 2 RTX A5000 GPUs.\\nBaselines As the very first attempt at full contrastive language-image pre-training for mammog-\\nraphy, we choose to compare with the following baselines: 1) ViT [13,37]: we compare with\\nvision-only baselines with both random initialization and DiNOv2 [ 37] pre-training. 2) CLIP [39]:\\nthe vanilla CLIP model without other additional design; 3) SLIP [34]: a contrastive learning frame-\\nwork that optimizes both image-image and image-text loss; 4) MM-MIL [52]: a CLIP model that\\nlearns local image-language relationship via a multiple instance learning paradigm; 5) ConVIRT [56]:\\none of the first Chest X-ray specific CLIP models; 6) MGCA [51]: one of the SoTA CLIP models\\nfor Chest X-ray that applies multi-granularity feature alignment. We pre-train and fine-tune all\\nthese baselines with the same settings as our model. We also replaced the original DeiT [ 48] ViT\\nwith DiNOv2 [ 37] for a fair comparison since DeiT-ViT [ 48] is only trained with a smaller image\\nsize. All the baseline methods use fully fine-tuned BioClinicalBERT [ 2] as text encoder. While we\\nacknowledge that there are other recent medical VLP methods [ 19,54,50,53], they either adapt\\n7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='Table 2: Zero-shot and Full Fine-tuning Results on EMBED [ 21]We evaluate zero-shot and fully\\nfine-tuned classification results for both BI-RADS and density prediction tasks. We report balanced\\naccuracy (bACC) and AUC. The best and second-best results are highlighted in bold and underlined,\\nrespectively. Our method is shaded in gray.\\nModelsEMBED BI-RADS [21] EMBED Density [21]\\nZero-shot Full Fine-tune Zero-shot Full Fine-tune\\nbACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%)\\nDeiT-based [48]\\nCLIP [38] 23.86 67.08 25.05 63.43 71.72 91.52 71.90 89.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='Tasks and Datasets We primarily evaluate our method on the EMBED [21] dataset for both\\nBI-RADS assessment category ('),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content=') prediction tasks. Note that\\nthe real-world distribution of labels for both tasks is extremely imbalanced. To demonstrate the\\nbehavior of each model in a more realistic scenario, we sample 7,6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='the BIRADS prediction test set. Detailed class distribution is provided in the appendix. We also\\nuse the RSNA-Mammo [4] dataset for out-of-domain evaluation for binary cancer detection, which\\nonly released a training set with 54k images. We split it into a training set of 85% data and used\\nthe remaining as the evaluation. Given the extremely imbalanced distribution of both datasets, we\\nchoose to report balanced accuracy and AUC as our primary metrics. We also report the sensitivity\\nand specificity of the RSNA-Mammo cancer detection task. We do not assess zero-shot classification'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='on this dataset since only a binary cancer label is available for all samples.\\nEvaluation Settings We evaluate all methods under zero-shot, linear classification, and full fine-\\ntuning settings. For zero-shot classification, we provide patient and imaging meta-information\\nalong with the class-wise captions, as this meta-information is readily available without a clinician’s\\ndiagnosis. For linear classification, we attach a linear classifier and fine-tune it using 1%, 10%, or\\n100% of the training data. Following [ 56,19,51,53,54,50], we perform this full data efficiency'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='study with linear classification and present as our primary results since this experiment mainly focuses\\non the quality of the pre-trained embedding and it can best demonstrate the difference between each\\nVLP method. For full fine-tuning, we again attach a linear classifier and fine-tune the whole model\\nusing 100% of the training data. Our learning rate is set to 5E-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='.\\nBaselines As the very first attempt at full contrastive language-image pre-training for mammog-\\nraphy, we choose to compare with the following baselines: 1) ViT [13,37]: we compare with\\nvision-only baselines with both random initialization and DiNOv2 [ 37] pre-training. 2) CLIP [39]:\\nthe vanilla CLIP model without other additional design; 3) SLIP [34]: a contrastive learning frame-\\nwork that optimizes both image-image and image-text loss; 4) MM-MIL [52]: a CLIP model that\\nlearns local image-language relationship via a multiple instance learning paradigm; 5) ConVIRT [56]:'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='one of the first Chest X-ray specific CLIP models; 6) MGCA [51]: one of the SoTA CLIP models\\nfor Chest X-ray that applies multi-granularity feature alignment. We pre-train and fine-tune all\\nthese baselines with the same settings as our model. We also replaced the original DeiT [ 48] ViT\\nwith DiNOv2 [ 37] for a fair comparison since DeiT-ViT [ 48] is only trained with a smaller image\\nsize. All the baseline methods use fully fine-tuned BioClinicalBERT [ 2] as text encoder. While we\\nacknowledge that there are other recent medical VLP methods [ 19,54,50,53], they either adapt\\n7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='Table 3: Classification Results on RSNA-Mammo [ 4]We evaluate linear classification and fully'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='fine-tuned settings for the cancer prediction task. We report balanced accuracy (bACC), AUC,\\nsensitivity (SEN), and specificity (SPE). The best and second-best results are highlighted in bold and\\nunderlined, respectively. Our method is shaded in gray.\\nModelsRSNA-Mammo [4]\\nLinear Classification Full Fine-tune\\nbACC (%) AUC (%) SEN (%) SPE (%) bACC (%) AUC (%) SEN (%) SPE (%)\\nVision only\\nRandom-ViT 51.90 56.34 72.60 31.21 56.71 57.62 77.88 35.53\\nDiNOv2-ViT 63.23 68.59 59.62 66.84 55.12 58.18 70.19 40.06\\nDeiT-based [48]\\nCLIP [38] 53.97 58.20 85.58 22.37 56.83 61.00 64.42 49.24\\nConVIRT [56] 65.96 69.81 66.83 65.10 53.31 69.16 8.65 97.96\\nMGCA [51] 63.01 69.16 62.50 63.52 53.88 73.04 12.02 95.74\\nDiNOv2-based\\nCLIP [38] 63.89 70.28 58.17 69.61 56.86 61.20 69.23 44.49\\nSLIP [34] 62.48 67.51 78.37 46.60 56.74 60.05 63.94 49.53\\nMM-MIL [52] 64.02 70.67 58.17 69.86 59.97 65.04 57.21 62.73\\nConVIRT [56] 65.89 70.70 66.83 64.96 54.53 69.85 11.06 98.01\\nMGCA [51] 60.79 67.45 71.15 50.43 55.99 68.67 14.90 97.07\\nMaMA 67.50 73.99 72.60 62.40 65.20 73.01 67.31 63.10\\ndomain-specific design and require annotations not presented in our dataset [ 53,50,54] or were\\nshown to perform worse in other studies than the chosen baselines [ 19,60]. We also do not compare\\nto related work that has no official implementation released [30, 6].\\n4.3 Results\\nLinear Classification We report the performance of both EMBED BI-RADS and density classifi-\\ncation tasks for each baseline in Tab. 1. We note MaMA achieves the best performance overall under\\ndifferent amounts of training data. Our method shows a non-trivial improvement of more than 4%\\nof balanced accuracy on the BI-RADS prediction task when fine-tuned with full training data. We\\nnote that reducing the amount of training data has a greater influence on the BI-RADS prediction\\ntask than the density prediction task, as the BI-RADS distribution is more imbalanced, e.g., there\\nare only 6 training images for BI-RADS category 5 and 2 images for category 6 when using 1%\\ntraining data. However, our method still maintains the best overall performance even when trained\\nwith only 1% data on the BI-RADS prediction task. This demonstrates the strong generalization\\nability and robustness of MaMA. Even if comparing with baselines also with local awareness [ 52,51],\\nour method is still the best. We also notice that the DiNOv2 [ 37]-based models tend to outperform\\nthe DeiT [ 48]-based models even if using the same VLP model design. This is not only because\\nDiNOv2 ViT [ 37] was trained on more data, but also due to the use of a larger image size, which is\\ncritical for high-resolution mammography.\\nZero-shot Classification We report the zero-shot classification performance for each of the meth-\\nods on both EMBED [ 21] tasks in Tab. 2. While our method still outperforms all the baselines,\\nwe highlight the zero-shot performance of the BI-RADS score prediction task, where our model\\noutperforms the best baseline by ∼5% in terms of balanced accuracy and more than 7% in AUC\\nscore. Compared with baselines using the fully fine-tuned small BioClinicalBERT [ 2], our method\\nwith pre-trained LLM with PEFT shows much better zero-shot performance as the LLM can provide\\na text-supervised signal with higher quality. Meanwhile, the PEFT helps to prevent the LLM from\\ncollapsing during fine-tuning. As a result, our LLM text encoder with PEFT can provide better\\nzero-shot text embedding and improve the zero-shot performance greatly. Meanwhile, we note that\\nthe adopted LLM with PEFT encoder only has 2.6 M trainable parameters, which is only 3% of the\\nBioClinicalBERT [2] in terms of size.\\nFull Fine-tuning Classification We also report the classification results after full-fine-tuning for\\nEMBED [ 21] tasks in Tab. 2. We note that while the gap between each method is somewhat reduced\\ndue to full fine-tuning, our model still beats all other baselines on both tasks.\\nOut-of-Domain Data Analysis We report performance of each method on the out-of-domain\\nRSNA-Mammo dataset in Tab. 3. Since RSNA-Mammo [ 4] is an extremely imbalanced dataset\\n(48:1 negative to positive ratio), we report the sensitivity and specificity as well. We note our model\\n8'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='Table 3: Classification Results on RSNA-Mammo [ 4]We evaluate linear classification and fully\\nfine-tuned settings for the cancer prediction task. We report balanced accuracy (bACC), AUC,\\nsensitivity (SEN), and specificity (SPE). The best and second-best results are highlighted in bold and\\nunderlined, respectively. Our method is shaded in gray.\\nModelsRSNA-Mammo [4]\\nLinear Classification Full Fine-tune\\nbACC (%) AUC (%) SEN (%) SPE (%) bACC (%) AUC (%) SEN (%) SPE (%)\\nVision only\\nRandom-ViT 51.90 56.34 72.60 31.21 56.71 57.62 77.88 35.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='-specific design and require annotations not presented in our dataset [ 53,50,54] or were\\nshown to perform worse in other studies than the chosen baselines [ 19,60]. We also do not compare\\nto related work that has no official implementation released [30, 6].\\n4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='Linear Classification We report the performance of both EMBED BI-RADS and density classifi-\\ncation tasks for each baseline in Tab. 1. We note MaMA achieves the best performance overall under\\ndifferent amounts of training data. Our method shows a non-trivial improvement of more than 4%\\nof balanced accuracy on the BI-RADS prediction task when fine-tuned with full training data. We\\nnote that reducing the amount of training data has a greater influence on the BI-RADS prediction\\ntask than the density prediction task, as the BI-RADS distribution is more imbalanced, e.g., there\\nare only'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='1%\\ntraining data. However, our method still maintains the best overall performance even when trained\\nwith only 1% data on the BI-RADS prediction task. This demonstrates the strong generalization\\nability and robustness of MaMA. Even if comparing with baselines also with local awareness [ 52,51],\\nour method is still the best. We also notice that the DiNOv2 [ 37]-based models tend to outperform\\nthe DeiT [ 48]-based models even if using the same VLP model design. This is not only because\\nDiNOv'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='[ 37] was trained on more data, but also due to the use of a larger image size, which is\\ncritical for high-resolution mammography.\\nZero-shot Classification We report the zero-shot classification performance for each of the meth-\\nods on both EMBED [ 21] tasks in Tab. 2. While our method still outperforms all the baselines,\\nwe highlight the zero-shot performance of the BI-RADS score prediction task, where our model\\noutperforms the best baseline by ∼5% in terms of balanced accuracy and more than 7% in AUC'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='score. Compared with baselines using the fully fine-tuned small BioClinicalBERT [ 2], our method\\nwith pre-trained LLM with PEFT shows much better zero-shot performance as the LLM can provide\\na text-supervised signal with higher quality. Meanwhile, the PEFT helps to prevent the LLM from\\ncollapsing during fine-tuning. As a result, our LLM text encoder with PEFT can provide better\\nzero-shot text embedding and improve the zero-shot performance greatly. Meanwhile, we note that\\nthe adopted LLM with PEFT encoder only has 2.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content=', which is only 3% of the\\nBioClinicalBERT [2] in terms of size.\\nFull Fine-tuning Classification We also report the classification results after full-fine-tuning for\\nEMBED [ 21] tasks in Tab. 2. We note that while the gap between each method is somewhat reduced\\ndue to full fine-tuning, our model still beats all other baselines on both tasks.\\nOut-of-Domain Data Analysis We report performance of each method on the out-of-domain\\nRSNA-Mammo dataset in Tab. 3. Since RSNA-Mammo [ 4] is an extremely imbalanced dataset\\n(48:'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='Table 4: Ablation of Model Design We ablate different model designs on the EMBED [ 21] BI-RADS'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='prediction task and report balanced accuracy (bACC) and AUC score. The best and second best\\nresults are highlighted in bold and underlined, respectively. Our full method is shaded in gray.\\nMethods EMBED BI-RADS [21]\\nSLA Symm. LV T LV V PEFT-LLMZero-shot Linear Classification Full Fine-tune\\nbACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%)\\n✓ ✓ ✓ 29.28 71.16 38.71 77.50 30.55 70.69\\n✓ ✓ ✓ 31.03 72.79 39.57 77.39 39.47 76.23\\n✓ ✓ ✓ 27.32 70.18 37.21 77.95 23.78 63.97\\n✓ ✓ ✓ 23.88 62.84 38.96 77.43 22.29 63.77\\n✓ ✓ ✓ ✓ 31.04 74.83 39.75 77.50 40.31 77.36\\nTable 5: Multi-view Ablation We ablate different\\nmulti-view contrastive learning strategies.\\nMethodsEMBED BI-RADS [21]\\nZero-shot Linear Classification Full Fine-tune\\nbACC(%) AUC(%) bACC(%) AUC(%) bACC(%) AUC(%)\\nSame Image 30.48 73.95 39.70 77.73 39.35 76.44\\nIntra-side 30.71 74.21 39.93 77.41 35.17 76.09\\nIntra-study 31.04 74.83 39.75 77.50 40.31 77.36Table 6: Caption Ablation We ablate different\\ntext caption construction strategies.\\nMethodsEMBED BI-RADS [21]\\nZero-shot Linear Classification Full Fine-tune\\nbACC(%) AUC(%) bACC(%) AUC(%) bACC(%) AUC(%)\\nCLIP-style 35.99 77.66 37.74 77.25 24.00 65.35\\nNo Meta Mask 27.19 68.20 36.94 76.33 24.06 64.85\\nStruct. Cap. 31.04 74.83 39.75 77.50 40.31 77.36\\nperforms best in terms of balanced accuracy and AUC with a notable gap. While some of the\\nbaselines outperform our model on either the sensitivity or specificity metric, we note these models\\nare not informative, i.e., they tend to collapse and predict the majority of images to one of the classes.\\nThis will lead to a high score in one of the sensitivity or specificity metrics while result in a low\\nperformance in the other. In contrast, our approach shows reasonable results for both metrics and is\\nthe only method with both sensitivity and specificity greater than 60% under both the linear and full\\nfine-tuning settings. Furthermore, the other few methods that demonstrated higher sensitivity than\\nours all resulted in a specificity of ∼45% or worse.\\n4.4 Ablation Experiments\\nModel Design We ablate the influence of each component in Tab. 4. Compared with these baselines,\\nwe note each component has an important contribution to the overall model performance, as removing\\nany one resulted in inferior performance. We note that the baseline without PEFT-LLM instead\\nemploys BioClinicalBERT [ 2] and shows a clear drop in zero-shot performance, which validates the\\nimportance of using a PEFT-LLM. However, this model still performs well on the linear classification\\nand full fine-tuning tasks, which demonstrates the effectiveness of our other design choices.\\nMulti-view Ablation We ablate the multi-view sampling strategy here by using: 1) the same image,\\n2) an intra-side image, and 3) the complete intra-study image (Tab. 5). We can see that the model\\ntrained with only one image loses the multi-view understanding. The model using only intra-side\\nimages only considers ipsilateral correspondence and also results in a worse performance.\\nCaption Ablation We evaluate the influence of using different caption construction strategies\\nin Tab. 6. We note that a CLIP style caption that only focuses on class labels shows a better\\nzero-shot performance, but degenerates greatly in the linear classification and full fine-tuning tasks.\\nMeanwhile, if simply using the full meta-information during training, the model will fail with\\nzero-shot classification since it may mainly rely on the meta-information during the training and\\nignore more important clinical information. Our full design of using a structural caption with\\nmeta-information masking shows the best performance.\\n5 Discussion and Conclusion\\nIn this work, we presented a complete and novel multi-view and multi-scale alignment contrastive\\nlanguage-image pre-training method for mammography. We proposed utilizing the multi-view nature\\nof mammography and providing local image-sentence correspondence to help address the challenges\\nof small ROIs and high image resolution and provide fine-grained visual clues for decisions. The\\nproposed method greatly outperforms multiple existing medical CLIP baselines.\\n9'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='Table 4: Ablation of Model Design We ablate different model designs on the EMBED [ 21] BI-RADS\\nprediction task and report balanced accuracy (bACC) and AUC score. The best and second best\\nresults are highlighted in bold and underlined, respectively. Our full method is shaded in gray.\\nMethods EMBED BI-RADS [21]\\nSLA Symm. LV T LV V PEFT-LLMZero-shot Linear Classification Full Fine-tune\\nbACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%)\\n✓ ✓ ✓ 29.28 71.16 38.71 77.50 30.55 70.69\\n✓ ✓ ✓ 31.03 72.79 39.57 77.39 39.47 76.23\\n✓ ✓ ✓ 27.32 70.18 37.21 77.95 23.78 63.97'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='✓ ✓ ✓ 27.32 70.18 37.21 77.95 23.78 63.97\\n✓ ✓ ✓ 23.88 62.84 38.96 77.43 22.29 63.77\\n✓ ✓ ✓ ✓ 31.04 74.83 39.75 77.50 40.31 77.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='5: Multi-view Ablation We ablate different\\nmulti-view contrastive learning strategies.\\nMethodsEMBED BI-RADS [21]\\nZero-shot Linear Classification Full Fine-tune\\nbACC(%) AUC(%) bACC(%) AUC(%) bACC(%) AUC(%)\\nSame Image 30.48 73.95 39.70 77.73 39.35 76.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='-study 31.04 74.83 39.75 77.50 40.31 77.36Table 6: Caption Ablation We ablate different\\ntext caption construction strategies.\\nMethodsEMBED BI-RADS [21]\\nZero-shot Linear Classification Full Fine-tune\\nbACC(%) AUC(%) bACC(%) AUC(%) bACC(%) AUC(%)\\nCLIP-style 35.99 77.66 37.74 77.25 24.00 65.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='. While some of the\\nbaselines outperform our model on either the sensitivity or specificity metric, we note these models\\nare not informative, i.e., they tend to collapse and predict the majority of images to one of the classes.\\nThis will lead to a high score in one of the sensitivity or specificity metrics while result in a low\\nperformance in the other. In contrast, our approach shows reasonable results for both metrics and is\\nthe only method with both sensitivity and specificity greater than 60% under both the linear and full'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='fine-tuning settings. Furthermore, the other few methods that demonstrated higher sensitivity than\\nours all resulted in a specificity of ∼45% or worse.\\n4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='Model Design We ablate the influence of each component in Tab. 4. Compared with these baselines,\\nwe note each component has an important contribution to the overall model performance, as removing\\nany one resulted in inferior performance. We note that the baseline without PEFT-LLM instead\\nemploys BioClinicalBERT [ 2] and shows a clear drop in zero-shot performance, which validates the\\nimportance of using a PEFT-LLM. However, this model still performs well on the linear classification\\nand full fine-tuning tasks, which demonstrates the effectiveness of our other design choices.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='Multi-view Ablation We ablate the multi-view sampling strategy here by using: 1) the same image,\\n2) an intra-side image, and 3) the complete intra-study image (Tab. 5). We can see that the model\\ntrained with only one image loses the multi-view understanding. The model using only intra-side\\nimages only considers ipsilateral correspondence and also results in a worse performance.\\nCaption Ablation We evaluate the influence of using different caption construction strategies\\nin Tab. 6. We note that a CLIP style caption that only focuses on class labels shows a better'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='zero-shot performance, but degenerates greatly in the linear classification and full fine-tuning tasks.\\nMeanwhile, if simply using the full meta-information during training, the model will fail with\\nzero-shot classification since it may mainly rely on the meta-information during the training and\\nignore more important clinical information. Our full design of using a structural caption with\\nmeta-information masking shows the best performance.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='In this work, we presented a complete and novel multi-view and multi-scale alignment contrastive\\nlanguage-image pre-training method for mammography. We proposed utilizing the multi-view nature\\nof mammography and providing local image-sentence correspondence to help address the challenges\\nof small ROIs and high image resolution and provide fine-grained visual clues for decisions. The\\nproposed method greatly outperforms multiple existing medical CLIP baselines.\\n9'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='Limitation and Future Work As we mainly focus on image representation learning, we have yet to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='evaluate other downstream tasks like image-text retrieval, object detection, and segmentation. While\\nalso limited by accessible data in this domain, our method will be evaluated on more downstream\\ntasks in future work. We plan to extend this current framework to more mammography imaging\\nmodalities including C-view and digital breast tomosynthesis to further enhance its understanding of\\nmammography. Meanwhile, we also plan to integrate this pre-trained component into a multi-modal\\nquestion-answering and grounding model, to further explore the potential of medical VLP.\\n6 Acknowledgement\\nThis work was supported by NIH grant R21EB032950.\\nReferences\\n[1] AI@Meta. Llama 3 model card, 2024. Accessed: 2024-05-10.\\n[2]Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew\\nMcDermott. Publicly available clinical BERT embeddings. In Proceedings of the 2nd Clinical Natural\\nLanguage Processing Workshop , pages 72–78, Minneapolis, Minnesota, USA, June 2019. Association for\\nComputational Linguistics.\\n[3]Elliot Bolton, David Hall, Michihiro Yasunaga, Tony Lee, Chris Manning, and Percy Liang. Biomedlm,\\n2023. Accessed: 2023-03-02.\\n[4] Chris Carr and Yan Chen et.al. Rsna screening mammography breast cancer detection, 2022.\\n[5]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\\ncontrastive learning of visual representations. In International conference on machine learning , pages\\n1597–1607. PMLR, 2020.\\n[6]Xuxin Chen, Yuheng Li, Mingzhe Hu, Ella Salari, Xiaoqian Chen, Richard LJ Qiu, Bin Zheng, and\\nXiaofeng Yang. Mammo-clip: Leveraging contrastive language-image pre-training (clip) for enhanced\\nbreast cancer diagnosis with multi-view mammography. arXiv preprint arXiv:2404.15946 , 2024.\\n[7]Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco\\nSalvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. Meditron-70b: Scaling\\nmedical pretraining for large language models. arXiv preprint arXiv:2311.16079 , 2023.\\n[8]Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers.\\narXiv preprint arXiv:2309.16588 , 2023.\\n[9]Alexandros Delitzas, Maria Parelli, Nikolas Hars, Georgios Vlassis, Sotirios Anagnostidis, Gregor Bach-\\nmann, and Thomas Hofmann. Multi-clip: Contrastive vision-language pre-training for question answering\\ntasks in 3d scenes. arXiv preprint arXiv:2306.02329 , 2023.\\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255.\\nIeee, 2009.\\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\\n[12] Jon Donnelly, Luke Moffett, Alina Jade Barnett, Hari Trivedi, Fides Schwartz, Joseph Lo, and Cynthia\\nRudin. Asymmirai: Interpretable mammography-based deep learning model for 1–5-year breast cancer\\nrisk prediction. Radiology , 310(3):e232780, 2024.\\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.\\n[14] Yuexi Du, Regina J Hooley, John Lewin, and Nicha C Dvornek. Sift-dbt: Self-supervised initializa-\\ntion and fine-tuning for imbalanced digital breast tomosynthesis image classification. arXiv preprint\\narXiv:2403.13148 , 2024.\\n[15] Sedigheh Eslami, Christoph Meinel, and Gerard De Melo. Pubmedclip: How much does clip benefit visual\\nquestion answering in the medical domain? In Findings of the Association for Computational Linguistics:\\nEACL 2023 , pages 1181–1193, 2023.\\n[16] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,\\nCarl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your\\nown latent-a new approach to self-supervised learning. Advances in neural information processing systems ,\\n33:21271–21284, 2020.\\n[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\\nvisual representation learning. arXiv preprint arXiv:1911.05722 , 2019.\\n10'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='Limitation and Future Work As we mainly focus on image representation learning, we have yet to\\nevaluate other downstream tasks like image-text retrieval, object detection, and segmentation. While\\nalso limited by accessible data in this domain, our method will be evaluated on more downstream\\ntasks in future work. We plan to extend this current framework to more mammography imaging\\nmodalities including C-view and digital breast tomosynthesis to further enhance its understanding of\\nmammography. Meanwhile, we also plan to integrate this pre-trained component into a multi-modal'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='question-answering and grounding model, to further explore the potential of medical VLP.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content=', 2024. Accessed: 2024-05-10.\\n[2]Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew\\nMcDermott. Publicly available clinical BERT embeddings. In Proceedings of the 2nd Clinical Natural\\nLanguage Processing Workshop , pages 72–78, Minneapolis, Minnesota, USA, June 2019. Association for\\nComputational Linguistics.\\n[3]Elliot Bolton, David Hall, Michihiro Yasunaga, Tony Lee, Chris Manning, and Percy Liang. Biomedlm,\\n2023. Accessed: 2023-03-02.\\n[4] Chris Carr and Yan Chen et.al. Rsna screening mammography breast cancer detection, 2022.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='[5]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\\ncontrastive learning of visual representations. In International conference on machine learning , pages\\n1597–1607. PMLR, 2020.\\n[6]Xuxin Chen, Yuheng Li, Mingzhe Hu, Ella Salari, Xiaoqian Chen, Richard LJ Qiu, Bin Zheng, and\\nXiaofeng Yang. Mammo-clip: Leveraging contrastive language-image pre-training (clip) for enhanced\\nbreast cancer diagnosis with multi-view mammography. arXiv preprint arXiv:2404.15946 , 2024.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='[7]Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco\\nSalvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. Meditron-70b: Scaling\\nmedical pretraining for large language models. arXiv preprint arXiv:2311.16079 , 2023.\\n[8]Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers.\\narXiv preprint arXiv:2309.16588 , 2023.\\n[9]Alexandros Delitzas, Maria Parelli, Nikolas Hars, Georgios Vlassis, Sotirios Anagnostidis, Gregor Bach-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='mann, and Thomas Hofmann. Multi-clip: Contrastive vision-language pre-training for question answering\\ntasks in 3d scenes. arXiv preprint arXiv:2306.02329 , 2023.\\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In 20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content=', pages 248–255.\\nIeee, 2009.\\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\\n[12] Jon Donnelly, Luke Moffett, Alina Jade Barnett, Hari Trivedi, Fides Schwartz, Joseph Lo, and Cynthia\\nRudin. Asymmirai: Interpretable mammography-based deep learning model for 1–5-year breast cancer\\nrisk prediction. Radiology , 310(3):e232780, 2024.\\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\\n16x'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content=': Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.\\n[14] Yuexi Du, Regina J Hooley, John Lewin, and Nicha C Dvornek. Sift-dbt: Self-supervised initializa-\\ntion and fine-tuning for imbalanced digital breast tomosynthesis image classification. arXiv preprint\\narXiv:2403.13148 , 2024.\\n[15] Sedigheh Eslami, Christoph Meinel, and Gerard De Melo. Pubmedclip: How much does clip benefit visual\\nquestion answering in the medical domain? In Findings of the Association for Computational Linguistics:\\nEACL 2023 , pages 1181–1193, 2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='EACL 2023 , pages 1181–1193, 2023.\\n[16] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,\\nCarl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your\\nown latent-a new approach to self-supervised learning. Advances in neural information processing systems ,\\n33:21271–21284, 2020.\\n[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\\nvisual representation learning. arXiv preprint arXiv:1911.05722 , 2019.\\n10'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 ,\\n2021.\\n[19] Shih-Cheng Huang, Liyue Shen, Matthew P Lungren, and Serena Yeung. Gloria: A multimodal global-\\nlocal representation learning framework for label-efficient medical image recognition. In Proceedings of\\nthe IEEE/CVF International Conference on Computer Vision , pages 3942–3951, 2021.\\n[20] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik\\nMarklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph\\ndataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial\\nintelligence , volume 33, pages 590–597, 2019.\\n[21] Jiwoong J Jeong, Brianna L Vey, Ananth Bhimireddy, Thomas Kim, Thiago Santos, Ramon Correa, Raman\\nDutt, Marina Mosunjac, Gabriela Oprea-Ilies, Geoffrey Smith, et al. The emory breast imaging dataset\\n(embed): A racially diverse, granular dataset of 3.4 million screening and diagnostic mammographic\\nimages. Radiology: Artificial Intelligence , 5(1):e220047, 2023.\\n[22] Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Yifan\\nPeng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. Mimic-cxr-jpg, a large publicly\\navailable database of labeled chest radiographs. arXiv preprint arXiv:1901.07042 , 2019.\\n[23] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi,\\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible\\ncritical care database. Scientific data , 3(1):1–9, 2016.\\n[24] Zhengfeng Lai, Zhuoheng Li, Luca Cerny Oliveira, Joohi Chauhan, Brittany N Dugger, and Chen-Nee\\nChuah. Clipath: Fine-tune clip with visual feature fusion for pathology image analysis towards minimizing\\ndata collection efforts. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\\npages 2374–2380, 2023.\\n[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\\nwith frozen image encoders and large language models. In International conference on machine learning ,\\npages 19730–19742. PMLR, 2023.\\n[26] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\\nfor unified vision-language understanding and generation. In International conference on machine learning ,\\npages 12888–12900. PMLR, 2022.\\n[27] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie\\nYan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm.\\narXiv preprint arXiv:2110.05208 , 2021.\\n[28] Zheren Li, Zhiming Cui, Sheng Wang, Yuji Qi, Xi Ouyang, Qitian Chen, Yuezhi Yang, Zhong Xue,\\nDinggang Shen, and Jie-Zhi Cheng. Domain generalization for mammography detection via multi-style\\nand multi-view contrastive learning. In Medical Image Computing and Computer Assisted Intervention–\\nMICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021,\\nProceedings, Part VII 24 , pages 98–108. Springer, 2021.\\n[29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural\\ninformation processing systems , 36, 2024.\\n[30] Jiarun Liu, Hong-Yu Zhou, Cheng Li, Weijian Huang, Hao Yang, Yong Liang, and Shanshan Wang.\\nMlip: Medical language-image pre-training with masked local representation learning. arXiv preprint\\narXiv:2401.01591 , 2024.\\n[31] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,\\nHang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object\\ndetection. arXiv preprint arXiv:2303.05499 , 2023.\\n[32] Yuhang Liu, Fandong Zhang, Chaoqi Chen, Siwen Wang, Yizhou Wang, and Yizhou Yu. Act like a\\nradiologist: towards reliable multi-view correspondence reasoning for mammogram mass detection. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence , 44(10):5947–5961, 2021.\\n[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101 , 2017.\\n[34] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-\\nimage pre-training. In European conference on computer vision , pages 529–544. Springer, 2022.\\n[35] HQ Nguyen, HH Pham, LT Le, M Dao, and K VinDr-CXR Lam. An open dataset of chest x-rays with\\nradiologist annotations. PhysioNet https://doi. org/10.13026/3akn-b287 , 2021.\\n[36] Michael Onken, Marco Eichelberg, Jörg Riesmeier, and Peter Jensch. Digital imaging and communications\\nin medicine. In Biomedical Image Processing , pages 427–454. Springer, 2010.\\n[37] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V . V o, Marc Szafraniec, Vasil Khalidov, Pierre\\nFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu\\nXu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel\\n11'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 ,\\n2021.\\n[19] Shih-Cheng Huang, Liyue Shen, Matthew P Lungren, and Serena Yeung. Gloria: A multimodal global-\\nlocal representation learning framework for label-efficient medical image recognition. In Proceedings of\\nthe IEEE/CVF International Conference on Computer Vision , pages 3942–3951, 2021.\\n[20] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph\\ndataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial\\nintelligence , volume 33, pages 590–597, 2019.\\n[21] Jiwoong J Jeong, Brianna L Vey, Ananth Bhimireddy, Thomas Kim, Thiago Santos, Ramon Correa, Raman\\nDutt, Marina Mosunjac, Gabriela Oprea-Ilies, Geoffrey Smith, et al. The emory breast imaging dataset\\n(embed): A racially diverse, granular dataset of 3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='images. Radiology: Artificial Intelligence , 5(1):e220047, 2023.\\n[22] Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Yifan\\nPeng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. Mimic-cxr-jpg, a large publicly\\navailable database of labeled chest radiographs. arXiv preprint arXiv:1901.07042 , 2019.\\n[23] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi,\\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='critical care database. Scientific data , 3(1):1–9, 2016.\\n[24] Zhengfeng Lai, Zhuoheng Li, Luca Cerny Oliveira, Joohi Chauhan, Brittany N Dugger, and Chen-Nee\\nChuah. Clipath: Fine-tune clip with visual feature fusion for pathology image analysis towards minimizing\\ndata collection efforts. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\\npages 2374–2380, 2023.\\n[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='with frozen image encoders and large language models. In International conference on machine learning ,\\npages 19730–19742. PMLR, 2023.\\n[26] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\\nfor unified vision-language understanding and generation. In International conference on machine learning ,\\npages 12888–12900. PMLR, 2022.\\n[27] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie\\nYan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='arXiv preprint arXiv:2110.05208 , 2021.\\n[28] Zheren Li, Zhiming Cui, Sheng Wang, Yuji Qi, Xi Ouyang, Qitian Chen, Yuezhi Yang, Zhong Xue,\\nDinggang Shen, and Jie-Zhi Cheng. Domain generalization for mammography detection via multi-style\\nand multi-view contrastive learning. In Medical Image Computing and Computer Assisted Intervention–\\nMICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021,\\nProceedings, Part VII 24 , pages 98–108. Springer, 2021.\\n[29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='information processing systems , 36, 2024.\\n[30] Jiarun Liu, Hong-Yu Zhou, Cheng Li, Weijian Huang, Hao Yang, Yong Liang, and Shanshan Wang.\\nMlip: Medical language-image pre-training with masked local representation learning. arXiv preprint\\narXiv:2401.01591 , 2024.\\n[31] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,\\nHang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object\\ndetection. arXiv preprint arXiv:2303.05499 , 2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='detection. arXiv preprint arXiv:2303.05499 , 2023.\\n[32] Yuhang Liu, Fandong Zhang, Chaoqi Chen, Siwen Wang, Yizhou Wang, and Yizhou Yu. Act like a\\nradiologist: towards reliable multi-view correspondence reasoning for mammogram mass detection. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence , 44(10):5947–5961, 2021.\\n[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101 , 2017.\\n[34] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='image pre-training. In European conference on computer vision , pages 529–544. Springer, 2022.\\n[35] HQ Nguyen, HH Pham, LT Le, M Dao, and K VinDr-CXR Lam. An open dataset of chest x-rays with\\nradiologist annotations. PhysioNet https://doi. org/10.13026/3akn-b287 , 2021.\\n[36] Michael Onken, Marco Eichelberg, Jörg Riesmeier, and Peter Jensch. Digital imaging and communications\\nin medicine. In Biomedical Image Processing , pages 427–454. Springer, 2010.\\n[37] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V . V o, Marc Szafraniec, Vasil Khalidov, Pierre'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu\\nXu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel\\n11'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='Dinov2: Learning robust visual features without supervision, 2023.\\n[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\\nnatural language supervision. In International conference on machine learning , pages 8748–8763. PMLR,\\n2021.\\n[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\\nmodels are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\\n[40] Ketul Shah, Anshul Shah, Chun Pong Lau, Celso M de Melo, and Rama Chellappa. Multi-view action\\nrecognition using contrastive learning. In Proceedings of the IEEE/CVF Winter Conference on Applications\\nof Computer Vision , pages 3381–3391, 2023.\\n[41] E. A. Sickles, C. J. D’Orsi, L. W. Bassett, et al. ACR BI-RADS mammography. In ACR BI-RADS Atlas,\\nBreast Imaging Reporting and Data System . American College of Radiology, Reston, V A, 5th edition,\\n2013.\\n[42] Dan Song, Xinwei Fu, Weizhi Nie, Wenhui Li, and Anan Liu. Mv-clip: Multi-view clip for zero-shot 3d\\nshape recognition. arXiv preprint arXiv:2311.18402 , 2023.\\n[43] Lilei Sun, Jie Wen, Junqian Wang, Zheng Zhang, Yong Zhao, Guiying Zhang, and Yong Xu. Breast\\nmass classification based on supervised contrastive learning and multi-view consistency penalty on\\nmammography. IET Biometrics , 11(6):588–600, 2022.\\n[44] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques\\nfor clip at scale. arXiv preprint arXiv:2303.15389 , 2023.\\n[45] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques\\nfor clip at scale. arXiv preprint arXiv:2303.15389 , 2023.\\n[46] Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang.\\nEva-clip-18b: Scaling clip to 18 billion parameters. arXiv preprint arXiv:2402.04252 , 2024.\\n[47] Hyuna Sung, Jacques Ferlay, Rebecca L Siegel, Mathieu Laversanne, Isabelle Soerjomataram, Ahmedin\\nJemal, and Freddie Bray. Global cancer statistics 2020: Globocan estimates of incidence and mortality\\nworldwide for 36 cancers in 185 countries. CA: a cancer journal for clinicians , 71(3):209–249, 2021.\\n[48] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve\\nJegou. Training data-efficient image transformers and distillation through attention. In Marina Meila and\\nTong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning , volume 139\\nofProceedings of Machine Learning Research , pages 10347–10357. PMLR, 18–24 Jul 2021.\\n[49] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\\nfine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\\n[50] Zhongwei Wan, Che Liu, Mi Zhang, Jie Fu, Benyou Wang, Sibo Cheng, Lei Ma, César Quilodrán-\\nCasas, and Rossella Arcucci. Med-unic: Unifying cross-lingual medical vision-language pre-training by\\ndiminishing bias. Advances in Neural Information Processing Systems , 36, 2024.\\n[51] Fuying Wang, Yuyin Zhou, Shujun Wang, Varut Vardhanabhuti, and Lequan Yu. Multi-granularity cross-\\nmodal alignment for generalized medical visual representation learning. Advances in Neural Information\\nProcessing Systems , 35:33536–33549, 2022.\\n[52] Peiqi Wang, William M Wells, Seth Berkowitz, Steven Horng, and Polina Golland. Using multiple instance\\nlearning to build multimodal representations. In International Conference on Information Processing in\\nMedical Imaging , pages 457–470. Springer, 2023.\\n[53] Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun. Medclip: Contrastive learning from\\nunpaired medical images and text. arXiv preprint arXiv:2210.10163 , 2022.\\n[54] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Medklip: Medical knowledge\\nenhanced language-image pre-training. medRxiv , pages 2023–01, 2023.\\n[55] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh\\nRao, Mu Wei, Naveen Valluri, et al. Biomedclip: a multimodal biomedical foundation model pretrained\\nfrom fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915 , 2023.\\n[56] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. Contrastive\\nlearning of medical visual representations from paired images and text. In Machine Learning for Healthcare\\nConference , pages 2–25. PMLR, 2022.\\n[57] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai. Groundhog:\\nGrounding large language models to holistic segmentation. arXiv preprint arXiv:2402.16846 , 2024.\\n[58] Zihao Zhao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Xiang Li, Zhiming\\nCui, Qian Wang, et al. Clip in medical imaging: A comprehensive survey. arXiv preprint arXiv:2312.07353 ,\\n2023.\\n12'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.\\nDinov2: Learning robust visual features without supervision, 2023.\\n[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\\nnatural language supervision. In International conference on machine learning , pages 8748–8763. PMLR,\\n2021.\\n[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\\n[40] Ketul Shah, Anshul Shah, Chun Pong Lau, Celso M de Melo, and Rama Chellappa. Multi-view action\\nrecognition using contrastive learning. In Proceedings of the IEEE/CVF Winter Conference on Applications\\nof Computer Vision , pages 3381–3391, 2023.\\n[41] E. A. Sickles, C. J. D’Orsi, L. W. Bassett, et al. ACR BI-RADS mammography. In ACR BI-RADS Atlas,\\nBreast Imaging Reporting and Data System . American College of Radiology, Reston, V A, 5th edition,\\n2013.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='2013.\\n[42] Dan Song, Xinwei Fu, Weizhi Nie, Wenhui Li, and Anan Liu. Mv-clip: Multi-view clip for zero-shot 3d\\nshape recognition. arXiv preprint arXiv:2311.18402 , 2023.\\n[43] Lilei Sun, Jie Wen, Junqian Wang, Zheng Zhang, Yong Zhao, Guiying Zhang, and Yong Xu. Breast\\nmass classification based on supervised contrastive learning and multi-view consistency penalty on\\nmammography. IET Biometrics , 11(6):588–600, 2022.\\n[44] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques\\nfor clip at scale. arXiv preprint arXiv:2303.15389 , 2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='for clip at scale. arXiv preprint arXiv:2303.15389 , 2023.\\n[45] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques\\nfor clip at scale. arXiv preprint arXiv:2303.15389 , 2023.\\n[46] Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang.\\nEva-clip-18b: Scaling clip to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='. arXiv preprint arXiv:2402.04252 , 2024.\\n[47] Hyuna Sung, Jacques Ferlay, Rebecca L Siegel, Mathieu Laversanne, Isabelle Soerjomataram, Ahmedin\\nJemal, and Freddie Bray. Global cancer statistics 2020: Globocan estimates of incidence and mortality\\nworldwide for'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='. CA: a cancer journal for clinicians , 71(3):209–249, 2021.\\n[48] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve\\nJegou. Training data-efficient image transformers and distillation through attention. In Marina Meila and\\nTong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning , volume 1'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='2021.\\n[49] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\\nfine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\\n[50] Zhongwei Wan, Che Liu, Mi Zhang, Jie Fu, Benyou Wang, Sibo Cheng, Lei Ma, César Quilodrán-\\nCasas, and Rossella Arcucci. Med-unic: Unifying cross-lingual medical vision-language pre-training by\\ndiminishing bias. Advances in Neural Information Processing Systems , 36, 2024.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='[51] Fuying Wang, Yuyin Zhou, Shujun Wang, Varut Vardhanabhuti, and Lequan Yu. Multi-granularity cross-\\nmodal alignment for generalized medical visual representation learning. Advances in Neural Information\\nProcessing Systems , 35:33536–33549, 2022.\\n[52] Peiqi Wang, William M Wells, Seth Berkowitz, Steven Horng, and Polina Golland. Using multiple instance\\nlearning to build multimodal representations. In International Conference on Information Processing in\\nMedical Imaging , pages 457–470. Springer, 2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='Medical Imaging , pages 457–470. Springer, 2023.\\n[53] Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun. Medclip: Contrastive learning from\\nunpaired medical images and text. arXiv preprint arXiv:2210.10163 , 2022.\\n[54] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Medklip: Medical knowledge\\nenhanced language-image pre-training. medRxiv , pages 2023–01, 2023.\\n[55] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh\\nRao, Mu Wei, Naveen Valluri, et al. Biomedclip: a multimodal biomedical foundation model pretrained'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='from fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915 , 2023.\\n[56] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. Contrastive\\nlearning of medical visual representations from paired images and text. In Machine Learning for Healthcare\\nConference , pages 2–25. PMLR, 2022.\\n[57] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai. Groundhog:\\nGrounding large language models to holistic segmentation. arXiv preprint arXiv:2402.16846 , 2024.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='[58] Zihao Zhao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Xiang Li, Zhiming\\nCui, Qian Wang, et al. Clip in medical imaging: A comprehensive survey. arXiv preprint arXiv:2312.07353 ,\\n2023.\\n12'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 12}, page_content='[59] Kecheng Zheng, Yifei Zhang, Wei Wu, Fan Lu, Shuailei Ma, Xin Jin, Wei Chen, and Yujun Shen. Dreamlip:'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 12}, page_content='Language-image pre-training with long captions. arXiv preprint arXiv:2403.17007 , 2024.\\n[60] Hong-Yu Zhou, Chenyu Lian, Liansheng Wang, and Yizhou Yu. Advancing radiograph representation\\nlearning with masked record modeling. arXiv preprint arXiv:2301.13155 , 2023.\\n13'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 12}, page_content='[59] Kecheng Zheng, Yifei Zhang, Wei Wu, Fan Lu, Shuailei Ma, Xin Jin, Wei Chen, and Yujun Shen. Dreamlip:\\nLanguage-image pre-training with long captions. arXiv preprint arXiv:2403.17007 , 2024.\\n[60] Hong-Yu Zhou, Chenyu Lian, Liansheng Wang, and Yizhou Yu. Advancing radiograph representation\\nlearning with masked record modeling. arXiv preprint arXiv:2301.13155 , 2023.\\n13'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='A Appendix'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='In the appendix, we provide more detailed training settings, evaluation settings, model configurations,\\nand additional analysis.\\nA.1 Further Limitations\\nThe EMBED data comes from the Atlanta, GA region. While the dataset is highly ethnically\\ndiverse, the geographic focus could limit generalizability to other populations, e.g., the breast density\\ndistribution may differ from data gathered in other regions of the world. Additionally, The caption\\nis created from the templated-based method, which may potentially harm the model due to limited\\ncaption diversity. Future works may consider augment the template-based prompt with LLM.\\nA.2 Broader Impacts\\nThis paper proposed a promising visual language pre-training scheme for mammography that can be\\nused for various downstream tasks. It can also potentially speed up the real-world mammography\\nscreening or diagnostic process by filtering out low-risk studies and highlighting high-risk images for\\nthe clinician. While the EMBED dataset is one of the largest and most diverse public mammography\\ndatasets available, it is notable that the data were collected from four specific hospitals and thus\\nthe trained model may have a specific bias towards a specific group of people due to training data\\ncomposition. Any user who wants to use this model in their own research may need to carefully\\nanalyze such bias and their own application and tasks and avoid using the model in real-world clinical\\ntrials without further approval.\\nAlgorithm 1 SLA Loss Pseudocode\\n1:# fp, fs: local patch, sentence projectors\\n2:# N, tau_local: batch size and SLA loss temperature\\n3:# patch_feats: patch-wise image feature. (N, num_patch, C)\\n4:# sent_feats: sentence-wise text feature. list of N tensors, (num_sent, C)\\n5:def SLA_loss(patch_feats, sent_feats):\\n6: t2v_scores = [] # cV: visual localization correspondence\\n7: v2t_scores = [] # cT: textual localization correspondence\\n8: patch_feats = normalize(fp(patch_feats))\\n9: # Each report may have different num_sent\\n10: for sent in sent_feats:\\n11: sent = normalize(fs(sent))\\n12: score = torch.bmm(path_feats, sent.T) # (N, num_patch, num_sent)\\n13: # Visual localization: Max over patches + Avg over sentences\\n14: t2v_scores.append(score.max(dim=1, keepdim=True).mean(dim=2).squeeze())\\n15: # Textual localization: Max over sentences + Avg over patches\\n16: v2t_scores.append(score.max(dim=2, keepdim=True).mean(dim=1).squeeze())\\n17: t2v_scores = torch.stack(t2v_scores, dim=0) / tau_local # (N, N)\\n18: v2t_scores = torch.stack(v2t_scores, dim=0) / tau_local # (N, N)\\n19: labels = torch.arange(N)\\n20: loss0 = cross_entropy(t2v_scores, labels)\\n21: loss1 = cross_entropy(v2t_scores, labels)\\n22: return 0.5 * (loss0 + loss1)\\nA.3 Pseudo-Code of SLA Module\\nWe provide the pytorch pseudo-code of the SLA module in the Algorithm 1 to better illustrate the\\ndesign of the SLA module.\\nA.4 Pre-training Implementation Details\\nDataset and Pre-processing As mentioned in Sec. 4.1, we use the EMBED [ 21] dataset for pre-\\ntraining. We only use the 2D mammography and split the dataset into 70%/10%/20% for training,\\nvalidation, and testing at the patient level. We filter out the studies for males or those that have\\nmissing BI-RADS or density labels. We provide the detailed distribution of BI-RADS score and\\n14'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='A Appendix\\nIn the appendix, we provide more detailed training settings, evaluation settings, model configurations,\\nand additional analysis.\\nA.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='The EMBED data comes from the Atlanta, GA region. While the dataset is highly ethnically\\ndiverse, the geographic focus could limit generalizability to other populations, e.g., the breast density\\ndistribution may differ from data gathered in other regions of the world. Additionally, The caption\\nis created from the templated-based method, which may potentially harm the model due to limited\\ncaption diversity. Future works may consider augment the template-based prompt with LLM.\\nA.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='This paper proposed a promising visual language pre-training scheme for mammography that can be\\nused for various downstream tasks. It can also potentially speed up the real-world mammography\\nscreening or diagnostic process by filtering out low-risk studies and highlighting high-risk images for\\nthe clinician. While the EMBED dataset is one of the largest and most diverse public mammography\\ndatasets available, it is notable that the data were collected from four specific hospitals and thus\\nthe trained model may have a specific bias towards a specific group of people due to training data'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='composition. Any user who wants to use this model in their own research may need to carefully\\nanalyze such bias and their own application and tasks and avoid using the model in real-world clinical\\ntrials without further approval.\\nAlgorithm'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='1:# fp, fs: local patch, sentence projectors\\n2:# N, tau_local: batch size and SLA loss temperature\\n3:# patch_feats: patch-wise image feature. (N, num_patch, C)\\n4:# sent_feats: sentence-wise text feature. list of N tensors, (num_sent, C)\\n5:def SLA_loss(patch_feats, sent_feats):\\n6: t2v_scores = [] # cV: visual localization correspondence\\n7: v2t_scores = [] # cT: textual localization correspondence\\n8: patch_feats = normalize(fp(patch_feats))\\n9: # Each report may have different num_sent\\n10: for sent in sent_feats:\\n11: sent = normalize(fs(sent))'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='10: for sent in sent_feats:\\n11: sent = normalize(fs(sent))\\n12: score = torch.bmm(path_feats, sent.T) # (N, num_patch, num_sent)\\n13: # Visual localization: Max over patches + Avg over sentences\\n14: t2v_scores.append(score.max(dim=1, keepdim=True).mean(dim=2).squeeze())\\n15: # Textual localization: Max over sentences + Avg over patches\\n16: v2t_scores.append(score.max(dim=2, keepdim=True).mean(dim=1).squeeze())\\n17: t2v_scores = torch.stack(t2v_scores, dim=0) / tau_local # (N, N)\\n18: v2t_scores = torch.stack(v2t_scores, dim=0) / tau_local # (N, N)\\n19: labels = torch.arange(N)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='19: labels = torch.arange(N)\\n20: loss0 = cross_entropy(t2v_scores, labels)\\n21: loss1 = cross_entropy(v2t_scores, labels)\\n22: return 0.5 * (loss0 + loss1)\\nA.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='-training Implementation Details\\nDataset and Pre-processing As mentioned in Sec. 4.1, we use the EMBED [ 21] dataset for pre-\\ntraining. We only use the 2D mammography and split the dataset into 70%/10%/20% for training,\\nvalidation, and testing at the patient level. We filter out the studies for males or those that have\\nmissing BI-RADS or density labels. We provide the detailed distribution of BI-RADS score and\\n14'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 14}, page_content='Table 7: Model Trainable Parameters We provide the number of trainable parameters for each'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 14}, page_content='model here below. Our method as described in the main paper is shaded in gray.\\nModels#Trainable Parameters (M)\\nVisual Encoder Language Encoder Total\\nVision only\\nRandom-ViT [13] 89.6 - 86.6\\nDiNOv2-ViT [37] 89.6 - 86.6\\nDeiT-based [48]\\nCLIP [38] 86.6 84.6 172.5\\nConVIRT [56] 86.6 84.6 173.2\\nMGCA [51] 86.6 84.6 174.4\\nDiNOv2-based [37]\\nCLIP [38] 89.6 84.6 174.5\\nSLIP [34] 89.6 84.6 174.8\\nMM-MIL [52] 89.6 84.6 174.9\\nConVIRT [56] 89.6 84.6 176.2\\nMGCA [51] 89.6 84.6 177.4\\nMaMA-BioClinicalBERT [2] 89.6 84.6 177.5\\nMaMA-LoRA-BioMedLM [18, 3] 89.6 2.6 92.8\\nMaMA-LoRA-Meditron [18, 7] 89.6 4.2 94.3\\nMaMA-LoRA-Llama3 [18, 1] 89.6 3.4 93.4\\nBreast density in Fig. 3, displaying the extremely imbalanced labels. Each of the sampled splits\\nshares roughly the same distribution. More details about the dataset can be found in [ 21]. For the\\ndata pre-processing, we first convert each original DICOM image file to JPEG format and resize the\\nimage based on its long side to 1,024 pixels without changing its aspect ratio. These images are then\\nused directly for training.\\nPre-training Data Augmentation Different from CLIP [ 39], we use a strong data augmentation\\nduring the pre-training stage for both images. We first apply the OTSU threshold masking to cut the\\nunnecessary background regions and only keep the breast tissue. This image is then resized to 518\\npixels on its long side and padded with zeros on the short side to have a square shape of 518,×518.\\nWe then apply SimCLR [ 5] style augmentation including random horizontal and vertical flips, color\\njitter, grayscale, and Gaussian blur. During test time, we only keep the resize operation and drop all\\nrandom augmentations.\\nModel Details As mentioned in Sec. 4.1, we use DiNOv2 pre-trained ViT-B-reg [ 13,8] model with\\nimage size 518 and patch size 14 as our visual encoder. We use BioMedLM [ 3], a 3M level GPT-2\\ndecoder-only transformer of 32 layers as our language encoder. We adapt LoRA [ 18] to fine-tune\\nthis encoder. As for the baselines, we choose to experiment with both a DeiT [ 48]-based and a\\nDiNOv2-based visual encoder. The DeiT-based transformer was pre-trained with a patch size of 16\\nand image size of 384 on ImageNet [ 10]. The input for the corresponding baselines is resized to 384\\nas well. For the DiNOv2 [ 37] visual encoder for the baselines, the setting is the same as our model.\\nAll the baselines use BioClinicalBERT [ 2], a BERT-style encoder-only transformer without PEFT.\\nEMBED Dataset Distribution\\nFigure 3: Data Distribution of EMBED [ 21] Dataset . We visualize the data distribution of the\\nEMBED [21] dataset for both BI-RADS and Density labels.\\n15'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 14}, page_content='Table 7: Model Trainable Parameters We provide the number of trainable parameters for each\\nmodel here below. Our method as described in the main paper is shaded in gray.\\nModels#Trainable Parameters (M)\\nVisual Encoder Language Encoder Total\\nVision only\\nRandom-ViT [13] 89.6 - 86.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 14}, page_content='. 3, displaying the extremely imbalanced labels. Each of the sampled splits\\nshares roughly the same distribution. More details about the dataset can be found in [ 21]. For the\\ndata pre-processing, we first convert each original DICOM image file to JPEG format and resize the\\nimage based on its long side to 1,0'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 14}, page_content='. These images are then\\nused directly for training.\\nPre-training Data Augmentation Different from CLIP [ 39], we use a strong data augmentation\\nduring the pre-training stage for both images. We first apply the OTSU threshold masking to cut the\\nunnecessary background regions and only keep the breast tissue. This image is then resized to 5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 14}, page_content='518,×518.\\nWe then apply SimCLR [ 5] style augmentation including random horizontal and vertical flips, color\\njitter, grayscale, and Gaussian blur. During test time, we only keep the resize operation and drop all\\nrandom augmentations.\\nModel Details As mentioned in Sec. 4.1, we use DiNOv'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 14}, page_content='. We adapt LoRA [ 18] to fine-tune\\nthis encoder. As for the baselines, we choose to experiment with both a DeiT [ 48]-based and a\\nDiNOv2-based visual encoder. The DeiT-based transformer was pre-trained with a patch size of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 14}, page_content='. For the DiNOv2 [ 37] visual encoder for the baselines, the setting is the same as our model.\\nAll the baselines use BioClinicalBERT [ 2], a BERT-style encoder-only transformer without PEFT.\\nEMBED Dataset Distribution\\nFigure 3: Data Distribution of EMBED [ 21] Dataset . We visualize the data distribution of the\\nEMBED [21] dataset for both BI-RADS and Density labels.\\n15'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='Full Fine-tuned Model Confusion Matrix'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='Figure 4: Confusion Matrix of Our Full Fine-tuned Model . We visualize the class-wise confusion\\nmatrix of our model fully fine-tuned with BI-RADS and density classification tasks, respectively.\\nWe use the online implementation for ConVIRT [ 56] and MGCA [ 51]2and adjust the vision encoder\\npart, and we re-implement the CLIP [ 39], SLIP [ 34], and MM-MIL [ 52] following the corresponding\\npapers under our environment. We provide the model size comparison in Tab. 7. We can easily see\\nthat our model has the smallest number of trainable parameters, only ∼52% compared with other\\nbaselines. We choose to use the last checkpoint for all models in downstream evaluations.\\nPEFT Settings As for the parameter-efficient fine-tuning (PEFT) module, we use the LoRA\\nimplemented by HuggingFace with default hyperparameters: r= 8,α= 32 ,dropout = 0.1. We\\nchoose to use LoRA as it is one of the most popular PEFT methods and has been proven to be\\neffective in prior research.\\nOverall Pre-training Target As a supplement to the method section, we here provide the overall\\npre-training optimization target in Eq. (4).\\nL(xi,˜vi, ti) =LV V(vi,˜vi) +LV T(vi, ti) +LV T(˜vi, ti) +wLlocal. (4)\\nWe set w= 0.0in the first 8,000 steps of training and w= 1.0in the latter process.\\nA.5 Downstream Evaluation Details\\nZero-shot Caption During zero-shot evaluation, we prepend the meta-information to the class-wise\\ndescription sentence, since this meta-information can be readily obtained with the images without\\nneeding the clinician’s diagnosis. More specifically, we prepend the information including: Procedure\\nreported ,Reason for procedure ,Patient info , and Image info before the class description sentence\\nof each BI-RADS or density class. This improves the zero-shot balanced accuracy of the BI-RADS\\nclassification from 29.65% to 31.04% and improves the corresponding AUC from 68.05% to 74.83%.\\nLinear Classifier We attach a linear classifier to each of the baseline models for linear classification\\nand full fine-tuned tasks. The linear classifier uses the average of all patch tokens as input rather than\\nusing [CLS] token since the [CLS] token is not used during training as well. We use the full training\\nset and balanced weighted sampling during training for all the linear classification and fine-tuning\\nexperiments.\\nBI-RADS Prediction For EMBED [ 21] BI-RADS score prediction task, we sample 10% data\\nrandomly from the test set. However, we added more images for BI-RADS scores 5 and 6 to ensure\\nthese 2 classes at least have 200 images. This is to avoid bias due to limited evaluation samples. The\\nfinal distribution of this dataset is: [901, 4472, 1166, 517, 210, 200, 200] for BI-RADS scores from 0\\nto 6 respectively. The pre-processing is the same as described in Appendix A.4.\\n2https://github.com/HKU-MedAI/MGCA\\n16'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='Full Fine-tuned Model Confusion Matrix\\nFigure 4: Confusion Matrix of Our Full Fine-tuned Model . We visualize the class-wise confusion\\nmatrix of our model fully fine-tuned with BI-RADS and density classification tasks, respectively.\\nWe use the online implementation for ConVIRT [ 56] and MGCA [ 51]2and adjust the vision encoder\\npart, and we re-implement the CLIP [ 39], SLIP [ 34], and MM-MIL [ 52] following the corresponding\\npapers under our environment. We provide the model size comparison in Tab. 7. We can easily see'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='that our model has the smallest number of trainable parameters, only ∼52% compared with other\\nbaselines. We choose to use the last checkpoint for all models in downstream evaluations.\\nPEFT Settings As for the parameter-efficient fine-tuning (PEFT) module, we use the LoRA\\nimplemented by HuggingFace with default hyperparameters: r= 8,α= 32 ,dropout = 0.1. We\\nchoose to use LoRA as it is one of the most popular PEFT methods and has been proven to be\\neffective in prior research.\\nOverall Pre-training Target As a supplement to the method section, we here provide the overall'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='pre-training optimization target in Eq. (4).\\nL(xi,˜vi, ti) =LV V(vi,˜vi) +LV T(vi, ti) +LV T(˜vi, ti) +wLlocal. (4)\\nWe set w= 0.0in the first 8,0'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='Zero-shot Caption During zero-shot evaluation, we prepend the meta-information to the class-wise\\ndescription sentence, since this meta-information can be readily obtained with the images without\\nneeding the clinician’s diagnosis. More specifically, we prepend the information including: Procedure\\nreported ,Reason for procedure ,Patient info , and Image info before the class description sentence\\nof each BI-RADS or density class. This improves the zero-shot balanced accuracy of the BI-RADS\\nclassification from 29.65% to 31.04% and improves the corresponding AUC from 68.05% to 74.83%.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='Linear Classifier We attach a linear classifier to each of the baseline models for linear classification\\nand full fine-tuned tasks. The linear classifier uses the average of all patch tokens as input rather than\\nusing [CLS] token since the [CLS] token is not used during training as well. We use the full training\\nset and balanced weighted sampling during training for all the linear classification and fine-tuning\\nexperiments.\\nBI-RADS Prediction For EMBED [ 21] BI-RADS score prediction task, we sample 10% data\\nrandomly from the test set. However, we added more images for BI-RADS scores'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='. This is to avoid bias due to limited evaluation samples. The\\nfinal distribution of this dataset is: [901, 4472, 1166, 517, 210, 200, 200] for BI-RADS scores from'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='. The pre-processing is the same as described in Appendix A.4.\\n2https://github.com/HKU-MedAI/MGCA\\n16'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='Table 8: Ablation with Different Meta Masking Ratio on EMBED BI-RADS [ 21]We evaluate'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='the influence of using different meta-masking ratios on the input text during pre-training and test the\\nmodel on zero-shot settings. Our method as described in the main paper is shaded in gray.\\nModel Settings bACC(%) AUC(%)\\nm= 0.0 27.19 68.20\\nm= 0.2 29.52 71.23\\nm= 0.5 30.37 72.44\\nm= 0.2 31.04 74.83\\nTable 9: Ablation with Different Visual Contrastive Learning Style on EMBED [ 21]We evaluate\\nthe influence of using different visual contrastive pre-training schemes. We evaluate the zero-shot\\nand linear classification performance for each method. Our method as described in the main paper is\\nshaded in gray.\\nModel SettingsEMBED [21] BI-RADS EMBED [21] Density\\nZero-shot Linear classification Zero-shot Linear Probing\\nbACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%)\\nSimCLR [5] style 31.04 74.83 39.75 77.50 75.40 93.46 78.09 93.65\\nMoCo [17] style 29.04 74.67 36.74 78.16 76.18 92.58 78.03 93.49\\nDensity Prediction Similar to BI-RADS prediction, we randomly sample another 10% data from\\nthe test set stratified by density label to create the density prediction set. The distribution of this\\ntest set is: [738, 3103, 3043, 417] for density from 1 to 4. We use the full training set and balanced\\nweighted sampling during training.\\nRSNA-Mammo [ 4] Cancer Detection Similar to EMBED pre-processing, we convert the DICOM\\nmammography to a JPEG image and resize its long side to 1,024 without changing the aspect ratio.\\nSince this dataset does not provide the corresponding meta-information, we only evaluate the linear\\nclassification and full fine-tuning tasks. We use the full 15% test set for the RSNA-Mammo [ 4]\\nevaluation, where the distribution of this test set is [7979, 208] for normal and cancerous samples,\\nrespectively. We use the full training set and balanced weighted sampling during training.\\nA.6 Classification Results Analysis\\nWe visualize the confusion matrix for classification results of the fully fine-tuned model on both\\nEMBED [ 21] prediction tasks in Fig. 4. While the overall accuracy for the BI-RADS prediction task\\nstill needs improvement, we note that the misclassification mainly happens for BI-RADS categories\\n2, 3, and 4, which is reasonable since these classes are semantically close to each other (“Benign”,\\n“Probably Benign”, and “Suspicious Abnormality”). Meanwhile, we note our model shows a high\\nrecall for BI-RADS category 6, i.e., “Known biopsy-proven malignancy”, which indicates the\\npotential application of the model to filter out high-risk abnormal mammography quickly.\\nMisclassifications for the density predictions are also reasonable, as mammographic density increases\\nwith the higher density class label. Notably, most errors for the middle two density classes are for\\nthe more extreme version of that class (e.g., 3 corresponding to \"heterogeneously dense\" is more\\noften mistaken for 4 \"extremely dense\" compared to 2 \"scattered density\"); thus the binary dense\\n(labels 3/4) and non-dense (labels 1/2) prediction does well. This is important as women with dense\\nbreasts are required to be notified by US regulations, and this has ramifications for potential follow-up\\nscreening recommendations.\\nA.7 Additional Ablation Experiments\\nMeta Masking Ratio To better understand the influence of masking the meta-information, we here\\nprovide an extra zero-shot evaluation on different mask ratios mduring the pre-training stage in\\nTab. 8. As shown above, the zero-shot performance increases as the meta-information masking ratio\\nincreases, which means the model tends to rely more on clinical-related information, and therefore,\\ndoes better in the zero-shot classification task.\\n17'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='Table 8: Ablation with Different Meta Masking Ratio on EMBED BI-RADS [ 21]We evaluate\\nthe influence of using different meta-masking ratios on the input text during pre-training and test the\\nmodel on zero-shot settings. Our method as described in the main paper is shaded in gray.\\nModel Settings bACC(%) AUC(%)\\nm= 0.0 27.19 68.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='9: Ablation with Different Visual Contrastive Learning Style on EMBED [ 21]We evaluate\\nthe influence of using different visual contrastive pre-training schemes. We evaluate the zero-shot\\nand linear classification performance for each method. Our method as described in the main paper is\\nshaded in gray.\\nModel SettingsEMBED [21] BI-RADS EMBED [21] Density\\nZero-shot Linear classification Zero-shot Linear Probing\\nbACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%)\\nSimCLR [5] style 31.04 74.83 39.75 77.50 75.40 93.46 78.09 93.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='-RADS prediction, we randomly sample another 10% data from\\nthe test set stratified by density label to create the density prediction set. The distribution of this\\ntest set is: [738, 3103, 3043, 417] for density from'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='4. We use the full training set and balanced\\nweighted sampling during training.\\nRSNA-Mammo [ 4] Cancer Detection Similar to EMBED pre-processing, we convert the DICOM\\nmammography to a JPEG image and resize its long side to 1,0'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='.\\nSince this dataset does not provide the corresponding meta-information, we only evaluate the linear\\nclassification and full fine-tuning tasks. We use the full 15% test set for the RSNA-Mammo [ 4]\\nevaluation, where the distribution of this test set is [7979, 208] for normal and cancerous samples,\\nrespectively. We use the full training set and balanced weighted sampling during training.\\nA.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='We visualize the confusion matrix for classification results of the fully fine-tuned model on both\\nEMBED [ 21] prediction tasks in Fig. 4. While the overall accuracy for the BI-RADS prediction task\\nstill needs improvement, we note that the misclassification mainly happens for BI-RADS categories\\n2, 3, and 4, which is reasonable since these classes are semantically close to each other (“Benign”,\\n“Probably Benign”, and “Suspicious Abnormality”). Meanwhile, we note our model shows a high\\nrecall for BI-RADS category 6, i.e., “Known biopsy-proven malignancy”, which indicates the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='potential application of the model to filter out high-risk abnormal mammography quickly.\\nMisclassifications for the density predictions are also reasonable, as mammographic density increases\\nwith the higher density class label. Notably, most errors for the middle two density classes are for\\nthe more extreme version of that class (e.g.,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='\"heterogeneously dense\" is more\\noften mistaken for 4 \"extremely dense\" compared to 2 \"scattered density\"); thus the binary dense\\n(labels 3/4) and non-dense (labels 1/2) prediction does well. This is important as women with dense\\nbreasts are required to be notified by US regulations, and this has ramifications for potential follow-up\\nscreening recommendations.\\nA.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='Meta Masking Ratio To better understand the influence of masking the meta-information, we here\\nprovide an extra zero-shot evaluation on different mask ratios mduring the pre-training stage in\\nTab. 8. As shown above, the zero-shot performance increases as the meta-information masking ratio\\nincreases, which means the model tends to rely more on clinical-related information, and therefore,\\ndoes better in the zero-shot classification task.\\n17'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='Table 10: Ablation with Different Multi-view Contrastive Learning Probability on EMBED [ 21]'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='We evaluate the influence of using different multi-view contrastive learning probabilities pon EMBED\\nBI-RADS prediction. We evaluate the zero-shot and linear classification performance for each pre-\\ntrained model. Our method as described in the main paper is shaded in gray.\\nModel SettingsEMBED [21] BI-RADS\\nZero-shot Linear Probing\\nbACC (%) AUC (%) bACC (%) AUC (%)\\np= 0.0 30.48 73.95 39.70 77.23\\np= 0.2 30.26 73.35 39.37 77.50\\np= 0.5 31.04 74.83 39.75 77.50\\np= 0.8 30.76 74.26 39.41 77.45\\np= 1.0 29.33 73.21 38.20 77.49\\nTable 11: Comparison with Medical Pre-trained Visual Encoder on EMBED [ 21]We compare\\nour method with SimCLR [ 5] pre-trained visual encoder on the EMEBD [ 21] dataset under linear\\nclassification settings. Our method as described in the main paper is shaded in gray.\\nModel SettingsEMBED [21] BIRADS EMBED Density\\nbACC (%) AUC (%) bACC (%) AUC (%)\\nSimCLR Pre-trained 26.19 65.06 77.06 92.64\\nMaMA 39.75 77.50 78.09 93.65\\nDifferent Visual Contrastive Learning Scheme We here provide additional analysis of the\\ninfluence of using different visual contrastive learning schemes by comparing a variation of the\\nproposed model, i.e., MoCo-style image-to-image contrastive loss [ 17], where a memory queue\\nof size 4096 is used to store the negative samples during pre-training. This can properly address\\nthe sensitivity of the image-to-image contrastive loss to the batch size, as there will always be a\\nlarge number of negative examples during pre-training (see Tab. 2 in [ 17], where a batch size of\\n256 was sufficient). Here, we provide a comparison between the proposed method (SimCLR style\\nimage-to-image loss) and MoCo-style variation in Tab. 9.\\nWe note that there is no clear difference between the two models. The chosen SimCLR method\\nis slightly better from a general perspective. This result potentially suggests that the batch size\\nmay not be that important in our task, or that the used batch size was large enough. We provide\\ntwo possible explanations for this result: 1) Different from natural images, where the difference\\nbetween each sample is fairly large, the inter-sample difference for mammograms is much smaller.\\nMammography generally has very similar global content. Thus, fewer negative samples are sufficient\\nto provide a robust contrastive signal during image-to-image contrastive pre-training. 2) Apart from\\nthe image-to-image loss, the symmetric image-to-text loss between the caption and two images also\\nindirectly minimizes the distance between the two images, which helps alleviate the necessity of a\\nlarge batch size.\\nDifferent Multi-view Probability Additionally, we here provide more analysis on the multi-view\\nsampling strategy. We adjust the probability of using intra-study sampling and the augmented view of\\nthe same image as the extra image ˜xi, which is p= 0.5in the proposed method. When p= 0.0, the\\nmodel always samples the same augmented image as the other view during pre-training (equivalent\\nto the \"Single Image\" baseline in Tab. 5). In contrast, when p= 1.0, the model always samples\\none of the other images from the same study as the other view. We here provide the results of the\\nZero-shot and Linear classification BI-RADS prediction evaluation in Tab. 10. It is clear that either\\nusing no inter-study sampling ( p= 0.0) or using only the multi-view sampling ( p= 1.0) will harm\\nthe performance. An equal-weight mix of both sampling methods shows the best performance, as it\\nprovides a more diverse contrastive image and reduces the potential contradictory image pairs (by\\nusing the augmented view of the same image).\\nVisual Constrastive Only Baseline We here include the linear classification results in comparison\\nto the ViT baseline pre-trained with the SimCLR [ 5] method on the EMBED dataset in Tab. 11. The\\nvision-only pre-trained model performs worse compared with our method according to the results.\\n18'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='Table 10: Ablation with Different Multi-view Contrastive Learning Probability on EMBED [ 21]\\nWe evaluate the influence of using different multi-view contrastive learning probabilities pon EMBED\\nBI-RADS prediction. We evaluate the zero-shot and linear classification performance for each pre-\\ntrained model. Our method as described in the main paper is shaded in gray.\\nModel SettingsEMBED [21] BI-RADS\\nZero-shot Linear Probing\\nbACC (%) AUC (%) bACC (%) AUC (%)\\np= 0.0 30.48 73.95 39.70 77.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='11: Comparison with Medical Pre-trained Visual Encoder on EMBED [ 21]We compare\\nour method with SimCLR [ 5] pre-trained visual encoder on the EMEBD [ 21] dataset under linear\\nclassification settings. Our method as described in the main paper is shaded in gray.\\nModel SettingsEMBED [21] BIRADS EMBED Density\\nbACC (%) AUC (%) bACC (%) AUC (%)\\nSimCLR Pre-trained 26.19 65.06 77.06 92.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='influence of using different visual contrastive learning schemes by comparing a variation of the\\nproposed model, i.e., MoCo-style image-to-image contrastive loss [ 17], where a memory queue\\nof size 40'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='-training. This can properly address\\nthe sensitivity of the image-to-image contrastive loss to the batch size, as there will always be a\\nlarge number of negative examples during pre-training (see Tab.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='). Here, we provide a comparison between the proposed method (SimCLR style\\nimage-to-image loss) and MoCo-style variation in Tab. 9.\\nWe note that there is no clear difference between the two models. The chosen SimCLR method\\nis slightly better from a general perspective. This result potentially suggests that the batch size\\nmay not be that important in our task, or that the used batch size was large enough. We provide\\ntwo possible explanations for this result: 1) Different from natural images, where the difference'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='between each sample is fairly large, the inter-sample difference for mammograms is much smaller.\\nMammography generally has very similar global content. Thus, fewer negative samples are sufficient\\nto provide a robust contrastive signal during image-to-image contrastive pre-training. 2) Apart from\\nthe image-to-image loss, the symmetric image-to-text loss between the caption and two images also\\nindirectly minimizes the distance between the two images, which helps alleviate the necessity of a\\nlarge batch size.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='large batch size.\\nDifferent Multi-view Probability Additionally, we here provide more analysis on the multi-view\\nsampling strategy. We adjust the probability of using intra-study sampling and the augmented view of\\nthe same image as the extra image ˜xi, which is p= 0.5in the proposed method. When p= 0.0, the\\nmodel always samples the same augmented image as the other view during pre-training (equivalent\\nto the \"Single Image\" baseline in Tab. 5). In contrast, when p= 1.0, the model always samples\\none of the other images from the same study as the other view. We here provide the results of the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='Zero-shot and Linear classification BI-RADS prediction evaluation in Tab. 10. It is clear that either\\nusing no inter-study sampling ( p= 0.0) or using only the multi-view sampling ( p= 1.0) will harm\\nthe performance. An equal-weight mix of both sampling methods shows the best performance, as it\\nprovides a more diverse contrastive image and reduces the potential contradictory image pairs (by\\nusing the augmented view of the same image).\\nVisual Constrastive Only Baseline We here include the linear classification results in comparison'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='to the ViT baseline pre-trained with the SimCLR [ 5] method on the EMBED dataset in Tab. 11. The\\nvision-only pre-trained model performs worse compared with our method according to the results.\\n18'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='Table 12: Linear Classification Results on EMBED [ 21] for Different Text Encoder We evaluate'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='linear classification results with different amounts of fine-tuning data for both BI-RADS and density\\nprediction tasks of our model with different text encoder. All methods are based on DiNOv2 [ 37]\\nvision encoder for a fair comparison. We report both balanced accuracy (bACC) and AUC metrics.\\nThe best and second-best results are highlighted in bold and underlined respectively. Our method as\\ndescribed in the main paper is shaded in gray.\\nModelsEMBED BI-RADS [21] EMBED Density [21]\\nbACC (%) AUC (%) bACC (%) AUC (%)\\n1% 10% 100% 1% 10% 100% 1% 10% 100% 1% 10% 100%\\nBioClinicalBERT-based [2]\\nCLIP [38] 26.66 31.65 34.35 70.35 74.98 74.11 74.64 75.00 75.97 91.50 90.62 92.39\\nSLIP [34] 22.94 27.86 30.93 64.43 69.48 71.95 73.24 74.79 75.23 91.56 92.37 92.46\\nMM-MIL [52] 25.85 30.94 35.11 67.16 71.99 76.12 74.23 76.69 75.77 91.96 93.34 91.65\\nConVIRT [56] 24.62 30.38 31.27 65.09 73.33 74.03 74.34 74.95 74.74 92.21 92.56 92.58\\nMGCA [51] 23.66 30.11 30.27 64.19 72.24 72.54 71.43 72.25 72.20 90.83 91.21 91.24\\nMaMA-BERT 27.81 34.25 38.96 68.99 74.61 77.43 74.77 77.50 78.15 92.90 93.50 93.68\\nLoRA-LLM-based [18]\\nMaMA-BioMedLM 28.46 35.12 39.75 70.63 75.98 77.50 76.26 78.11 78.09 93.11 93.62 93.65\\nMaMA-Meditron 26.94 33.28 38.68 68.93 74.45 77.51 74.48 77.77 78.30 92.65 93.54 93.66\\nMaMA-Llama3 28.00 34.30 39.99 70.83 75.47 77.50 74.70 77.93 78.13 93.02 93.70 93.72\\nA.8 Benchmark Different Text Encoders\\nWe evaluate all methods with the same DiNOv2 [ 37] vision encoders but compare the influence of\\nusing different text encoders in Tab. 12.\\nText Encoders 1)BioClinicalBERT [2]: The standard text encoder used for previous medical CLIP\\nmodels [ 52,56,19,51,50] and also our baseline methods, which is a BERT [ 11]-style transformer\\npre-trained with MIMIC-III [ 23] clinical report. 2) BioMedLM [3]: A 2.7B level GPT-2 [ 39]\\ntransformer pre-trained with PubMed data, which is also one of the best 3B LLM according to\\nmultiple benchmarks [ 7]. 3) Meditron-7B [7]: A newly released Llama2 [ 49] model fine-tuned with\\nPubMed papers. 4) Llama3-8B [1]: Recently released, the most robust open-souced LLM, with\\nroughly the same architecture as Llama2 [ 49] but pre-trained with much more data. All the latter\\nthree LLMs are fine-tuned with LoRA [18]\\nResults We report the results on linear classification in Tab. 12. We note that even our model with\\nBioClinicalBERT [ 2] text encoder outperforms all the baselines in this evaluation; this demonstrates\\nthe effectiveness of the proposed multi-view mammography pre-training and symmetric local align-\\nment module. Comparing three different LLMs with LoRA [ 18], we note that BioMedLM [ 3] and\\nLlama3-8B [ 1] roughly have a similar level of performance, while the BioMedLM-based model has a\\nsmaller GPU memory cost and faster training speed due to its relative size. Meanwhile, we notice\\nthat the Meditron [ 7]-based model is not as good as the other two LLMs, but all these LLM-based\\nmethods outperform the model with smaller BERT-style [ 11] encoder in general. Overall, our choice\\nof BioMedLM [3]-based model has the best balance between performance and model size.\\nA.9 Local Similarity Map Analysis\\nWe visualize the learned local patch-sentence similarity map in Fig. 5. As described in Sec. 3.3, the\\nlocal patch-sentence similarity map indicates the relationship between each region of the image and\\nthe corresponding input sentence. We visualize the similarity map for the “Impression” sentence in\\nthe report (see examples in Fig. 6 to Fig. 8), which includes the most important diagnosis information.\\nWe also visualize the same similarity map for MM-MIL [ 52] and a variation of our method that\\noptimizes local similarity with only visual localization (similar to including the MM-MIL local\\nbranch).\\nWe note that our methods generally have a better localization quality with more fine-grained details.\\nThe model can accurately locate the high-density and tumor-related regions in the given maps.\\nWe also see from the examples for patients 3 and 4 that our method has a better correspondence\\nbetween mammograms from different views or sides. Especially for column 3, our method accurately\\nidentified the same region in both views, while the baseline method failed to locate the tissue in the\\nRMLO view (left image). The MM-MIL [ 52] model even failed to detect the tumor for patient 4.\\nOn the other hand, the variation of our model that optimizes only visual localization loss can only\\n19'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='Table 12: Linear Classification Results on EMBED [ 21] for Different Text Encoder We evaluate\\nlinear classification results with different amounts of fine-tuning data for both BI-RADS and density\\nprediction tasks of our model with different text encoder. All methods are based on DiNOv2 [ 37]\\nvision encoder for a fair comparison. We report both balanced accuracy (bACC) and AUC metrics.\\nThe best and second-best results are highlighted in bold and underlined respectively. Our method as\\ndescribed in the main paper is shaded in gray.\\nModelsEMBED BI-RADS [21] EMBED Density [21]'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='ModelsEMBED BI-RADS [21] EMBED Density [21]\\nbACC (%) AUC (%) bACC (%) AUC (%)\\n1% 10% 100% 1% 10% 100% 1% 10% 100% 1% 10% 100%\\nBioClinicalBERT-based [2]\\nCLIP [38] 26.66 31.65 34.35 70.35 74.98 74.11 74.64 75.00 75.97 91.50 90.62 92.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='We evaluate all methods with the same DiNOv2 [ 37] vision encoders but compare the influence of\\nusing different text encoders in Tab. 12.\\nText Encoders 1)BioClinicalBERT [2]: The standard text encoder used for previous medical CLIP\\nmodels [ 52,56,19,51,50] and also our baseline methods, which is a BERT [ 11]-style transformer\\npre-trained with MIMIC-III [ 23] clinical report. 2) BioMedLM [3]: A 2.7B level GPT-2 [ 39]\\ntransformer pre-trained with PubMed data, which is also one of the best 3B LLM according to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='multiple benchmarks [ 7]. 3) Meditron-7B [7]: A newly released Llama2 [ 49] model fine-tuned with\\nPubMed papers. 4) Llama3-8B [1]: Recently released, the most robust open-souced LLM, with\\nroughly the same architecture as Llama2 [ 49] but pre-trained with much more data. All the latter\\nthree LLMs are fine-tuned with LoRA [18]\\nResults We report the results on linear classification in Tab. 12. We note that even our model with\\nBioClinicalBERT [ 2] text encoder outperforms all the baselines in this evaluation; this demonstrates'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='the effectiveness of the proposed multi-view mammography pre-training and symmetric local align-\\nment module. Comparing three different LLMs with LoRA [ 18], we note that BioMedLM [ 3] and\\nLlama3-8B [ 1] roughly have a similar level of performance, while the BioMedLM-based model has a\\nsmaller GPU memory cost and faster training speed due to its relative size. Meanwhile, we notice\\nthat the Meditron [ 7]-based model is not as good as the other two LLMs, but all these LLM-based\\nmethods outperform the model with smaller BERT-style [ 11] encoder in general. Overall, our choice'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='of BioMedLM [3]-based model has the best balance between performance and model size.\\nA.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='We visualize the learned local patch-sentence similarity map in Fig. 5. As described in Sec. 3.3, the\\nlocal patch-sentence similarity map indicates the relationship between each region of the image and\\nthe corresponding input sentence. We visualize the similarity map for the “Impression” sentence in\\nthe report (see examples in Fig.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='. 8), which includes the most important diagnosis information.\\nWe also visualize the same similarity map for MM-MIL [ 52] and a variation of our method that\\noptimizes local similarity with only visual localization (similar to including the MM-MIL local\\nbranch).\\nWe note that our methods generally have a better localization quality with more fine-grained details.\\nThe model can accurately locate the high-density and tumor-related regions in the given maps.\\nWe also see from the examples for patients'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='between mammograms from different views or sides. Especially for column 3, our method accurately\\nidentified the same region in both views, while the baseline method failed to locate the tissue in the\\nRMLO view (left image). The MM-MIL [ 52] model even failed to detect the tumor for patient 4.\\nOn the other hand, the variation of our model that optimizes only visual localization loss can only\\n19'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 19}, page_content='MM‐MIL'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 19}, page_content='Only\\xa0Visual\\xa0Local. MaMA (Ours)Patient \\xa0#1 Patient \\xa0#2 Patient \\xa0#4\\xa0(two\\xa0sides) Patient \\xa0#3\\xa0(two\\xa0views)Figure 5: Visualization of Local Similarity Maps over Input Mammograms . We visualize the\\nlearned local similarity map for the “Impressions” sentence on a few test mammograms from the\\nEMBED dataset [ 21] for MM-MIL [ 52], our method with only visual localization, and our full\\nmethod here. All the heat maps are normalized to [0,1]. The third column shows mammograms from\\nthe same side but a different view and the fourth column shows mammograms from the same view but\\nfrom a different side. The white box in the image represents the ROI annotated from the dataset [ 21].\\nTable 13: Zero-shot Visual Grounding Analysis We report the mean intersection-over-union\\n(mIoU), mean DICE score, and ROI recall with 50% coverage for methods with local sentence-region\\nsimilarity map on the EMBED [21] dataset. Our method is shaded in gray.\\nModelsEMBED [21] Visual Grounding\\nmIoU (%) mDICE (%) Recall (%)\\nMM-MIL [52] 5.25 9.72 39.23\\nMaMA 6.22 11.88 47.67\\nprovide a vague and inaccurate similarity map. We believe this is because the asymmetric max and\\naverage pooling operation drops too much information during training, resulting in only one of the\\npatches being optimized.\\nQuantitative Visual Grounding Analysis Similar to the analysis in MM-MIL [ 52], we further\\nconduct a zero-shot visual grounding analysis with the pre-trained model. We compare the similarity\\nmap extracted for the image and the “Impressions” description with the provided ROIs from a subset\\nof the EMBED [ 21] dataset, which contains 841 images from the test split, each with one or more\\nROI annotations. We report the mean intersection-over-union (mIoU), mean DICE (mDICE) score,\\nand ROI recall for both the MM-MIL [ 52] method and ours. Different from Wang et al. [52], we\\nuse a set of thresholds of [0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85]since the ROI is generally smaller\\nin the mammogram and needs a higher threshold to have better detection results. We compute IoU\\n20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 19}, page_content='#4\\xa0(two\\xa0sides) Patient \\xa0#3\\xa0(two\\xa0views)Figure 5: Visualization of Local Similarity Maps over Input Mammograms . We visualize the\\nlearned local similarity map for the “Impressions” sentence on a few test mammograms from the\\nEMBED dataset [ 21] for MM-MIL [ 52], our method with only visual localization, and our full\\nmethod here. All the heat maps are normalized to [0,1]. The third column shows mammograms from\\nthe same side but a different view and the fourth column shows mammograms from the same view but'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 19}, page_content='from a different side. The white box in the image represents the ROI annotated from the dataset [ 21].\\nTable 13: Zero-shot Visual Grounding Analysis We report the mean intersection-over-union\\n(mIoU), mean DICE score, and ROI recall with 50% coverage for methods with local sentence-region\\nsimilarity map on the EMBED [21] dataset. Our method is shaded in gray.\\nModelsEMBED [21] Visual Grounding\\nmIoU (%) mDICE (%) Recall (%)\\nMM-MIL [52] 5.25 9.72 39.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 19}, page_content='. We believe this is because the asymmetric max and\\naverage pooling operation drops too much information during training, resulting in only one of the\\npatches being optimized.\\nQuantitative Visual Grounding Analysis Similar to the analysis in MM-MIL [ 52], we further\\nconduct a zero-shot visual grounding analysis with the pre-trained model. We compare the similarity\\nmap extracted for the image and the “Impressions” description with the provided ROIs from a subset\\nof the EMBED [ 21] dataset, which contains 8'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 19}, page_content=', each with one or more\\nROI annotations. We report the mean intersection-over-union (mIoU), mean DICE (mDICE) score,\\nand ROI recall for both the MM-MIL [ 52] method and ours. Different from Wang et al. [52], we\\nuse a set of thresholds of [0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85]since the ROI is generally smaller\\nin the mammogram and needs a higher threshold to have better detection results. We compute IoU\\n20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='Table 14: Linear Classification Bootstrap Results for Balanced Accuracy on EMBED [ 21]We'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='conduct the bootstrap evaluation for the linear classification predicted result of our method on both\\nBI-RADS and density prediction tasks. We sample N= 10,000bootstrapped samples and compute\\nthe average balanced Accuracy (bACC) with the corresponding 95% confidence interval for each\\nsetting. This illustrates the statistical stability of our method.\\nTaskbACC (%)\\n1% 10% 100%\\nEMBED BI-RADS [21] 28.46 [27.12,29.84] 35.11 [33.36,36.86] 39.75 [37.81,41.64]\\nEMBED Density [21] 76.25 [74.88,77.60] 78.11 [73.65,75.66] 78.10 [76.82,79.34]\\nTable 15: Linear Classification Bootstrap Results for AUC on EMBED [ 21]We conduct the\\nbootstrap evaluation for the linear classification predicted result of our method on both BI-RADS and\\ndensity prediction tasks. We sample N= 10,000bootstrapped samples and compute the average\\nAUC with the corresponding 95% confidence interval for each setting. This illustrates the statistical\\nstability of our method.\\nTaskAUC (%)\\n1% 10% 100%\\nEMBED BI-RADS [21] 70.64 [69.56,71.69] 75.98 [75.09,76.87] 77.50 [76.61,78.35]\\nEMBED Density [21] 93.11 [92.70,93.52] 93.62 [93.23,94.00] 93.65 [93.26,94.02]\\nand DICE scores for each threshold and then average them to get mIoU and mDICE. For ROI recall,\\nan ROI is considered successfully predicted when the overlap between the binarized similarity map\\n(with a fixed threshold of 50%) and the ROI is greater than 50%. Our method generally shows a better\\nperformance over the MM-MIL [ 52] model and achieves a recall near 50% without training. We note\\nthat the number reported here may look low since this is a parameter-free zero-shot evaluation, and\\nthe ROI in the mammography is generally small compared with the whole image, which makes the\\ntask more challenging.\\nA.10 Performance Statistical Analysis\\nWe further evaluate the stability of the proposed method by bootstrap sampling test set results from\\nlinear classification and report the 95% confidence interval in Tab. 14 and Tab. 15. Notably, our\\nmethod generally shows a small confidence interval, especially for AUC scores. Comparing our\\nresults with confidence interval with the baselines in Tab. 1, we see that there is still a marked\\nimprovement in performance.\\nA.11 Report Construction Template\\nWe provide here the template used to construct our structured image caption during training. We\\ndescribe each segment below, and the keywords wrapped with “{{” and “}}” will be replaced with\\ncorresponding information from the tabular data.\\n1.Procedure reported : {{PROCEDURE}}.\\n2.Reason for procedure : {{SCREENING/DIAGNOSTIC}}.\\n3.Patient info : This patient is {{RACE}}, {{ETHNIC}}, and {{AGE}} years old.\\n4.Image info : This is a {{IMAGE_TYPE}} full-field digital mammogram of the {{SIDE}}\\nbreast with {{VIEW}} view.\\n5.Breast composition : The breast is {{DENSITY_DESC}}.\\n6.Findings : The mammogram shows that {{MASS_DESC}}. The mass is {{SHAPE}} and\\n{{DENSITY}}. A {{DISTRI}} {{SHAPE}} calcification is present.\\n7.Impressions : BI-RADS Category {{BIRADS}}: {{BIRADS_DESC}}.\\n8.Overall Assessment : {{BIRADS_DESC}}\\nWe provide more details and corresponding description strings in our implementation file.\\n21'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='Table 14: Linear Classification Bootstrap Results for Balanced Accuracy on EMBED [ 21]We\\nconduct the bootstrap evaluation for the linear classification predicted result of our method on both\\nBI-RADS and density prediction tasks. We sample N= 10,000bootstrapped samples and compute\\nthe average balanced Accuracy (bACC) with the corresponding 95% confidence interval for each\\nsetting. This illustrates the statistical stability of our method.\\nTaskbACC (%)\\n1% 10% 100%\\nEMBED BI-RADS [21] 28.46 [27.12,29.84] 35.11 [33.36,36.86] 39.75 [37.81,41.64]'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='EMBED Density [21] 76.25 [74.88,77.60] 78.11 [73.65,75.66] 78.10 [76.82,79.34]\\nTable 15: Linear Classification Bootstrap Results for AUC on EMBED [ 21]We conduct the\\nbootstrap evaluation for the linear classification predicted result of our method on both BI-RADS and\\ndensity prediction tasks. We sample N= 10,000bootstrapped samples and compute the average\\nAUC with the corresponding 95% confidence interval for each setting. This illustrates the statistical\\nstability of our method.\\nTaskAUC (%)\\n1% 10% 100%\\nEMBED BI-RADS [21] 70.64 [69.56,71.69] 75.98 [75.09,76.87] 77.50 [76.61,78.35]'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='EMBED Density [21] 93.11 [92.70,93.52] 93.62 [93.23,94.00] 93.65 [93.26,94.02]\\nand DICE scores for each threshold and then average them to get mIoU and mDICE. For ROI recall,\\nan ROI is considered successfully predicted when the overlap between the binarized similarity map\\n(with a fixed threshold of 50%) and the ROI is greater than 50%. Our method generally shows a better\\nperformance over the MM-MIL [ 52] model and achieves a recall near 50% without training. We note\\nthat the number reported here may look low since this is a parameter-free zero-shot evaluation, and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='the ROI in the mammography is generally small compared with the whole image, which makes the\\ntask more challenging.\\nA.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='We further evaluate the stability of the proposed method by bootstrap sampling test set results from\\nlinear classification and report the 95% confidence interval in Tab.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='. 15. Notably, our\\nmethod generally shows a small confidence interval, especially for AUC scores. Comparing our\\nresults with confidence interval with the baselines in Tab. 1, we see that there is still a marked\\nimprovement in performance.\\nA.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='We provide here the template used to construct our structured image caption during training. We\\ndescribe each segment below, and the keywords wrapped with “{{” and “}}” will be replaced with\\ncorresponding information from the tabular data.\\n1.Procedure reported : {{PROCEDURE}}.\\n2.Reason for procedure : {{SCREENING/DIAGNOSTIC}}.\\n3.Patient info : This patient is {{RACE}}, {{ETHNIC}}, and {{AGE}} years old.\\n4.Image info : This is a {{IMAGE_TYPE}} full-field digital mammogram of the {{SIDE}}\\nbreast with {{VIEW}} view.\\n5.Breast composition : The breast is {{DENSITY_DESC}}.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='5.Breast composition : The breast is {{DENSITY_DESC}}.\\n6.Findings : The mammogram shows that {{MASS_DESC}}. The mass is {{SHAPE}} and\\n{{DENSITY}}. A {{DISTRI}} {{SHAPE}} calcification is present.\\n7.Impressions : BI-RADS Category {{BIRADS}}: {{BIRADS_DESC}}.\\n8.Overall Assessment : {{BIRADS_DESC}}\\nWe provide more details and corresponding description strings in our implementation file.\\n21'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 21}, page_content='A.12 Example Mammography Images with Captions'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 21}, page_content='We provide 7 randomly sampled mammography images with corresponding captions for each of the\\nBI-RADS categories in Fig. 6 to Fig. 8.\\n22'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 22}, page_content='•Procedure reported : MG Screen Bilat w/Tomo/CAD Stnd'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 22}, page_content='Protocol. \\n•Reason for procedure : screening. \\n•Patient info : This patient is African American  or Black, \\nNon-Hispanic or Latino, and 56 years old.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe right breast with MLO view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that an additional imaging is \\nrecommended.  \\n•Impressions : BI-RADS Category 0: additional imaging \\nrequired. \\n•Overall Assessment : Additional imaging is recommended.\\n•Procedure reported : MG Screen Bilat w/Tomo/CAD Stnd\\nProtocol. \\n•Reason for procedure : screening. \\n•Patient info : This patient is Caucasian or White, Non-\\nHispanic or Latino, and 50 years old.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe right breast with CC view.  \\n•Breast composition : The breast is heterogeneously dense. \\nThis may lower the sensitivity of mammography.  \\n•Findings : The mammogram shows that no significant masses, \\ncalcification, or other abnormalities are present.  \\n•Impressions : BI-RADS Category 1: negative. \\n•Overall Assessment : Negative.\\n•Procedure reported : MG Diagnostic Bilateral w/ CAD. \\n•Reason for procedure : diagnostic. \\n•Patient info : This patient is Caucasian or White, Non-\\nHispanic or Latino, and 67 years old.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe left breast with MLO view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a benign finding is \\npresent.  \\n•Impressions : BI-RADS Category 2: benign finding. \\n•Overall Assessment : Benign.Figure 6: Example Multi-view Mammography BI-RADS 0-2 with Constructed Caption . We\\nprovide random sampled multi-view mammography with the corresponding caption constructed by\\nus. We highlight the image match exactly with the caption in a green bounding box.\\n23'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 22}, page_content='•Procedure reported : MG Screen Bilat w/Tomo/CAD Stnd\\nProtocol. \\n•Reason for procedure : screening. \\n•Patient info : This patient is African American  or Black, \\nNon-Hispanic or Latino, and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 22}, page_content='.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe right breast with MLO view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that an additional imaging is \\nrecommended.  \\n•Impressions : BI-RADS Category 0: additional imaging \\nrequired. \\n•Overall Assessment : Additional imaging is recommended.\\n•Procedure reported : MG Screen Bilat w/Tomo/CAD Stnd\\nProtocol. \\n•Reason for procedure : screening. \\n•Patient info : This patient is Caucasian or White, Non-\\nHispanic or Latino, and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 22}, page_content='.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe right breast with CC view.  \\n•Breast composition : The breast is heterogeneously dense. \\nThis may lower the sensitivity of mammography.  \\n•Findings : The mammogram shows that no significant masses, \\ncalcification, or other abnormalities are present.  \\n•Impressions : BI-RADS Category 1: negative. \\n•Overall Assessment : Negative.\\n•Procedure reported : MG Diagnostic Bilateral w/ CAD. \\n•Reason for procedure : diagnostic. \\n•Patient info : This patient is Caucasian or White, Non-\\nHispanic or Latino, and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 22}, page_content='.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe left breast with MLO view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a benign finding is \\npresent.  \\n•Impressions : BI-RADS Category 2: benign finding. \\n•Overall Assessment : Benign.Figure 6: Example Multi-view Mammography BI-RADS 0-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 22}, page_content='. We\\nprovide random sampled multi-view mammography with the corresponding caption constructed by\\nus. We highlight the image match exactly with the caption in a green bounding box.\\n23'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 23}, page_content='•Procedure reported : MG Diagnostic Left w/CAD.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 23}, page_content='•Reason for procedure : diagnostic. \\n•Patient info : This patient is African American  or Black, \\nNon-Hispanic or Latino, and 80 years old.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe left breast with CC view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a probably benign \\nfinding is present.  A Grouped Coarse calcification is \\npresent.  \\n•Impressions : BI-RADS Category 3: probably benign finding. \\n•Overall Assessment : Probably benign.\\n•Procedure reported : MG Diagnostic Right w/CAD. \\n•Reason for procedure : diagnostic. \\n•Patient info : This patient is Caucasian or White, Non-\\nHispanic or Latino, and 41 years old.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe right breast with MLO view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a suspicious abnormality \\nis present.  \\n•Impressions : BI-RADS Category 4: suspicious abnormality. \\n•Overall Assessment : Suspicious abnormality.\\n•Procedure reported : MG Diagnostic Mammo Bilateral. \\n•Reason for procedure : diagnostic. \\n•Patient info : This patient is African American  or Black, \\nNon-Hispanic or Latino, and 59 years old.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe left breast with MLO view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a highly suggestive of \\nmalignancy is present, a biopsy is recommended.  \\n•Impressions : BI-RADS Category 5: highly suggestive of \\nmalignancy. \\n•Overall Assessment : Highly suggestive of malignancy.Figure 7: Example Multi-view Mammography BI-RADS 3-5 with Constructed Caption . We\\nprovide random sampled multi-view mammography with the corresponding caption constructed by\\nus. We highlight the image match exactly with the caption in a green bounding box.\\n24'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 23}, page_content='•Procedure reported : MG Diagnostic Left w/CAD. \\n•Reason for procedure : diagnostic. \\n•Patient info : This patient is African American  or Black, \\nNon-Hispanic or Latino, and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 23}, page_content='.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe left breast with CC view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a probably benign \\nfinding is present.  A Grouped Coarse calcification is \\npresent.  \\n•Impressions : BI-RADS Category 3: probably benign finding. \\n•Overall Assessment : Probably benign.\\n•Procedure reported : MG Diagnostic Right w/CAD. \\n•Reason for procedure : diagnostic. \\n•Patient info : This patient is Caucasian or White, Non-\\nHispanic or Latino, and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 23}, page_content='.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe right breast with MLO view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a suspicious abnormality \\nis present.  \\n•Impressions : BI-RADS Category 4: suspicious abnormality. \\n•Overall Assessment : Suspicious abnormality.\\n•Procedure reported : MG Diagnostic Mammo Bilateral. \\n•Reason for procedure : diagnostic. \\n•Patient info : This patient is African American  or Black, \\nNon-Hispanic or Latino, and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 23}, page_content='.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe left breast with MLO view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a highly suggestive of \\nmalignancy is present, a biopsy is recommended.  \\n•Impressions : BI-RADS Category 5: highly suggestive of \\nmalignancy. \\n•Overall Assessment : Highly suggestive of malignancy.Figure 7: Example Multi-view Mammography BI-RADS 3-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 23}, page_content='. We\\nprovide random sampled multi-view mammography with the corresponding caption constructed by\\nus. We highlight the image match exactly with the caption in a green bounding box.\\n24'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 24}, page_content='•Procedure reported : MG Diagnostic Bilateral w/ CAD.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 24}, page_content='•Reason for procedure : diagnostic. \\n•Patient info : This patient is African American  or Black, \\nNon-Hispanic or Latino, and 68 years old.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe left breast with CC view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a known biopsy-proven \\nmalignant mass is present.  \\n•Impressions : BI-RADS Category 6: known biopsy-proven \\nmalignancy. \\n•Overall Assessment : Known biopsy-proven malignancy.Figure 8: Example Multi-view Mammography BI-RADS 6 with Constructed Caption . We\\nprovide random sampled multi-view mammography with the corresponding caption constructed by\\nus. We highlight the image match exactly with the caption in a green bounding box.\\n25'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 24}, page_content='•Procedure reported : MG Diagnostic Bilateral w/ CAD. \\n•Reason for procedure : diagnostic. \\n•Patient info : This patient is African American  or Black, \\nNon-Hispanic or Latino, and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 24}, page_content='.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe left breast with CC view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a known biopsy-proven \\nmalignant mass is present.  \\n•Impressions : BI-RADS Category 6: known biopsy-proven \\nmalignancy. \\n•Overall Assessment : Known biopsy-proven malignancy.Figure 8: Example Multi-view Mammography BI-RADS'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 24}, page_content='. We\\nprovide random sampled multi-view mammography with the corresponding caption constructed by\\nus. We highlight the image match exactly with the caption in a green bounding box.\\n25'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 0}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 0}, page_content='LOTUS : DIFFUSION -BASED VISUAL FOUNDATION MODEL\\nFOR HIGH-QUALITY DENSE PREDICTION\\nJing He1✱Haodong Li1✱Wei Yin2Yixun Liang1Leheng Li1Kaiqiang Zhou3\\nHongbo Zhang3Bingbing Liu3Yingcong Chen1,4 \\x00\\n1HKUST(GZ)2University of Adelaide3Huawei Noah’s Ark Lab4HKUST\\n{jhe812, hli736 }@connect.hkust-gz.edu.cn; yingcongchen@ust.hk\\nDepthAnything  V2\\nDepthAnything  V2Lotus (Ours)\\nLotus (Ours)\\nLotus (Ours)\\nLotus (Ours)DSINE\\nDSINE\\nAvg. RankOmnidataDPTHDNGenPerceptDepthAnything V2DepthAnythingLotus-DGeoWizardMarigold(LCM)MarigoldLotus-GTraining DataTraining DataOASISOmnidataEESNUGenPerceptOmnidata V2Lotus-DDSINEMarigoldGeoWizardStableNormalLotus-GAvg. RankAvg. Rank\\nAvg. Rank\\nFigure 1: We present Lotus , a diffusion-based visual foundation model for dense geometry predic-\\ntion. With minimal training data, Lotus achieves SoTA performance in two key geometry perception\\ntasks, i.e., zero-shot depth and normal estimation. “Avg. Rank” indicates the average ranking across\\nall metrics, where lower values are better. Bar length represents the amount of training data used.\\n✱Both authors contributed equally (order randomized).\\x00Corresponding author.\\n1arXiv:2409.18124v1  [cs.CV]  26 Sep 2024'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 0}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nLOTUS : DIFFUSION -BASED VISUAL FOUNDATION MODEL\\nFOR HIGH-QUALITY DENSE PREDICTION\\nJing He1✱Haodong Li1✱Wei Yin2Yixun Liang1Leheng Li1Kaiqiang Zhou'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 0}, page_content='3Bingbing Liu3Yingcong Chen1,4 \\x00\\n1HKUST(GZ)2University of Adelaide3Huawei Noah’s Ark Lab4HKUST\\n{jhe812, hli736 }@connect.hkust-gz.edu.cn; yingcongchen@ust.hk\\nDepthAnything  V'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 0}, page_content='2Lotus (Ours)\\nLotus (Ours)\\nLotus (Ours)\\nLotus (Ours)DSINE\\nDSINE\\nAvg. RankOmnidataDPTHDNGenPerceptDepthAnything V2DepthAnythingLotus-DGeoWizardMarigold(LCM)MarigoldLotus-GTraining DataTraining DataOASISOmnidataEESNUGenPerceptOmnidata V2Lotus-DDSINEMarigoldGeoWizardStableNormalLotus-GAvg. RankAvg. Rank\\nAvg. Rank\\nFigure 1: We present Lotus , a diffusion-based visual foundation model for dense geometry predic-\\ntion. With minimal training data, Lotus achieves SoTA performance in two key geometry perception'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 0}, page_content='tasks, i.e., zero-shot depth and normal estimation. “Avg. Rank” indicates the average ranking across\\nall metrics, where lower values are better. Bar length represents the amount of training data used.\\n✱Both authors contributed equally (order randomized).\\x00Corresponding author.\\n1arXiv:2409.18124v'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='ABSTRACT\\nLeveraging the visual priors of pre-trained text-to-image diffusion models offers a\\npromising solution to enhance zero-shot generalization in dense prediction tasks.\\nHowever, existing methods often uncritically use the original diffusion formula-\\ntion, which may not be optimal due to the fundamental differences between dense\\nprediction and image generation. In this paper, we provide a systemic analysis of\\nthe diffusion formulation for the dense prediction, focusing on both quality and\\nefficiency. And we find that the original parameterization type for image gener-\\nation, which learns to predict noise, is harmful for dense prediction; the multi-\\nstep noising/denoising diffusion process is also unnecessary and challenging to\\noptimize. Based on these insights, we introduce Lotus , a diffusion-based visual\\nfoundation model with a simple yet effective adaptation protocol for dense predic-\\ntion. Specifically, Lotus is trained to directly predict annotations instead of noise,\\nthereby avoiding harmful variance. We also reformulate the diffusion process into\\na single-step procedure, simplifying optimization and significantly boosting infer-\\nence speed. Additionally, we introduce a novel tuning strategy called detail pre-\\nserver, which achieves more accurate and fine-grained predictions. Without scal-\\ning up the training data or model capacity, Lotus achieves SoTA performance in\\nzero-shot depth and normal estimation across various datasets. It also significantly\\nenhances efficiency, being hundreds of times faster than most existing diffusion-\\nbased methods. Lotus’ superior quality and efficiency also enable a wide range of\\npractical applications, such as joint estimation, single/multi-view 3D reconstruc-\\ntion, etc. Project page: lotus3d.github.io .\\n1 I NTRODUCTION\\nDense prediction is a fundamental task in computer vision, benefiting a wide range of applications,\\nsuch as 3D/4D reconstruction [Huang et al. (2024); Long et al. (2024); Wang et al. (2024); Lei\\net al. (2024)], tracking [Xiao et al. (2024); Song et al. (2024)], and autonomous driving [Yurtsever\\net al. (2020); Hu et al. (2023)]. Estimating pixel-level geometric attributes from a single image re-\\nquires comprehensive scene understanding. Although deep learning has advanced dense prediction,\\nprogress is limited by the quality, diversity, and scale of training data, leading to poor zero-shot gen-\\neralization. Instead of merely scaling data and model size, recent works [Lee et al. (2024); Ke et al.\\n(2024); Fu et al. (2024); Xu et al. (2024)] leverage diffusion priors for zero-shot dense prediction.\\nThese studies demonstrate that text-to-image diffusion models like Stable Diffusion [Rombach et al.\\n(2022)], pretrained on billions of images, possess powerful and comprehensive visual priors to ele-\\nvate dense prediction performance. However, most of these methods directly inherit the pre-trained\\ndiffusion models for dense prediction tasks, without exploring more suitable diffusion formulations.\\nThis oversight often leads to challenging issues. For example, Marigold [Ke et al. (2024)] directly\\nfine-tunes Stable Diffusion for image-conditioned depth generation. While it significantly improves\\ndepth estimation, its performance is still constrained by overlooking the fundamental differences\\nbetween dense prediction and image generation. Especially, its efficiency is also severely limited by\\nstandard iterative denoising processes and ensemble inferences.\\nMotivated by these concerns, we systematically analyze the diffusion formulation, trying to find a\\nbetter formulation to fit the pre-trained diffusion model into dense prediction. Our analysis yields\\nseveral important findings: ①The widely used parameterization, i.e., noise prediction, for diffusion-\\nbased image generation is ill-suited for dense prediction. It results in large prediction errors due\\nto harmful prediction variance at initial denoising steps, which are subsequently propagated and\\nmagnified throughout the entire denoising process (Sec. 4.1). ②Multi-step diffusion formulation is\\ncomputation-intensive and is prone to sub-optimal with limited data and resources. These factors\\nsignificantly hinder the adaptation of diffusion priors to dense prediction tasks, leading to decreased\\naccuracy and efficiency (Sec. 4.2). ③Though remarkable performance achieved, we observed that\\nthe model usually outputs vague predictions in highly-detailed areas (Fig. 8). This vagueness is\\nattributed to catastrophic forgetting: the pre-trained diffusion models gradually lose their ability to\\ngenerate detailed regions during fine-tuning (Sec. 4.3).\\n2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nABSTRACT\\nLeveraging the visual priors of pre-trained text-to-image diffusion models offers a\\npromising solution to enhance zero-shot generalization in dense prediction tasks.\\nHowever, existing methods often uncritically use the original diffusion formula-\\ntion, which may not be optimal due to the fundamental differences between dense\\nprediction and image generation. In this paper, we provide a systemic analysis of\\nthe diffusion formulation for the dense prediction, focusing on both quality and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='efficiency. And we find that the original parameterization type for image gener-\\nation, which learns to predict noise, is harmful for dense prediction; the multi-\\nstep noising/denoising diffusion process is also unnecessary and challenging to\\noptimize. Based on these insights, we introduce Lotus , a diffusion-based visual\\nfoundation model with a simple yet effective adaptation protocol for dense predic-\\ntion. Specifically, Lotus is trained to directly predict annotations instead of noise,\\nthereby avoiding harmful variance. We also reformulate the diffusion process into'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='a single-step procedure, simplifying optimization and significantly boosting infer-\\nence speed. Additionally, we introduce a novel tuning strategy called detail pre-\\nserver, which achieves more accurate and fine-grained predictions. Without scal-\\ning up the training data or model capacity, Lotus achieves SoTA performance in\\nzero-shot depth and normal estimation across various datasets. It also significantly\\nenhances efficiency, being hundreds of times faster than most existing diffusion-\\nbased methods. Lotus’ superior quality and efficiency also enable a wide range of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='practical applications, such as joint estimation, single/multi-view 3D reconstruc-\\ntion, etc. Project page: lotus3d.github.io .'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='Dense prediction is a fundamental task in computer vision, benefiting a wide range of applications,\\nsuch as 3D/4D reconstruction [Huang et al. (2024); Long et al. (2024); Wang et al. (2024); Lei\\net al. (2024)], tracking [Xiao et al. (2024); Song et al. (2024)], and autonomous driving [Yurtsever\\net al. (2020); Hu et al. (2023)]. Estimating pixel-level geometric attributes from a single image re-\\nquires comprehensive scene understanding. Although deep learning has advanced dense prediction,\\nprogress is limited by the quality, diversity, and scale of training data, leading to poor zero-shot gen-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='eralization. Instead of merely scaling data and model size, recent works [Lee et al. (2024); Ke et al.\\n(2024); Fu et al. (2024); Xu et al. (2024)] leverage diffusion priors for zero-shot dense prediction.\\nThese studies demonstrate that text-to-image diffusion models like Stable Diffusion [Rombach et al.\\n(2022)], pretrained on billions of images, possess powerful and comprehensive visual priors to ele-\\nvate dense prediction performance. However, most of these methods directly inherit the pre-trained'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='diffusion models for dense prediction tasks, without exploring more suitable diffusion formulations.\\nThis oversight often leads to challenging issues. For example, Marigold [Ke et al. (2024)] directly\\nfine-tunes Stable Diffusion for image-conditioned depth generation. While it significantly improves\\ndepth estimation, its performance is still constrained by overlooking the fundamental differences\\nbetween dense prediction and image generation. Especially, its efficiency is also severely limited by\\nstandard iterative denoising processes and ensemble inferences.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='Motivated by these concerns, we systematically analyze the diffusion formulation, trying to find a\\nbetter formulation to fit the pre-trained diffusion model into dense prediction. Our analysis yields\\nseveral important findings: ①The widely used parameterization, i.e., noise prediction, for diffusion-\\nbased image generation is ill-suited for dense prediction. It results in large prediction errors due\\nto harmful prediction variance at initial denoising steps, which are subsequently propagated and\\nmagnified throughout the entire denoising process (Sec. 4.1). ②Multi-step diffusion formulation is'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='computation-intensive and is prone to sub-optimal with limited data and resources. These factors\\nsignificantly hinder the adaptation of diffusion priors to dense prediction tasks, leading to decreased\\naccuracy and efficiency (Sec. 4.2). ③Though remarkable performance achieved, we observed that\\nthe model usually outputs vague predictions in highly-detailed areas (Fig. 8). This vagueness is\\nattributed to catastrophic forgetting: the pre-trained diffusion models gradually lose their ability to\\ngenerate detailed regions during fine-tuning (Sec. 4.3).\\n2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content=\"Image x Annotation y ℰ\\n𝐳𝐱𝐳𝐲\\n𝑡=Taddnoiseconcat.𝐳𝒕𝐲denoiserU-Net 𝑓!𝐳𝐲−𝑓!𝐳𝒕𝐲,𝐳𝐱,𝑡,𝑠%&Training Objective:𝑡=Tswitcher 𝑠𝐳𝐱−𝑓!𝐳𝒕𝐲,𝐳𝐱,𝑡,𝑠'&+((\\n❄\\n🔥\\n𝒛$𝐲𝒛$𝐱single-step\\nsingle-step𝑥(-prediction(image reconstruction)(predict annotation)detail preserver\\nFigure 3: Adaptation protocol of Lotus. After the pre-trained V AE encoder Eencodes the image\\nxand annotation yto the latent space: ①the denoiser U-Net model fθis fine-tuned using x0-\\nprediction; ②we employ single-step diffusion formulation at time-step t=Tfor better coverage; ③\\nwe propose a novel detail preserver, to switch the model either to reconstruct the image or generate\\nthe dense prediction via a switcher s, ensuring a more fine-grained prediction. The noise zy\\nTin\\nbracket is used for our generative Lotus-G and is omitted for the discriminative Lotus-D .\\n1.On Single A800\\n2.We keep the original shape of input \\nimage during inference.\\n3.DA在2048下爆显存啦！\\nFigure 2: Inference time comparison in depth esti-\\nmation between Lotus and SoTA methods. Lotus\\nis hundreds of times faster than Marigold and slightly\\nfaster than DepthAnything V2 at high resolutions (Our\\nperformance at high resolutions is also promising, as\\nevidenced on the ETH3D dataset presented in Tab. 1\\nand Fig. 11). DepthAnything V2’s inference time at\\n2048×2048 is not plotted because it requires >80GB\\ngraphic memory.Following our analysis, we propose Lo-\\ntus, a diffusion-based visual foundation\\nmodel for dense prediction, featuring a\\nsimple yet effective fine-tuning protocol\\n(see Fig. 3). First, Lotus is trained to di-\\nrectly predict annotations, thereby avoid-\\ning the harmful variance associated with\\nstandard noise prediction. Next, we intro-\\nduce a one-step formulation, i.e., one step\\nbetween pure noise and clean output, to\\nfacilitate model convergence and achieve\\nbetter optimization performance with lim-\\nited high-quality data. It also consider-\\nably boosts both training and inference ef-\\nficiency. Moreover, we implement a novel\\ndetail preserver through a task switcher,\\nallowing the model either to generate an-\\nnotations or reconstruct the input images.\\nIt preserves fine-grained details in the in-\\nput image during dense annotation gener-\\nation, achieving higher performance with-\\nout compromising efficiency, requiring ad-\\nditional parameters, or being affected by\\nsurface textures.\\nTo validate Lotus, we conduct extensive experiments on two primary geometric dense predic-\\ntion tasks: zero-shot monocular depth and normal estimation. The results demonstrate that Lotus\\nachieves SoTA performance on these tasks across a wide range of evaluation datasets. Compared to\\ntraditional discriminative methods, Lotus delivers remarkable results with only 59K training sam-\\nples by effectively leveraging the powerful diffusion priors. Among generative approaches, Lotus\\nalso outperforms previous methods in both accuracy and efficiency, being significantly faster than\\nmethods like Marigold [Ke et al. (2024)] (Fig. 2). Beyond these improvements, Lotus seamlessly\\nsupports various applications, such as joint estimation, single/multi-view 3D reconstruction, etc.\\nIn conclusion, our key contributions are as follows:\\n• We systematically analyze the diffusion formulation and find their parameterization type,\\ndesigned for image generation, is unsuitable for dense prediction and the computation-\\nintensive multi-step diffusion process is also unnecessary and challenging to optimize.\\n3\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content=\"Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nImage x Annotation y ℰ\\n𝐳𝐱𝐳𝐲\\n𝑡=Taddnoiseconcat.𝐳𝒕𝐲denoiserU-Net 𝑓!𝐳𝐲−𝑓!𝐳𝒕𝐲,𝐳𝐱,𝑡,𝑠%&Training Objective:𝑡=Tswitcher 𝑠𝐳𝐱−𝑓!𝐳𝒕𝐲,𝐳𝐱,𝑡,𝑠'&+((\\n❄\\n🔥\\n𝒛$𝐲𝒛$𝐱single-step\\nsingle-step𝑥(-prediction(image reconstruction)(predict annotation)detail preserver\\nFigure 3: Adaptation protocol of Lotus. After the pre-trained V AE encoder Eencodes the image\\nxand annotation yto the latent space: ①the denoiser U-Net model fθis fine-tuned using x0-\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content='prediction; ②we employ single-step diffusion formulation at time-step t=Tfor better coverage; ③\\nwe propose a novel detail preserver, to switch the model either to reconstruct the image or generate\\nthe dense prediction via a switcher s, ensuring a more fine-grained prediction. The noise zy\\nTin\\nbracket is used for our generative Lotus-G and is omitted for the discriminative Lotus-D .\\n1.On Single A800\\n2.We keep the original shape of input \\nimage during inference.\\n3.DA在2048下爆显存啦！\\nFigure 2: Inference time comparison in depth esti-\\nmation between Lotus and SoTA methods. Lotus'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content='mation between Lotus and SoTA methods. Lotus\\nis hundreds of times faster than Marigold and slightly\\nfaster than DepthAnything V'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content='(Our\\nperformance at high resolutions is also promising, as\\nevidenced on the ETH3D dataset presented in Tab.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content='>80GB\\ngraphic memory.Following our analysis, we propose Lo-\\ntus, a diffusion-based visual foundation\\nmodel for dense prediction, featuring a\\nsimple yet effective fine-tuning protocol\\n(see Fig. 3). First, Lotus is trained to di-\\nrectly predict annotations, thereby avoid-\\ning the harmful variance associated with\\nstandard noise prediction. Next, we intro-\\nduce a one-step formulation, i.e., one step\\nbetween pure noise and clean output, to\\nfacilitate model convergence and achieve\\nbetter optimization performance with lim-\\nited high-quality data. It also consider-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content='ited high-quality data. It also consider-\\nably boosts both training and inference ef-\\nficiency. Moreover, we implement a novel\\ndetail preserver through a task switcher,\\nallowing the model either to generate an-\\nnotations or reconstruct the input images.\\nIt preserves fine-grained details in the in-\\nput image during dense annotation gener-\\nation, achieving higher performance with-\\nout compromising efficiency, requiring ad-\\nditional parameters, or being affected by\\nsurface textures.\\nTo validate Lotus, we conduct extensive experiments on two primary geometric dense predic-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content='tion tasks: zero-shot monocular depth and normal estimation. The results demonstrate that Lotus\\nachieves SoTA performance on these tasks across a wide range of evaluation datasets. Compared to\\ntraditional discriminative methods, Lotus delivers remarkable results with only 59K training sam-\\nples by effectively leveraging the powerful diffusion priors. Among generative approaches, Lotus\\nalso outperforms previous methods in both accuracy and efficiency, being significantly faster than\\nmethods like Marigold [Ke et al. (2024)] (Fig. 2). Beyond these improvements, Lotus seamlessly'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content='supports various applications, such as joint estimation, single/multi-view 3D reconstruction, etc.\\nIn conclusion, our key contributions are as follows:\\n• We systematically analyze the diffusion formulation and find their parameterization type,\\ndesigned for image generation, is unsuitable for dense prediction and the computation-\\nintensive multi-step diffusion process is also unnecessary and challenging to optimize.\\n3'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='• We propose a novel detail preserver that ensures more accurate dense predictions especially\\nin detail-rich areas, without compromising efficiency, introducing additional network pa-\\nrameters, or being affected by surface textures.\\n• Based on our insights, we introduce Lotus , a diffusion-based visual foundation model for\\ndense prediction with simple yet effective fine-tuning protocol. Lotus achieves SoTA per-\\nformance on both zero-shot monocular depth and surface normal estimation. It also enables\\na wide range of applications.\\n2 R ELATED WORKS\\n2.1 T EXT-TO-IMAGE GENERATIVE MODELS\\nIn the field of text-to-image generation, the evolution of methodologies has transitioned from gen-\\nerative adversarial networks (GANs) [Goodfellow et al. (2014); Zhang et al. (2017; 2018; 2021);\\nHe et al. (2022); Karras et al. (2019; 2020; 2021); Zhang et al. (2017; 2018); Xu et al. (2018);\\nZhang et al. (2021)] to advanced diffusion models [Ho et al. (2020); Ramesh et al. (2022); Saharia\\net al. (2022); Ramesh et al. (2021); Nichol et al. (2021); Chen et al. (2023); Rombach et al. (2022);\\nRamesh et al. (2021)]. A series of diffusion-based methods such as GLIDE [Nichol et al. (2021)],\\nDALL·E2 [Ramesh et al. (2022)], and Imagen [Saharia et al. (2022)] have been introduced, offering\\nenhanced image quality and textual coherence. The Stable Diffusion (SD) [Rombach et al. (2022)],\\ntrained on large-scale LAION-5B dataset [Schuhmann et al. (2022)], further enhances the generative\\nquality, becoming the community standard. In our paper, we aim to leverage the comprehensive and\\nencyclopedic visual priors of SD to facilitate zero-shot generalization for dense prediction tasks.\\n2.2 G ENERATIVE MODELS FOR DENSE PERCEPTION\\nCurrently, a notable trend involves adopting pre-trained generative models, particularly diffusion\\nmodels, into dense prediction tasks. Marigold [Ke et al. (2024)] and GeoWizard [Fu et al. (2024)]\\ndirectly apply the standard diffusion formulation and the pre-trained parameters, without address-\\ning the inherent differences between image generation and dense prediction, leading to constrained\\nperformance. Their efficiency is also severely limited by standard iterative denoising processes and\\nensemble inferences. In this paper, we propose a novel diffusion formulation tailored to the charac-\\nteristics of dense prediction. Aiming to fully leveraging the pre-trained diffusion’s powerful visual\\npriors, Lotus enables more accurate and efficient predictions, finally achieving SoTA performance.\\nMore recent works, GenPercept [Xu et al. (2024)] and StableNormal [Ye et al. (2024)], also adopted\\nsingle-step diffusion. However, GenPercept [Xu et al. (2024)] first removes noise input for de-\\nterministic characteristic based on DMP [Lee et al. (2024)], and then adopts one-step strategy to\\navoid surface texture interference. It lacks systematic analysis of the diffusion formulation, only\\ntreats the U-Net as a deterministic backbone and still falls short in performance. In contrast, Lotus\\nsystematically analyzes the standard stochastic diffusion formulation for dense prediction and pro-\\nposes innovations such as the detail preserver to improve accuracy especially in detailed area, finally\\ndelivering much better results (Tab. 1 and Fig. 11). Additionally, Lotus is a stochastic model, in con-\\ntrast to GenPercept’s deterministic nature, enabling uncertainty predictions. StableNormal [Ye et al.\\n(2024)] predicts normal maps through a two-stage process. While the first stage produces coarse\\nnormal maps with single-step diffusion, the second stage performs refinement still with iterative\\ndiffusion which is computation-intensive. In comparison, Lotus not only achieves fine-grained pre-\\ndictions via the novel detail preserver without extra stages or parameters, but also delivers much\\nsuperior results (Tab. 2 and Fig. 12) thanks to our designed diffusion formulation that better fits\\npre-trained diffusion for dense prediction.\\n2.3 M ONOCULAR DEPTH AND NORMAL PREDICTION\\nMonocular depth and normal prediction are two crucial dense prediction tasks. Solving them typ-\\nically demands comprehensive scene understanding capability. Starting from Eigen et al. (2014),\\nearly CNN-based methods for depth prediction, such as Fu et al. (2018), Lee et al. (2019), Yuan\\net al. (2022), focus only on specific domains. Subsequently, in pursuit of a generic depth estima-\\ntor, many methods expand model capacity and train on larger and more diverse datasets, such as\\n4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\n• We propose a novel detail preserver that ensures more accurate dense predictions especially\\nin detail-rich areas, without compromising efficiency, introducing additional network pa-\\nrameters, or being affected by surface textures.\\n• Based on our insights, we introduce Lotus , a diffusion-based visual foundation model for\\ndense prediction with simple yet effective fine-tuning protocol. Lotus achieves SoTA per-\\nformance on both zero-shot monocular depth and surface normal estimation. It also enables'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='a wide range of applications.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='-TO-IMAGE GENERATIVE MODELS\\nIn the field of text-to-image generation, the evolution of methodologies has transitioned from gen-\\nerative adversarial networks (GANs) [Goodfellow et al. (2014); Zhang et al. (2017; 2018; 2021);\\nHe et al. (2022); Karras et al. (2019; 2020; 2021); Zhang et al. (2017; 2018); Xu et al. (2018);\\nZhang et al. (2021)] to advanced diffusion models [Ho et al. (2020); Ramesh et al. (2022); Saharia\\net al. (2022); Ramesh et al. (2021); Nichol et al. (2021); Chen et al. (2023); Rombach et al. (2022);'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='Ramesh et al. (2021)]. A series of diffusion-based methods such as GLIDE [Nichol et al. (2021)],\\nDALL·E2 [Ramesh et al. (2022)], and Imagen [Saharia et al. (2022)] have been introduced, offering\\nenhanced image quality and textual coherence. The Stable Diffusion (SD) [Rombach et al. (2022)],\\ntrained on large-scale LAION-5B dataset [Schuhmann et al. (2022)], further enhances the generative\\nquality, becoming the community standard. In our paper, we aim to leverage the comprehensive and\\nencyclopedic visual priors of SD to facilitate zero-shot generalization for dense prediction tasks.\\n2.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='Currently, a notable trend involves adopting pre-trained generative models, particularly diffusion\\nmodels, into dense prediction tasks. Marigold [Ke et al. (2024)] and GeoWizard [Fu et al. (2024)]\\ndirectly apply the standard diffusion formulation and the pre-trained parameters, without address-\\ning the inherent differences between image generation and dense prediction, leading to constrained\\nperformance. Their efficiency is also severely limited by standard iterative denoising processes and\\nensemble inferences. In this paper, we propose a novel diffusion formulation tailored to the charac-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='teristics of dense prediction. Aiming to fully leveraging the pre-trained diffusion’s powerful visual\\npriors, Lotus enables more accurate and efficient predictions, finally achieving SoTA performance.\\nMore recent works, GenPercept [Xu et al. (2024)] and StableNormal [Ye et al. (2024)], also adopted\\nsingle-step diffusion. However, GenPercept [Xu et al. (2024)] first removes noise input for de-\\nterministic characteristic based on DMP [Lee et al. (2024)], and then adopts one-step strategy to\\navoid surface texture interference. It lacks systematic analysis of the diffusion formulation, only'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='treats the U-Net as a deterministic backbone and still falls short in performance. In contrast, Lotus\\nsystematically analyzes the standard stochastic diffusion formulation for dense prediction and pro-\\nposes innovations such as the detail preserver to improve accuracy especially in detailed area, finally\\ndelivering much better results (Tab.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='. 11). Additionally, Lotus is a stochastic model, in con-\\ntrast to GenPercept’s deterministic nature, enabling uncertainty predictions. StableNormal [Ye et al.\\n(2024)] predicts normal maps through a two-stage process. While the first stage produces coarse\\nnormal maps with single-step diffusion, the second stage performs refinement still with iterative\\ndiffusion which is computation-intensive. In comparison, Lotus not only achieves fine-grained pre-\\ndictions via the novel detail preserver without extra stages or parameters, but also delivers much\\nsuperior results (Tab.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='. 12) thanks to our designed diffusion formulation that better fits\\npre-trained diffusion for dense prediction.\\n2.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='Monocular depth and normal prediction are two crucial dense prediction tasks. Solving them typ-\\nically demands comprehensive scene understanding capability. Starting from Eigen et al. (2014),\\nearly CNN-based methods for depth prediction, such as Fu et al. (2018), Lee et al. (2019), Yuan\\net al. (2022), focus only on specific domains. Subsequently, in pursuit of a generic depth estima-\\ntor, many methods expand model capacity and train on larger and more diverse datasets, such as\\n4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='addnoise\\nImage x Annotation y ℰ\\n𝐳𝐱𝐳𝐲\\n𝑡∈[1,T]concat.𝐳𝒕𝐲denoiserU-Net 𝑓!𝜖−𝑓!𝐳𝒕𝐲,𝐳𝐱,𝑡%Training Objective:𝑡∈[1,T]\\n❄\\n🔥\\n𝜖multi-step\\nmulti-step𝜖-prediction(predict noise)\\nFigure 4: Adaptation protocol of Direct Adaptation. Starting with a pre-trained Stable Diffusion\\nmodel, image xand annotation yare encoded using the pre-trained V AE. The noisy annotation zy\\ntis\\nobtained by adding noise at level t∈[1, T]. The U-Net input layer is coupled to accommodate the\\nconcatenated inputs and then fine-tuned using the standard diffusion objective, ϵ-prediction, under\\nthe original multi-step formulation.\\nDiverseDepth [Yin et al. (2021a)] and MiDaS [Ranftl et al. (2020)]. DPT [Ranftl et al. (2021)] and\\nOmnidata [Eftekhar et al. (2021)] are further proposed based on vision transformer [Ranftl et al.\\n(2021)], significantly enhancing performance. LeRes [Yin et al. (2021b)] and HDN [Zhang et al.\\n(2022)] further introduce novel training strategies and multi-scale depth normalization to improve\\npredictions in detailed areas. More recently, the DepthAnything series [Yang et al. (2024a;b)] and\\nMetric3D series [Yin et al. (2023); Hu et al. (2024)] collect and leverage millions of labeled data\\nto develop more powerful estimators. Normal prediction follows the same trend. Starting with the\\nearly CNN-based methods like OASIS [Chen et al. (2020)], EESNU [Bae & Davison (2021)] and\\nOmnidata series [Eftekhar et al. (2021); Kar et al. (2022)] expand the model capacity and scale up\\nthe training data. Recently, DSINE [Bae & Davison (2024)] achieves SoTA performance by rethink-\\ning inductive biases for surface normal estimation. In our paper, we focus on leveraging pre-trained\\ndiffusion priors to enhance zero-shot dense predictions, rather than expanding model capacity or\\nrelying on large training data, which avoids the need for intensive resources and computation.\\n3 P RELIMINARIES\\nDiffusion Formulation for Dense Prediction. Following Ke et al. (2024) and Fu et al. (2024), we\\nalso formulate dense prediction as an image-conditioned annotation generation task based on Stable\\nDiffusion [Rombach et al. (2022)], which performs the diffusion process in low-dimensional latent\\nspace for computational efficiency. First, there is a pair of auto-encoders {E(·),D(·)}trained to\\nmap between RGB space and latent space, i.e.,E(x) =zx,D(zx)≈x. The auto-encoder also maps\\nbetween dense annotations and latent space effectively, i.e.,E(y) =zy,D(zy)≈y[Ke et al. (2024);\\nFu et al. (2024); Xu et al. (2024); Ye et al. (2024)]. Following Ho et al. (2020), Stable Diffusion\\nestablishes a pair of forward nosing and reversal denoising processes in latent space. In forward\\nprocess, Gaussian noise is gradually added at levels t∈[1, T]into sample zyto obtain the noisy\\nsample zy\\nt:\\nzy\\nt=√αtzy+√\\n1−αtϵ, (1)\\nwhere ϵ∼ N(0, I),αt:=Qt\\ns=1(1−βs), and{β1, β2, . . . , β T}is the noise schedule with Tsteps.\\nAt time-step T, the sample zyis degraded to pure Gaussian noise. In the reversal process, a neural\\nnetwork fθ, usually a U-Net model [Ronneberger et al. (2015)), is trained to iteratively remove noise\\nfrom zy\\ntto predict the clean sample zy. The network is trained by sampling a random t∈[1, T]and\\nminimizing the loss function Lt.\\nParameterization Types. To enable gradient computation for network training, there are two basic\\nparameterizations of the loss function Lt.①ϵ-prediction [Ho et al. (2020)]: the model fθlearns to\\npredict the added noise ϵ;②x0-prediction [Ho et al. (2020)]: the model fθlearns to directly predict\\nthe clean sample zy. The loss functions for these parameterizations are formulated as:\\nLϵ\\nt=||ϵ−fϵ\\nθ(zy\\nt,zx, t)||2, (2)\\nLz\\nt=||zy−fz\\nθ(zy\\nt,zx, t)||2, (3)\\n5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\naddnoise\\nImage x Annotation y ℰ\\n𝐳𝐱𝐳𝐲\\n𝑡∈[1,T]concat.𝐳𝒕𝐲denoiserU-Net 𝑓!𝜖−𝑓!𝐳𝒕𝐲,𝐳𝐱,𝑡%Training Objective:𝑡∈[1,T]\\n❄\\n🔥\\n𝜖multi-step\\nmulti-step𝜖-prediction(predict noise)\\nFigure 4: Adaptation protocol of Direct Adaptation. Starting with a pre-trained Stable Diffusion\\nmodel, image xand annotation yare encoded using the pre-trained V AE. The noisy annotation zy\\ntis\\nobtained by adding noise at level t∈[1, T]. The U-Net input layer is coupled to accommodate the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='concatenated inputs and then fine-tuned using the standard diffusion objective, ϵ-prediction, under\\nthe original multi-step formulation.\\nDiverseDepth [Yin et al. (2021a)] and MiDaS [Ranftl et al. (2020)]. DPT [Ranftl et al. (2021)] and\\nOmnidata [Eftekhar et al. (2021)] are further proposed based on vision transformer [Ranftl et al.\\n(2021)], significantly enhancing performance. LeRes [Yin et al. (2021b)] and HDN [Zhang et al.\\n(2022)] further introduce novel training strategies and multi-scale depth normalization to improve'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='predictions in detailed areas. More recently, the DepthAnything series [Yang et al. (2024a;b)] and\\nMetric3D series [Yin et al. (2023); Hu et al. (2024)] collect and leverage millions of labeled data\\nto develop more powerful estimators. Normal prediction follows the same trend. Starting with the\\nearly CNN-based methods like OASIS [Chen et al. (2020)], EESNU [Bae & Davison (2021)] and\\nOmnidata series [Eftekhar et al. (2021); Kar et al. (2022)] expand the model capacity and scale up\\nthe training data. Recently, DSINE [Bae & Davison (2024)] achieves SoTA performance by rethink-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='ing inductive biases for surface normal estimation. In our paper, we focus on leveraging pre-trained\\ndiffusion priors to enhance zero-shot dense predictions, rather than expanding model capacity or\\nrelying on large training data, which avoids the need for intensive resources and computation.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='Diffusion Formulation for Dense Prediction. Following Ke et al. (2024) and Fu et al. (2024), we\\nalso formulate dense prediction as an image-conditioned annotation generation task based on Stable\\nDiffusion [Rombach et al. (2022)], which performs the diffusion process in low-dimensional latent\\nspace for computational efficiency. First, there is a pair of auto-encoders {E(·),D(·)}trained to\\nmap between RGB space and latent space, i.e.,E(x) =zx,D(zx)≈x. The auto-encoder also maps\\nbetween dense annotations and latent space effectively, i.e.,E(y) =zy,D(zy)≈y[Ke et al. (2024);'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='Fu et al. (2024); Xu et al. (2024); Ye et al. (2024)]. Following Ho et al. (2020), Stable Diffusion\\nestablishes a pair of forward nosing and reversal denoising processes in latent space. In forward\\nprocess, Gaussian noise is gradually added at levels t∈[1, T]into sample zyto obtain the noisy\\nsample zy\\nt:\\nzy\\nt=√αtzy+√\\n1−αtϵ, (1)\\nwhere ϵ∼ N(0, I),αt:=Qt\\ns=1(1−βs), and{β1, β2, . . . , β T}is the noise schedule with Tsteps.\\nAt time-step T, the sample zyis degraded to pure Gaussian noise. In the reversal process, a neural'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='network fθ, usually a U-Net model [Ronneberger et al. (2015)), is trained to iteratively remove noise\\nfrom zy\\ntto predict the clean sample zy. The network is trained by sampling a random t∈[1, T]and\\nminimizing the loss function Lt.\\nParameterization Types. To enable gradient computation for network training, there are two basic\\nparameterizations of the loss function Lt.①ϵ-prediction [Ho et al. (2020)]: the model fθlearns to\\npredict the added noise ϵ;②x0-prediction [Ho et al. (2020)]: the model fθlearns to directly predict'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='the clean sample zy. The loss functions for these parameterizations are formulated as:\\nLϵ\\nt=||ϵ−fϵ\\nθ(zy\\nt,zx, t)||2, (2)\\nLz\\nt=||zy−fz\\nθ(zy\\nt,zx, t)||2, (3)\\n5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='𝜏=1𝜏\\t=1000𝜏\\t=600𝜏\\t=200\\nInput Image\\n𝜀-prediction, seed 1𝜀-prediction, seed 2𝑥!-prediction, seed 1𝑥!-prediction, seed 2𝜏=1𝜏\\t=1000𝜏\\t=600𝜏\\t=200\\nFigure 5: Comparisons among different parameterizations using various seeds. All models are\\ntrained on Hypersim [Roberts et al. (2021)] and tested on the input image for depth estimation. The\\nstandard DDIM sampler is used with 50 denoising steps. Four steps are selected for clear illustration.\\nFrom left (larger τ) to right (smaller τ) is the iterative denoising process.\\nwhere f∗\\nθis the denoiser model to be learnt, ∗ ∈ { ϵ,z}.ϵ-prediction is commonly chosen as the\\nstandard for parameterizing the denoising model, as it empirically achieves high-quality image gen-\\neration with fine details and realism.\\nDenoising Process. DDIM [Song et al. (2020)] is a key technique for multi-step diffusion models\\nto achieve fast sampling, which implements an implicit probabilistic model that can significantly\\nreduce the number of denoising steps while maintaining output quality. Formally, the denoising\\nprocess from zy\\nτtozy\\nτ−1is:\\nzy\\nτ−1=p\\nατ−1ˆ zy\\nτ+direction (zy\\nτ) +στϵτ, (4)\\nwhere ˆ zy\\nτis the predicted clean sample at the denoising step τ, direction (zy\\nτ)represents the direction\\npointing to zy\\nτandστcan be set to 0if deterministic inference is needed. And τ∈ {τ1, τ2, . . . , τ S},\\nan increasing sub-sequence of the time-step set [1, T], is used for fast sampling. During inference,\\nDDIM iteratively denoises the sample from τStoτ1to obtain the clean one.\\n4 M ETHODOLOGY\\nWe start our analysis by directly adapting the original diffusion formulation with minimal modifica-\\ntions as illustrated in Fig. 41. We call this starting point as “ Direct Adaptation ”. Direct Adaptation\\nis optimized using the standard diffusion objective as formulated in Eq. 2 and inferred by standard\\nmulti-step DDIM sampler. As shown in Tab. 3, Direct Adaptation fails to achieve satisfactory per-\\nformance. In following sections, we will systematically analyze the key factors that affect adaptation\\nperformance step by step: parameterization types (Sec. 4.1); number of time-steps (Sec. 4.2); and\\nthe novel detail preserver (Sec. 4.3).\\n4.1 P ARAMETERIZATION TYPES\\nThe type of parameterization is a vital configuration, it not only determines the loss function dis-\\ncussed in Sec. 3, but also influences the inference process (Eq. 4). During inference, the predicted\\nclean sample ˆ zy\\nτ, a key component in Eq. 4, is calculated according to different parameterizations2\\nϵ-prediction: ˆ zy\\nτ=1√ατ(zy\\nτ−√\\n1−ατfϵ\\nθ(zy\\nτ,zx, τ))\\nx0-prediction: ˆ zy\\nτ=fz\\nθ(zy\\nτ,zx, τ)(5)\\nIn the community, ϵ-prediction is chosen as the standard for image generation. However, it is not\\neffective for dense prediction task. In the following, we will discuss the impact of different parame-\\nterization types in denoising inference process for dense prediction task.\\n1Details of “Direct Adaptation” will be provided in the supplementary materials.\\n2The latest parameterization, v-prediction, combines ϵ-prediction and x0-prediction, producing results that\\nare intermediate between the two. Please see the supplementary materials for more details.\\n6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\n𝜏=1𝜏\\t=1000𝜏\\t=600𝜏\\t=2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='𝜀-prediction, seed 1𝜀-prediction, seed 2𝑥!-prediction, seed 1𝑥!-prediction, seed 2𝜏=1𝜏\\t=1000𝜏\\t=600𝜏\\t=2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='5: Comparisons among different parameterizations using various seeds. All models are\\ntrained on Hypersim [Roberts et al. (2021)] and tested on the input image for depth estimation. The\\nstandard DDIM sampler is used with'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='. Four steps are selected for clear illustration.\\nFrom left (larger τ) to right (smaller τ) is the iterative denoising process.\\nwhere f∗\\nθis the denoiser model to be learnt, ∗ ∈ { ϵ,z}.ϵ-prediction is commonly chosen as the\\nstandard for parameterizing the denoising model, as it empirically achieves high-quality image gen-\\neration with fine details and realism.\\nDenoising Process. DDIM [Song et al. (2020)] is a key technique for multi-step diffusion models\\nto achieve fast sampling, which implements an implicit probabilistic model that can significantly'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='reduce the number of denoising steps while maintaining output quality. Formally, the denoising\\nprocess from zy\\nτtozy\\nτ−1is:\\nzy\\nτ−1=p\\nατ−1ˆ zy\\nτ+direction (zy\\nτ) +στϵτ, (4)\\nwhere ˆ zy\\nτis the predicted clean sample at the denoising step τ, direction (zy\\nτ)represents the direction\\npointing to zy\\nτandστcan be set to 0if deterministic inference is needed. And τ∈ {τ1, τ2, . . . , τ S},\\nan increasing sub-sequence of the time-step set [1, T], is used for fast sampling. During inference,\\nDDIM iteratively denoises the sample from τStoτ1to obtain the clean one.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='We start our analysis by directly adapting the original diffusion formulation with minimal modifica-\\ntions as illustrated in Fig. 41. We call this starting point as “ Direct Adaptation ”. Direct Adaptation\\nis optimized using the standard diffusion objective as formulated in Eq.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='multi-step DDIM sampler. As shown in Tab. 3, Direct Adaptation fails to achieve satisfactory per-\\nformance. In following sections, we will systematically analyze the key factors that affect adaptation\\nperformance step by step: parameterization types (Sec. 4.1); number of time-steps (Sec. 4.2); and\\nthe novel detail preserver (Sec. 4.3).\\n4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='The type of parameterization is a vital configuration, it not only determines the loss function dis-\\ncussed in Sec. 3, but also influences the inference process (Eq. 4). During inference, the predicted\\nclean sample ˆ zy\\nτ, a key component in Eq. 4, is calculated according to different parameterizations2\\nϵ-prediction: ˆ zy\\nτ=1√ατ(zy\\nτ−√\\n1−ατfϵ\\nθ(zy\\nτ,zx, τ))\\nx0-prediction: ˆ zy\\nτ=fz\\nθ(zy\\nτ,zx, τ)(5)\\nIn the community, ϵ-prediction is chosen as the standard for image generation. However, it is not'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='effective for dense prediction task. In the following, we will discuss the impact of different parame-\\nterization types in denoising inference process for dense prediction task.\\n1Details of “Direct Adaptation” will be provided in the supplementary materials.\\n2The latest parameterization, v-prediction, combines ϵ-prediction and x0-prediction, producing results that\\nare intermediate between the two. Please see the supplementary materials for more details.\\n6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content=\"0 200 400 600 800 1000\\nDenoising Step ()\\n89101112NYUv2 AbsRel (%) \\nNYUv2 AbsRel vs. Denoising Step\\nPred. Type=x0\\nPred. Type=\\nFigure 6: Quantitative evaluation of the predicted\\ndepth maps ˆ zy\\nτalong the denoising process. The\\nexperimental settings are same as Fig. 5. Six steps\\nare selected for illustration. The banded regions\\naround each line indicate the variance, wider areas\\nrepresenting larger variance.Insights from the literature [Benny & Wolf\\n(2022); Salimans & Ho (2022)] reveal that\\nϵ-prediction introduces larger pixel variance\\ncompared to x0-prediction, especially at the\\ninitial denoising steps (large τ). This vari-\\nance mainly originates from the noise in-\\nput. Specifically, for ϵ-prediction in Eq. 5,\\nat initial denoising step, τ→T, the value\\n1√ατ→+∞. Even small prediction vari-\\nance from fϵ\\nθ(zy\\nτ,zx, τ)will be amplified\\nsignificantly, resulting in large variance of\\npredicted ˆ zy\\nτ. In contrast, there is no coeffi-\\ncient for x0-prediction to re-scale the model\\noutput, achieving more stable predictions of\\nˆ zy\\nτat initial denoising steps. Subsequently,\\nthe predicted ˆ zy\\nτis used in Eq. 4. This it-\\nerative denoising process will preserve and\\namplify the influence of large variance. In\\nEq. 4, the coefficients√ατ−1of predicted\\nˆ zy\\nτare same across two parameterizations,\\nand other terms are of the same order of magnitude. Therefore, the ˆ zy\\nτpredicted by ϵ-prediction,\\nwhich has larger variance, exerts a more significant influence on the denoising process.\\n5K 10K 19K 39K\\nTraining Data67891011NYUv2 AbsRel (%) \\nNYUv2 AbsRel vs. Training Data\\nT'=1\\nT'=2\\nT'=5\\nT'=10\\nT'=100\\nT'=1000\\nFigure 7: Comparisons among various training\\ntime-steps and data scales evaluated on NYUv2\\nin depth estimation. All models are fine-tuned on\\nHypersim using x0-prediction. During inference, if\\nT′>50, the DDIM sampler is used with 50 denois-\\ning steps; otherwise, the number of denoising steps\\nis equal to T′. The results demonstrate improved\\nperformance with decreased training time-steps. The\\nsingle-step diffusion formulation ( T′= 1) exhibits\\nbest performance across different data volumes.We take the depth estimation as an example.\\nDuring the inference process, we compute\\nthe predicted depth map ˆ zy\\nτat each denois-\\ning step τ. As illustrated in Fig. 5, the depth\\nmaps predicted by ϵ-prediction significantly\\nvary under different seeds while those pre-\\ndicted by x0-prediction are more consistent.\\nAlthough the large variance enhances diver-\\nsity for image generation, it lead to unsta-\\nble predictions in dense prediction tasks, po-\\ntentially resulting in significant errors. For\\nexample in Fig. 5, the “dark gray cabinet”\\n(highlighted in red circles) maybe wrongly\\nconsidered as an “opened door” with sig-\\nnificantly larger depth. While the predicted\\ndepth map looks more and more “plausi-\\nble”, the error gradually propagates to the fi-\\nnal prediction ( τ= 1) along the denoising\\nprocess, indicating the persistent influence\\nof the large variance. We further quantita-\\ntively measure the predicted depth maps by\\nthe absolute mean relative error (AbsRel) on\\nNYUv2 dataset [Silberman et al. (2012)]. As\\nshown in Fig. 6, ϵ-prediction exhibits higher\\nerror with much larger variance compared to\\nx0-prediction at the initial denoising steps\\n(τ→T), and the prediction error propagates along the denoising process with higher slope. In\\ncontrast, x0-prediction, directly predicting ˆ zy\\nτwithout any coefficients to amplify the prediction\\nvariance, yields more stable and correct dense predictions than ϵ-prediction.\\nIn conclusion, to mitigate the errors from large variance that adversely affect the performance of\\ndense prediction, we replace the standard ϵ-prediction with the more tailored x0-prediction.\\n7\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\n0 200 400 600 800 10'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='. Type=\\nFigure 6: Quantitative evaluation of the predicted\\ndepth maps ˆ zy\\nτalong the denoising process. The\\nexperimental settings are same as Fig. 5. Six steps\\nare selected for illustration. The banded regions\\naround each line indicate the variance, wider areas\\nrepresenting larger variance.Insights from the literature [Benny & Wolf\\n(2022); Salimans & Ho (2022)] reveal that\\nϵ-prediction introduces larger pixel variance\\ncompared to x0-prediction, especially at the\\ninitial denoising steps (large τ). This vari-\\nance mainly originates from the noise in-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='ance mainly originates from the noise in-\\nput. Specifically, for ϵ-prediction in Eq. 5,\\nat initial denoising step, τ→T, the value\\n1√ατ→+∞. Even small prediction vari-\\nance from fϵ\\nθ(zy\\nτ,zx, τ)will be amplified\\nsignificantly, resulting in large variance of\\npredicted ˆ zy\\nτ. In contrast, there is no coeffi-\\ncient for x0-prediction to re-scale the model\\noutput, achieving more stable predictions of\\nˆ zy\\nτat initial denoising steps. Subsequently,\\nthe predicted ˆ zy\\nτis used in Eq. 4. This it-\\nerative denoising process will preserve and\\namplify the influence of large variance. In'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='amplify the influence of large variance. In\\nEq. 4, the coefficients√ατ−1of predicted\\nˆ zy\\nτare same across two parameterizations,\\nand other terms are of the same order of magnitude. Therefore, the ˆ zy\\nτpredicted by ϵ-prediction,\\nwhich has larger variance, exerts a more significant influence on the denoising process.\\n5K 10K 19K 39K\\nTraining Data67891011NYUv'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='. All models are fine-tuned on\\nHypersim using x0-prediction. During inference, if\\nT′>50, the DDIM sampler is used with'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='-\\ning steps; otherwise, the number of denoising steps\\nis equal to T′. The results demonstrate improved\\nperformance with decreased training time-steps. The\\nsingle-step diffusion formulation ( T′= 1) exhibits\\nbest performance across different data volumes.We take the depth estimation as an example.\\nDuring the inference process, we compute\\nthe predicted depth map ˆ zy\\nτat each denois-\\ning step τ. As illustrated in Fig. 5, the depth\\nmaps predicted by ϵ-prediction significantly\\nvary under different seeds while those pre-\\ndicted by x0-prediction are more consistent.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='dicted by x0-prediction are more consistent.\\nAlthough the large variance enhances diver-\\nsity for image generation, it lead to unsta-\\nble predictions in dense prediction tasks, po-\\ntentially resulting in significant errors. For\\nexample in Fig. 5, the “dark gray cabinet”\\n(highlighted in red circles) maybe wrongly\\nconsidered as an “opened door” with sig-\\nnificantly larger depth. While the predicted\\ndepth map looks more and more “plausi-\\nble”, the error gradually propagates to the fi-\\nnal prediction ( τ= 1) along the denoising\\nprocess, indicating the persistent influence'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='process, indicating the persistent influence\\nof the large variance. We further quantita-\\ntively measure the predicted depth maps by\\nthe absolute mean relative error (AbsRel) on\\nNYUv'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='[Silberman et al. (2012)]. As\\nshown in Fig. 6, ϵ-prediction exhibits higher\\nerror with much larger variance compared to\\nx0-prediction at the initial denoising steps\\n(τ→T), and the prediction error propagates along the denoising process with higher slope. In\\ncontrast, x0-prediction, directly predicting ˆ zy\\nτwithout any coefficients to amplify the prediction\\nvariance, yields more stable and correct dense predictions than ϵ-prediction.\\nIn conclusion, to mitigate the errors from large variance that adversely affect the performance of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='dense prediction, we replace the standard ϵ-prediction with the more tailored x0-prediction.\\n7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='Input Image Reconstruction\\nw/o Preserver w/ PreserverInput Image Reconstruction\\nw/o Preserver w/ Preserver\\nFigure 8: Depth maps w/andw/o the detail preserver and reconstruction outputs. Fine-tuning\\nthe diffusion model for dense prediction tasks can potentially degrade its ability to generate highly\\ndetailed images, resulting in blurred predictions in regions with rich detail. To preserve these fine-\\ngrained details, we introduce a detail preserver that incorporates an additional reconstruction task,\\nenhancing the model’s capacity to produce more accurate dense annotations.\\n4.2 N UMBER OF TIME-STEPS\\nAlthough x0-prediction can improve the prediction quality, the multi-step diffusion formulation still\\nleads to the propagation of predicted errors during the denoising process (Fig. 5, 6). Furthermore,\\nutilizing multiple time-steps enhances the model’s capacity, typically requiring large-scale training\\ndata to optimize and is beneficial—or even necessary—for complex tasks such as image generation.\\nHowever, for simpler tasks like dense prediction, where large-scale, high-quality training data is also\\nscarce, employing multiple time-steps can make the model difficult to optimize. Additionally, train-\\ning/inferring a multi-step diffusion model is slow and computation-intensive, hindering its practical\\napplication.\\nTherefore, to address these challenges, we propose fine-tuning the pre-trained diffusion model with\\nfewer training time steps. Specifically, the original set of training time-steps is defined as [1, T] =\\n{1,2,3, . . . , T }, where Tdenotes the total number of original training time-steps. We fine-tune the\\npre-trained diffusion model using a sub-sequence derived from this set. We define the length of this\\nsub-sequence as T′, where T′≪TandTis divisible by T′. This sub-sequence is obtained by\\nevenly sampling the original set at intervals, defined as:\\n{ti=i·k|i= 1,2, . . . , T′}, (6)\\nwhere k=T/T′is the sampling interval. During inference, the DDIM denoises the sample from\\nnoise to annotation using the same sub-sequence.\\nAs illustrated in Fig. 7, we conduct experiments by varying the number of time-steps T′under\\nx0-prediction. The results clearly show that the performance gradually improves as the number of\\ntime-steps is reduced, no matter the training data scales, culminating in the best result when re-\\nduced to only a single step. We further consider more strict scenarios with more limited training\\ndata to assess its impact on model optimization. As depicted in Fig. 7, these experiments reveal\\nthat the multi-step formulation is more sensitive to increases in training data scales compared with\\nsingle-step. Notably, the single-step formulation consistently yields lower prediction errors and\\ndemonstrates greater stability. Although it is conceivable that multi-step and single-step formula-\\ntions might achieve comparable performance with unlimited high-quality data, it’s expensive and\\nsometimes impractical in dense prediction.\\nDecreasing the number of denoising steps can reduce the optimization space of the diffusion model,\\nleading to more effective and efficient adaption, as suggested by the above phenomenon. There-\\nfore, for better adaptation performance under limited resource, we reduce the number of training\\ntime-steps of diffusion formulation to only one, and fixing the only time-step ttoT. Addition-\\nally, the single-step formulation is much more computationally efficient. It also naturally prevents\\nthe harmful error propagation as discussed in Sec. 4.1, further enhancing the diffusion’s adaptation\\nperformance in dense prediction.\\n8'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nInput Image Reconstruction\\nw/o Preserver w/ PreserverInput Image Reconstruction\\nw/o Preserver w/ Preserver\\nFigure 8: Depth maps w/andw/o the detail preserver and reconstruction outputs. Fine-tuning\\nthe diffusion model for dense prediction tasks can potentially degrade its ability to generate highly\\ndetailed images, resulting in blurred predictions in regions with rich detail. To preserve these fine-\\ngrained details, we introduce a detail preserver that incorporates an additional reconstruction task,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='enhancing the model’s capacity to produce more accurate dense annotations.\\n4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='-STEPS\\nAlthough x0-prediction can improve the prediction quality, the multi-step diffusion formulation still\\nleads to the propagation of predicted errors during the denoising process (Fig. 5, 6). Furthermore,\\nutilizing multiple time-steps enhances the model’s capacity, typically requiring large-scale training\\ndata to optimize and is beneficial—or even necessary—for complex tasks such as image generation.\\nHowever, for simpler tasks like dense prediction, where large-scale, high-quality training data is also'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='scarce, employing multiple time-steps can make the model difficult to optimize. Additionally, train-\\ning/inferring a multi-step diffusion model is slow and computation-intensive, hindering its practical\\napplication.\\nTherefore, to address these challenges, we propose fine-tuning the pre-trained diffusion model with\\nfewer training time steps. Specifically, the original set of training time-steps is defined as [1, T] =\\n{1,2,3, . . . , T }, where Tdenotes the total number of original training time-steps. We fine-tune the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='pre-trained diffusion model using a sub-sequence derived from this set. We define the length of this\\nsub-sequence as T′, where T′≪TandTis divisible by T′. This sub-sequence is obtained by\\nevenly sampling the original set at intervals, defined as:\\n{ti=i·k|i= 1,2, . . . , T′}, (6)\\nwhere k=T/T′is the sampling interval. During inference, the DDIM denoises the sample from\\nnoise to annotation using the same sub-sequence.\\nAs illustrated in Fig. 7, we conduct experiments by varying the number of time-steps T′under'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='x0-prediction. The results clearly show that the performance gradually improves as the number of\\ntime-steps is reduced, no matter the training data scales, culminating in the best result when re-\\nduced to only a single step. We further consider more strict scenarios with more limited training\\ndata to assess its impact on model optimization. As depicted in Fig. 7, these experiments reveal\\nthat the multi-step formulation is more sensitive to increases in training data scales compared with\\nsingle-step. Notably, the single-step formulation consistently yields lower prediction errors and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='demonstrates greater stability. Although it is conceivable that multi-step and single-step formula-\\ntions might achieve comparable performance with unlimited high-quality data, it’s expensive and\\nsometimes impractical in dense prediction.\\nDecreasing the number of denoising steps can reduce the optimization space of the diffusion model,\\nleading to more effective and efficient adaption, as suggested by the above phenomenon. There-\\nfore, for better adaptation performance under limited resource, we reduce the number of training'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='time-steps of diffusion formulation to only one, and fixing the only time-step ttoT. Addition-\\nally, the single-step formulation is much more computationally efficient. It also naturally prevents\\nthe harmful error propagation as discussed in Sec. 4.1, further enhancing the diffusion’s adaptation\\nperformance in dense prediction.\\n8'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='Input Image Seed 0 Seed 1 Seed 2 Seed 3 Uncertainty Map\\nInput Image Seed 0 Seed 1 Seed 2 Seed 3 Uncertainty Map\\nFigure 9: Depth maps of multiple inferences and uncertainty maps. Areas like the sky, object\\nedges, and intricate details ( e.g., cat whiskers) typically exhibit high uncertainty.\\n4.3 D ETAIL PRESERVER\\nDespite the effectiveness of the above designs, the model still struggles with processing detailed\\nareas (Fig. 8, w/o Preserver). The original diffusion model excels at generating detailed images.\\nHowever, when adapted to predict dense annotations, it can lose such detailed generation ability, due\\nto unexpected catastrophic forgetting [Zhai et al. (2023); Du et al. (2024)]. This leads to challenges\\nin predicting dense annotations in intricate regions.\\nTo preserve the rich details of the input images, we introduce a novel regularization strategy called\\nDetail Preserver . Inspired by previous works [Long et al. (2024); Fu et al. (2024)], we utilize a task\\nswitcher s∈ {sx, sy}, enabling the denoiser model fθto either generate annotation or reconstruct\\nthe input image. When activated by sy, the model focuses on predicting annotation. Conversely,\\nwhen sxis selected, it reconstructs the input image. The switcher sis a one-dimensional vector\\nencoded by the positional encoder and appended with the time embeddings of diffusion model,\\nensuring seamless domain switching without mutual interference. This dual capability enables the\\ndiffusion model to make detailed predictions and thus leading to better performance. Overall, the\\nloss function Ltis:\\nLt=||zx−fθ(zy\\nt,zx, t, sx)||2+||zy−fθ(zy\\nt,zx, t, sy)||2, (7)\\nwhere t=Tand thus zy\\ntis a pure Gaussian noise.\\n4.4 S TOCHASTIC NATURE OF DIFFUSION MODEL\\nImagexℰ\\n𝐳𝐱\\nconcat.𝐳𝑻𝐲denoiserU-Net 𝑓!𝑡=Tswitcher 𝑠\"Prediction#𝒛𝐲𝒟((\\nFigure 10: Inference Pipeline of Lotus. The\\nnoise zy\\nTin bracket is used for Lotus-G and\\nomitted for Lotus-D .One major characteristic of generative models is\\ntheir stochastic nature, which, in image generation,\\nenables the production of diverse outputs. In per-\\nception tasks like dense prediction, this stochasticity\\nhas the potential to allow the model generating pre-\\ndictions with uncertainty maps. Specifically, for any\\ninput image, we can conduct multiple inferences us-\\ning different initialization noises and aggregate these\\npredictions to create its uncertainty map. Thanks\\nto our systematic analysis and tailored fine-tuning\\nprotocol, our method effectively reduces excessive\\nflickering (large variance), only allowing for more\\naccurate uncertainty calculations in naturally uncer-\\ntain areas, such as the sky, object edges, and fine\\ndetails ( e.g.cat whiskers), as shown in Fig. 9.\\nMost existing perception models are deterministic.\\nTo align with these, we can remove the noise input zy\\ntand only input the encoded image features zx\\nto the U-Net denoiser. The model still performs well. In this paper, we finally present two versions\\n9'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='Figure 9: Depth maps of multiple inferences and uncertainty maps. Areas like the sky, object\\nedges, and intricate details ( e.g., cat whiskers) typically exhibit high uncertainty.\\n4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='Despite the effectiveness of the above designs, the model still struggles with processing detailed\\nareas (Fig. 8, w/o Preserver). The original diffusion model excels at generating detailed images.\\nHowever, when adapted to predict dense annotations, it can lose such detailed generation ability, due\\nto unexpected catastrophic forgetting [Zhai et al. (2023); Du et al. (2024)]. This leads to challenges\\nin predicting dense annotations in intricate regions.\\nTo preserve the rich details of the input images, we introduce a novel regularization strategy called'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='Detail Preserver . Inspired by previous works [Long et al. (2024); Fu et al. (2024)], we utilize a task\\nswitcher s∈ {sx, sy}, enabling the denoiser model fθto either generate annotation or reconstruct\\nthe input image. When activated by sy, the model focuses on predicting annotation. Conversely,\\nwhen sxis selected, it reconstructs the input image. The switcher sis a one-dimensional vector\\nencoded by the positional encoder and appended with the time embeddings of diffusion model,\\nensuring seamless domain switching without mutual interference. This dual capability enables the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='diffusion model to make detailed predictions and thus leading to better performance. Overall, the\\nloss function Ltis:\\nLt=||zx−fθ(zy\\nt,zx, t, sx)||2+||zy−fθ(zy\\nt,zx, t, sy)||2, (7)\\nwhere t=Tand thus zy\\ntis a pure Gaussian noise.\\n4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='Imagexℰ\\n𝐳𝐱\\nconcat.𝐳𝑻𝐲denoiserU-Net 𝑓!𝑡=Tswitcher 𝑠\"Prediction#𝒛𝐲𝒟((\\nFigure 10: Inference Pipeline of Lotus. The\\nnoise zy\\nTin bracket is used for Lotus-G and\\nomitted for Lotus-D .One major characteristic of generative models is\\ntheir stochastic nature, which, in image generation,\\nenables the production of diverse outputs. In per-\\nception tasks like dense prediction, this stochasticity\\nhas the potential to allow the model generating pre-\\ndictions with uncertainty maps. Specifically, for any\\ninput image, we can conduct multiple inferences us-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='input image, we can conduct multiple inferences us-\\ning different initialization noises and aggregate these\\npredictions to create its uncertainty map. Thanks\\nto our systematic analysis and tailored fine-tuning\\nprotocol, our method effectively reduces excessive\\nflickering (large variance), only allowing for more\\naccurate uncertainty calculations in naturally uncer-\\ntain areas, such as the sky, object edges, and fine\\ndetails ( e.g.cat whiskers), as shown in Fig. 9.\\nMost existing perception models are deterministic.\\nTo align with these, we can remove the noise input zy'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='To align with these, we can remove the noise input zy\\ntand only input the encoded image features zx\\nto the U-Net denoiser. The model still performs well. In this paper, we finally present two versions\\n9'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='of Lotus: Lotus-G (generative) with noise input and Lotus-D (discriminative) without noise input,\\ncatering to different needs.\\n4.5 I NFERENCE\\nThe inference pipeline is illustrated in Fig. 10. We initialize the annotation map with standard\\nGaussian noise zy\\nT, and encode the input image into its latent code zx. The noise zy\\nTand the image\\nzxare concatenated and fed into the denoiser U-Net model. In our single-step formulation, we\\nsett=Tand the switcher to sy. The denoiser U-Net model then predicts the latent code of the\\nannotation map. The final annotation map is decoded from the predicted latent code via the V AE\\ndecoder. For deterministic prediction, we eliminate the Gaussian noise zy\\nTand only feed the latent\\ncode of the input image into the denoiser.\\n5 E XPERIMENTS\\n5.1 E XPERIMENTAL SETTINGS\\nImplementation details. We implement Lotus based on Stable Diffusion V2 [Rombach et al.\\n(2022)], with text conditioning disabled. During training, we fix the time-step t= 1000 . To optimize\\nthe model, we utilize the standard Adam optimizer with the learning rate 3×10−5. All experiments\\nare conducted on 8 NVIDIA A800 GPUs and the total batch size is 128. For our discriminative vari-\\nant, we train for 4,000 steps, which takes ∼8.1 hours, while for the generative variant, we extend\\ntraining to 10,000 steps, requiring ∼20.3 hours.\\nTraining Datasets. Both depth and normal estimation are trained on two synthetic dataset covering\\nindoor and outdoor scenes.\\n①Hypersim [Roberts et al. (2021)] is a photorealistic synthetic dataset featuring 461 indoor scenes.\\nWe use the official training split, which contains approximately 54K samples. After filtering out\\nincomplete samples, around 39K samples remain, all resized to 576×768for training.\\n②Virtual KITTI [Cabon et al. (2020)] is a synthetic street-scene dataset with five urban scenes under\\nvarious imaging and weather conditions. We utilize four of these scenes for training, comprising\\nabout 20K samples. All samples are cropped to 352×1216 , with the far plane set at 80 meters.\\nFollowing Marigold [Ke et al. (2024)], we employ a mixed dataset strategy for training. For each\\nbatch, we probabilistically choose one of the two datasets and then draw samples from it ( Hypersim\\n90% and Virtual KITTI 10%). This approach yields better performance on both indoor and outdoor\\nreal datasets compared to training on a single synthetic dataset.\\nEvaluation Datasets. ①For affine-invariant depth estimation, we evaluate on 4 real-world datasets\\nthat are not seen during training: NYUv2 [Silberman et al. (2012)) and ScanNet [Dai et al. (2017))\\nall contain images of indoor scenes; KITTI [Geiger et al. (2013)) contains various outdoor scenes;\\nETH3D [Schops et al. (2017)), a high-resolution dataset, containing both indoor and outdoor scenes.\\n②For surface normal prediction, we employ 4 datasets for evaluation: NYUv2 [Silberman et al.\\n(2012)), ScanNet [Dai et al. (2017)), and iBims-1 [Koch et al. (2018)) contain real indoor scenes;\\nSintel [Butler et al. (2012)) contains highly dynamic outdoor scenes.\\nMetrics. ①For affine-invariant depth, we follow the evaluation protocol from [Ranftl et al. (2020);\\nKe et al. (2024); Yang et al. (2024a;b)], aligning the estimated depth predictions with available\\nground truths using least-squares fitting. The accuracy of the aligned predictions is assessed using\\ntheabsolute mean relative error (AbsRel), i.e.,1\\nMPM\\ni=1|ai−di|/di, where Mis the total number\\nof pixels, aiis the predicted depth map and direpresents the ground truth. We also report δ1and\\nδ2, the proportion of pixels satisfying Max (ai/di, di/ai)<1.25and<1.252respectively.\\n②For surface normal, following [Bae & Davison (2024); Ye et al. (2024)], we evaluate the predic-\\ntions of Lotus by measuring the mean angular error for pixels with available ground truth. Addition-\\nally, we report the percentage of pixels with an angular error below 11.25◦and30◦.\\n③For all tasks, we report the Avg. Rank , which indicates the average ranking of each method across\\nvarious datasets and evaluation metrics. A lower value signifies better overall performance.\\n10'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nof Lotus: Lotus-G (generative) with noise input and Lotus-D (discriminative) without noise input,\\ncatering to different needs.\\n4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='The inference pipeline is illustrated in Fig. 10. We initialize the annotation map with standard\\nGaussian noise zy\\nT, and encode the input image into its latent code zx. The noise zy\\nTand the image\\nzxare concatenated and fed into the denoiser U-Net model. In our single-step formulation, we\\nsett=Tand the switcher to sy. The denoiser U-Net model then predicts the latent code of the\\nannotation map. The final annotation map is decoded from the predicted latent code via the V AE\\ndecoder. For deterministic prediction, we eliminate the Gaussian noise zy\\nTand only feed the latent'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='Tand only feed the latent\\ncode of the input image into the denoiser.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='Implementation details. We implement Lotus based on Stable Diffusion V2 [Rombach et al.\\n(2022)], with text conditioning disabled. During training, we fix the time-step t= 1000 . To optimize\\nthe model, we utilize the standard Adam optimizer with the learning rate 3×10−5. All experiments\\nare conducted on'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='.\\nTraining Datasets. Both depth and normal estimation are trained on two synthetic dataset covering\\nindoor and outdoor scenes.\\n①Hypersim [Roberts et al. (2021)] is a photorealistic synthetic dataset featuring 4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='.\\nWe use the official training split, which contains approximately 54K samples. After filtering out\\nincomplete samples, around 39K samples remain, all resized to 576×768for training.\\n②Virtual KITTI [Cabon et al. (2020)] is a synthetic street-scene dataset with five urban scenes under\\nvarious imaging and weather conditions. We utilize four of these scenes for training, comprising\\nabout 20K samples. All samples are cropped to 352×1216 , with the far plane set at'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='.\\nFollowing Marigold [Ke et al. (2024)], we employ a mixed dataset strategy for training. For each\\nbatch, we probabilistically choose one of the two datasets and then draw samples from it ( Hypersim\\n90% and Virtual KITTI 10%). This approach yields better performance on both indoor and outdoor\\nreal datasets compared to training on a single synthetic dataset.\\nEvaluation Datasets. ①For affine-invariant depth estimation, we evaluate on'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='-world datasets\\nthat are not seen during training: NYUv2 [Silberman et al. (2012)) and ScanNet [Dai et al. (2017))\\nall contain images of indoor scenes; KITTI [Geiger et al. (2013)) contains various outdoor scenes;\\nETH3D [Schops et al. (2017)), a high-resolution dataset, containing both indoor and outdoor scenes.\\n②For surface normal prediction, we employ'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content=': NYUv2 [Silberman et al.\\n(2012)), ScanNet [Dai et al. (2017)), and iBims-1 [Koch et al. (2018)) contain real indoor scenes;\\nSintel [Butler et al. (2012)) contains highly dynamic outdoor scenes.\\nMetrics. ①For affine-invariant depth, we follow the evaluation protocol from [Ranftl et al. (2020);\\nKe et al. (2024); Yang et al. (2024a;b)], aligning the estimated depth predictions with available\\nground truths using least-squares fitting. The accuracy of the aligned predictions is assessed using\\ntheabsolute mean relative error (AbsRel), i.e.,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='i=1|ai−di|/di, where Mis the total number\\nof pixels, aiis the predicted depth map and direpresents the ground truth. We also report δ1and\\nδ2, the proportion of pixels satisfying Max (ai/di, di/ai)<1.25and<1.252respectively.\\n②For surface normal, following [Bae & Davison (2024); Ye et al. (2024)], we evaluate the predic-\\ntions of Lotus by measuring the mean angular error for pixels with available ground truth. Addition-\\nally, we report the percentage of pixels with an angular error below 11.25◦and30◦.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='③For all tasks, we report the Avg. Rank , which indicates the average ranking of each method across\\nvarious datasets and evaluation metrics. A lower value signifies better overall performance.\\n10'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='Table 1: Quantitative comparison on zero-shot affine-invariant depth estimation between Lotus\\nand SoTA methods. The upper section lists discriminative methods, the lower lists generative ones.\\nThe best and second best performances are highlighted. Lotus-G outperforms all others methods\\nwhile Lotus-D is slightly inferior to DepthAnything. Please note that DepthAnything is trained on\\n62.6M images while Lotus is only trained on 0.059M images.§indicates results revised by ourselves.\\n⋆denotes the method relies on pre-trained Stable Diffusion.\\nMethodTraining NYUv2 (Indoor) KITTI (Outdoor) ETH3D (Various) ScanNet (Indoor) Avg.\\nData AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑Rank\\nDiverseDepth 320K 11.7 87.5 - 19.0 70.4 - 22.8 69.4 - 10.9 88.2 - 9.5\\nMiDaS 2M 11.1 88.5 - 23.6 63.0 - 18.4 75.2 - 12.1 84.6 - 9.5\\nLeRes 354K 9.0 91.6 - 14.9 78.4 - 17.1 77.7 - 9.1 91.7 - 7.6\\nOmnidata 12.2M 7.4 94.5 - 14.9 83.5 - 16.6 77.8 - 7.5 93.6 - 6.4\\nDPT 1.4M 9.8 90.3 - 10.0 90.1 - 7.8 94.6 - 8.2 93.4 - 5.5\\nHDN 300K 6.9 94.8 - 11.5 86.7 - 12.1 83.3 - 8.0 93.9 - 5.0\\nGenPercept⋆§74K 5.6 96.0 99.2 13.0 84.2 - 7.0 95.6 98.8 6.2 96.1 99.1 3.8\\nDepthAnything V2 62.6M 4.5 97.9 99.3 7.4 94.6 98.6 13.1 86.5 - 4.2 97.8 99.3 2.9\\nLotus-D (Ours)⋆59K 5.3 96.7 99.2 9.3 92.8 98.8 6.8 95.3 98.9 6.0 96.3 99.1 2.5\\nDepthAnything 62.6M 4.3 98.1 99.6 7.6 94.7 99.2 12.7 88.2 - 4.3 98.1 99.6 2.0\\nGeoWizard⋆§280K 5.6 96.3 99.1 14.4 82.0 96.6 6.6 95.8 98.4 6.4 95.0 98.4 3.3\\nMarigold (LCM)§⋆74K 6.1 95.8 99.0 9.8 91.8 98.7 6.8 95.6 99.0 6.9 94.6 98.6 2.9\\nMarigold⋆74K 5.5 96.4 99.1 9.9 91.6 98.7 6.5 95.9 99.0 6.4 95.2 98.8 1.8\\nLotus-G (Ours)⋆59K 5.4 96.6 99.2 11.3 87.7 97.8 6.2 96.1 99.0 6.0 96.0 99.0 1.5\\nTable 2: Quantitative comparison on zero-shot surface normal estimation between Lotus and\\nSoTA methods. Discriminative methods are shown in the upper section, generative methods in the\\nlower. Both Lotus-D andLotus-G outperform all other methods.‡refers the Marigold normal\\nmodel as detailed in this link.⋆denotes the method relies on pre-trained Stable Diffusion.\\nMethodTraining NYUv2 (Indoor) ScanNet (Indoor) iBims-1 (Indoor) Sintel (Outdoor) Avg.\\nData m.↓11.25◦↑30◦↑m.↓11.25◦↑30◦↑m.↓11.25◦↑30◦↑m.↓11.25◦↑30◦↑Rank\\nOASIS 110K 29.2 23.8 60.7 32.8 15.4 52.6 32.6 23.5 57.4 43.1 7.0 35.7 7.0\\nOmnidata 12.2M 23.1 45.8 73.6 22.9 47.4 73.2 19.0 62.1 80.1 41.5 11.4 42.0 5.3\\nEESNU 2.5M 16.2 58.6 83.5 - - - 20.0 58.5 78.2 42.1 11.5 41.2 4.4\\nGenPercept⋆74K 18.2 56.3 81.4 17.7 58.3 82.7 18.2 64.0 82.0 37.6 16.2 51.0 3.7\\nOmnidata V2 12.2M 17.2 55.5 83.0 16.2 60.2 84.7 18.2 63.9 81.1 40.5 14.7 43.5 3.6\\nDSINE 160K 16.4 59.6 83.5 16.2 61.0 84.4 17.1 67.4 82.3 34.9 21.5 52.7 1.8\\nLotus-D (Ours)⋆59K 16.8 58.2 83.6 15.3 62.9 85.7 17.7 64.9 82.5 34.6 20.5 55.8 1.6\\nMarigold‡⋆74K 20.9 50.5 - 21.3 45.6 - 18.5 64.7 - - - - 4.2\\nGeoWizard⋆280K 18.9 50.7 81.5 17.4 53.8 83.5 19.3 63.0 80.3 40.3 12.3 43.5 3.2\\nStableNormal⋆250K 18.6 53.5 81.7 17.1 57.4 84.1 18.2 65.0 82.4 36.7 14.1 50.7 2.0\\nLotus-G (Ours)∗59K 16.9 59.1 83.2 15.3 64.0 85.2 17.5 66.1 82.7 35.2 19.9 54.8 1.0\\n5.2 Q UALITATIVE AND QUANTITATIVE COMPARISONS\\nDepth Estimation. As shown in Tab. 1, Lotus-G achieves the best comprehensive performance\\ncompared to all generative baselines on zero-shot affine-invariant depth estimation. Notice that we\\nonly require single step denoising process, significantly boosting the inference speed as shown in\\nTable 3: Ablation studies on the step-by-step design of our adaptation protocol for fitting pre-trained\\ndiffusion models into dense prediction. Here we show the results in monocular depth estimation.\\nMethodTraining NYUv2 (Indoor) KITTI (Outdoor) ETH3D (Various) ScanNet (Indoor)\\nData AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑\\nDirect Adaptation 39K 11.551 87.692 96.122 20.164 70.403 90.996 19.894 76.464 87.960 15.726 78.885 93.651\\n+x0-prediction 39K 8.332 92.769 97.941 17.008 74.969 93.611 11.075 87.952 94.978 10.212 89.130 97.181\\n+Single Time-step 39K 5.587 96.272 99.113 13.262 83.210 97.237 7.586 94.143 97.678 6.262 95.394 98.791\\n+Detail Preserver 39K 5.555 96.303 99.118 13.170 83.657 97.454 7.147 95.000 98.058 6.201 95.470 98.814\\n+Mixture Dataset (Lotus-G )59K 5.425 96.597 99.156 11.324 87.692 97.780 6.172 96.077 98.980 6.024 96.026 99.730\\n−Noise Input ( Lotus-D ) 59K 5.311 96.733 99.186 9.662 91.637 98.643 6.757 95.382 98.992 5.786 96.339 99.136\\n11'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nTable 1: Quantitative comparison on zero-shot affine-invariant depth estimation between Lotus\\nand SoTA methods. The upper section lists discriminative methods, the lower lists generative ones.\\nThe best and second best performances are highlighted. Lotus-G outperforms all others methods\\nwhile Lotus-D is slightly inferior to DepthAnything. Please note that DepthAnything is trained on\\n62.6M images while Lotus is only trained on 0.059M images.§indicates results revised by ourselves.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='⋆denotes the method relies on pre-trained Stable Diffusion.\\nMethodTraining NYUv2 (Indoor) KITTI (Outdoor) ETH3D (Various) ScanNet (Indoor) Avg.\\nData AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑Rank\\nDiverseDepth 320K 11.7 87.5 - 19.0 70.4 - 22.8 69.4 - 10.9 88.2 - 9.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='2: Quantitative comparison on zero-shot surface normal estimation between Lotus and\\nSoTA methods. Discriminative methods are shown in the upper section, generative methods in the\\nlower. Both Lotus-D andLotus-G outperform all other methods.‡refers the Marigold normal\\nmodel as detailed in this link.⋆denotes the method relies on pre-trained Stable Diffusion.\\nMethodTraining NYUv2 (Indoor) ScanNet (Indoor) iBims-1 (Indoor) Sintel (Outdoor) Avg.\\nData m.↓11.25◦↑30◦↑m.↓11.25◦↑30◦↑m.↓11.25◦↑30◦↑m.↓11.25◦↑30◦↑Rank\\nOASIS 110K 29.2 23.8 60.7 32.8 15.4 52.6 32.6 23.5 57.4 43.1 7.0 35.7 7.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='Depth Estimation. As shown in Tab. 1, Lotus-G achieves the best comprehensive performance\\ncompared to all generative baselines on zero-shot affine-invariant depth estimation. Notice that we\\nonly require single step denoising process, significantly boosting the inference speed as shown in\\nTable 3: Ablation studies on the step-by-step design of our adaptation protocol for fitting pre-trained\\ndiffusion models into dense prediction. Here we show the results in monocular depth estimation.\\nMethodTraining NYUv2 (Indoor) KITTI (Outdoor) ETH3D (Various) ScanNet (Indoor)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='Data AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑\\nDirect Adaptation 39K 11.551 87.692 96.122 20.164 70.403 90.996 19.894 76.464 87.960 15.726 78.885 93.651\\n+x0-prediction 39K 8.332 92.769 97.941 17.008 74.969 93.611 11.075 87.952 94.978 10.212 89.130 97.181\\n+Single Time-step 39K 5.587 96.272 99.113 13.262 83.210 97.237 7.586 94.143 97.678 6.262 95.394 98.791\\n+Detail Preserver 39K 5.555 96.303 99.118 13.170 83.657 97.454 7.147 95.000 98.058 6.201 95.470 98.814\\n+Mixture Dataset (Lotus-G )59K 5.425 96.597 99.156 11.324 87.692 97.780 6.172 96.077 98.980 6.024 96.026 99.730'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='−Noise Input ( Lotus-D ) 59K 5.311 96.733 99.186 9.662 91.637 98.643 6.757 95.382 98.992 5.786 96.339 99.136\\n11'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 11}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 11}, page_content='Input Image DepthAnythingV2Marigold(LCM)Lotus-DLotus-GGround TruthNYUv2KITTIScanNetETH3D\\nFigure 11: Qualitative comparison on zero-shot affine-invariant depth estimation. Lotus\\ndemonstrates higher accuracy especially in detailed areas.\\nInput Image DSINEStableNormalLotus-DLotus-GGround TruthNYUv2ScanNetiBims-1Sintel\\nFigure 12: Qualitative comparison on zero-shot surface normal estimation. Lotus offers im-\\nproved accuracy particularly in complex regions.\\nFig. 2. Lotus-D also performs well, though it is slightly inferior to DepthAnything. However,\\nit is worthy to notice that Lotus is trained on only 0.059M images compared to DepthAnything’s\\n62.6M images. In Fig. 11, we further compare the performance of our Lotus with other methods\\nin detailed areas. The quantitative results obviously demonstrate that our method can produce much\\nfiner and more accurate depth predictions, particularly in complex regions with intricate structures,\\nwhich sometimes cannot be reflected by the metrics.\\nNormal Estimation. In Tab. 2, both Lotus-G andLotus-D outperform all other methods on zero-\\nshot surface normal estimation. Also, as illustrated in Fig. 12, Lotus consistently provides accu-\\nrate surface normal predictions, effectively handling complex geometries and diverse environments,\\nhighlighting its robustness on fine-grained prediction.\\n12'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 11}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nInput Image DepthAnythingV2Marigold(LCM)Lotus-DLotus-GGround TruthNYUv2KITTIScanNetETH3D\\nFigure 11: Qualitative comparison on zero-shot affine-invariant depth estimation. Lotus\\ndemonstrates higher accuracy especially in detailed areas.\\nInput Image DSINEStableNormalLotus-DLotus-GGround TruthNYUv2ScanNetiBims-1Sintel\\nFigure 12: Qualitative comparison on zero-shot surface normal estimation. Lotus offers im-\\nproved accuracy particularly in complex regions.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 11}, page_content='proved accuracy particularly in complex regions.\\nFig. 2. Lotus-D also performs well, though it is slightly inferior to DepthAnything. However,\\nit is worthy to notice that Lotus is trained on only 0.059M images compared to DepthAnything’s\\n62.6M images. In Fig. 11, we further compare the performance of our Lotus with other methods\\nin detailed areas. The quantitative results obviously demonstrate that our method can produce much\\nfiner and more accurate depth predictions, particularly in complex regions with intricate structures,\\nwhich sometimes cannot be reflected by the metrics.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 11}, page_content='which sometimes cannot be reflected by the metrics.\\nNormal Estimation. In Tab. 2, both Lotus-G andLotus-D outperform all other methods on zero-\\nshot surface normal estimation. Also, as illustrated in Fig. 12, Lotus consistently provides accu-\\nrate surface normal predictions, effectively handling complex geometries and diverse environments,\\nhighlighting its robustness on fine-grained prediction.\\n12'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='5.3 A BLATION STUDY\\nAs shown in Tab. 3, we conduct ablation studies to validate our designs. Starting with “Direct\\nAdaptation”, we incrementally test the effects of different components, such as parameterization\\ntypes, the single-step diffusion process, and the detail preserver. Initially, we train the model using\\nonly the Hypersim dataset to establish a baseline. We then expand the training dataset using a\\nmixture dataset strategy by including Virtual KITTI , aiming to enhance the model’s generalization\\nability across different domains. The findings from these ablations validate the effectiveness of our\\nproposed adaptation protocol, demonstrating that each design plays a vital role in optimizing the\\ndiffusion models for dense prediction tasks.\\n6 C ONCLUSION AND FUTURE WORKS\\nIn this paper, we introduce Lotus, a diffusion-based visual foundation model for dense prediction.\\nThrough systematic analysis and specifically tailored diffusion formulation, Lotus finds a way to\\nbetter fit the rich visual prior from pre-trained diffusion models into dense prediction. Extensive\\nexperiments demonstrate that Lotus achieves SoTA performance on zero-shot depth and normal\\nestimation with minimal training data, paving the way of various practical applications.\\nFuture Work. While we have applied Lotus to two geometric dense prediction tasks, it can be\\nseamlessly adapted to other dense prediction tasks requiring per-pixel alignment with great poten-\\ntial, such as panoramic segmentation and image matting. Additionally, our performance is slightly\\nbehind DepthAnything [Yang et al. (2024a)] which utilizes large-scale training data. In the future,\\nscaling up the training data, as reveal in Fig. 7 and Tab. 3 (“Mixture Dataset”), has great potential to\\nfurther enhance Lotus’s performance.\\nREFERENCES\\nGilwon Bae and Andrew J Davison. Aleatoric uncertainty in monocular surface normal estimation.\\nIEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , pp. 1472–1485, 2021.\\nGilwon Bae and Andrew J Davison. Rethinking inductive biases for surface normal estimation.\\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2024.\\nYaniv Benny and Lior Wolf. Dynamic dual-output diffusion models. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 11482–11491, 2022.\\nDaniel J Butler, Jonas Wulff, Garrett B Stanley, and Michael J Black. A naturalistic open source\\nmovie for optical flow evaluation. In Computer Vision–ECCV 2012: 12th European Conference\\non Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI 12 , pp. 611–625.\\nSpringer, 2012.\\nYohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint\\narXiv:2001.10773 , 2020.\\nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James\\nKwok, Ping Luo, Huchuan Lu, et al. Pixart- α: Fast training of diffusion transformer for photore-\\nalistic text-to-image synthesis. arXiv preprint arXiv:2310.00426 , 2023.\\nWeifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton, and Jia Deng. Oasis: A\\nlarge-scale dataset for single image 3d in the wild. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pp. 679–688, 2020.\\nAngela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias\\nNießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the\\nIEEE conference on computer vision and pattern recognition , pp. 5828–5839, 2017.\\nWenyu Du, Shuang Cheng, Tongxu Luo, Zihan Qiu, Zeyu Huang, Ka Chun Cheung, Reynold\\nCheng, and Jie Fu. Unlocking continual learning abilities in language models. arXiv preprint\\narXiv:2406.17245 , 2024.\\n13'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='As shown in Tab. 3, we conduct ablation studies to validate our designs. Starting with “Direct\\nAdaptation”, we incrementally test the effects of different components, such as parameterization\\ntypes, the single-step diffusion process, and the detail preserver. Initially, we train the model using\\nonly the Hypersim dataset to establish a baseline. We then expand the training dataset using a\\nmixture dataset strategy by including Virtual KITTI , aiming to enhance the model’s generalization\\nability across different domains. The findings from these ablations validate the effectiveness of our'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='proposed adaptation protocol, demonstrating that each design plays a vital role in optimizing the\\ndiffusion models for dense prediction tasks.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='In this paper, we introduce Lotus, a diffusion-based visual foundation model for dense prediction.\\nThrough systematic analysis and specifically tailored diffusion formulation, Lotus finds a way to\\nbetter fit the rich visual prior from pre-trained diffusion models into dense prediction. Extensive\\nexperiments demonstrate that Lotus achieves SoTA performance on zero-shot depth and normal\\nestimation with minimal training data, paving the way of various practical applications.\\nFuture Work. While we have applied Lotus to two geometric dense prediction tasks, it can be'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='seamlessly adapted to other dense prediction tasks requiring per-pixel alignment with great poten-\\ntial, such as panoramic segmentation and image matting. Additionally, our performance is slightly\\nbehind DepthAnything [Yang et al. (2024a)] which utilizes large-scale training data. In the future,\\nscaling up the training data, as reveal in Fig.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='. 3 (“Mixture Dataset”), has great potential to\\nfurther enhance Lotus’s performance.\\nREFERENCES\\nGilwon Bae and Andrew J Davison. Aleatoric uncertainty in monocular surface normal estimation.\\nIEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , pp. 1472–1485, 2021.\\nGilwon Bae and Andrew J Davison. Rethinking inductive biases for surface normal estimation.\\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2024.\\nYaniv Benny and Lior Wolf. Dynamic dual-output diffusion models. In Proceedings of the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 11482–11491, 2022.\\nDaniel J Butler, Jonas Wulff, Garrett B Stanley, and Michael J Black. A naturalistic open source\\nmovie for optical flow evaluation. In Computer Vision–ECCV 2012: 12th European Conference\\non Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI 12 , pp. 611–625.\\nSpringer, 2012.\\nYohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint\\narXiv:2001.10773 , 2020.\\nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='Kwok, Ping Luo, Huchuan Lu, et al. Pixart- α: Fast training of diffusion transformer for photore-\\nalistic text-to-image synthesis. arXiv preprint arXiv:2310.00426 , 2023.\\nWeifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton, and Jia Deng. Oasis: A\\nlarge-scale dataset for single image 3d in the wild. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pp. 679–688, 2020.\\nAngela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the\\nIEEE conference on computer vision and pattern recognition , pp. 5828–5839, 2017.\\nWenyu Du, Shuang Cheng, Tongxu Luo, Zihan Qiu, Zeyu Huang, Ka Chun Cheung, Reynold\\nCheng, and Jie Fu. Unlocking continual learning abilities in language models. arXiv preprint\\narXiv:2406.17245 , 2024.\\n13'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: A scalable pipeline\\nfor making multi-task mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision , pp. 10786–10796, 2021.\\nDavid Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using\\na multi-scale deep network. Advances in neural information processing systems , 27, 2014.\\nHuan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal\\nregression network for monocular depth estimation. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pp. 2002–2011, 2018.\\nXiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and\\nXiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from a\\nsingle image. arXiv preprint arXiv:2403.12013 , 2024.\\nAndreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The\\nkitti dataset. The International Journal of Robotics Research , 32(11):1231–1237, 2013.\\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information\\nprocessing systems , 27, 2014.\\nJing He, Yiyi Zhou, Qi Zhang, Jun Peng, Yunhang Shen, Xiaoshuai Sun, Chao Chen, and Rongrong\\nJi. Pixelfolder: An efficient progressive pixel synthesis network for image generation. arXiv\\npreprint arXiv:2204.00833 , 2022.\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\\nneural information processing systems , 33:6840–6851, 2020.\\nMu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang\\nYu, Chunhua Shen, and Shaojie Shen. Metric3d v2: A versatile monocular geometric\\nfoundation model for zero-shot metric depth and surface normal estimation. arXiv preprint\\narXiv:2404.15506 , 2024.\\nYihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du,\\nTianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 17853–17862, 2023.\\nBinbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting\\nfor geometrically accurate radiance fields. In SIGGRAPH 2024 Conference Papers . Association\\nfor Computing Machinery, 2024. doi: 10.1145/3641519.3657428.\\nO˘guzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and Amir Zamir. 3d common corruptions and data\\naugmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pp. 18963–18974, 2022.\\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\\nadversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition , pp. 4401–4410, 2019.\\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-\\ning and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition , pp. 8110–8119, 2020.\\nTero Karras, Miika Aittala, Samuli Laine, Erik H ¨ark¨onen, Janne Hellsten, Jaakko Lehtinen, and\\nTimo Aila. Alias-free generative adversarial networks. Advances in Neural Information Process-\\ning Systems , 34:852–863, 2021.\\nBingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Kon-\\nrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation.\\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.\\n9492–9502, 2024.\\n14'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nAinaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: A scalable pipeline\\nfor making multi-task mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision , pp. 10786–10796, 2021.\\nDavid Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using\\na multi-scale deep network. Advances in neural information processing systems , 27, 2014.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal\\nregression network for monocular depth estimation. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pp. 2002–2011, 2018.\\nXiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and\\nXiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from a\\nsingle image. arXiv preprint arXiv:2403.12013 , 2024.\\nAndreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='kitti dataset. The International Journal of Robotics Research , 32(11):1231–1237, 2013.\\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information\\nprocessing systems , 27, 2014.\\nJing He, Yiyi Zhou, Qi Zhang, Jun Peng, Yunhang Shen, Xiaoshuai Sun, Chao Chen, and Rongrong\\nJi. Pixelfolder: An efficient progressive pixel synthesis network for image generation. arXiv\\npreprint arXiv:2204.00833 , 2022.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='preprint arXiv:2204.00833 , 2022.\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\\nneural information processing systems , 33:6840–6851, 2020.\\nMu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang\\nYu, Chunhua Shen, and Shaojie Shen. Metric3d v2: A versatile monocular geometric\\nfoundation model for zero-shot metric depth and surface normal estimation. arXiv preprint\\narXiv:2404.15506 , 2024.\\nYihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 17853–17862, 2023.\\nBinbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting\\nfor geometrically accurate radiance fields. In SIGGRAPH 20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='. Association\\nfor Computing Machinery, 2024. doi: 10.1145/3641519.3657428.\\nO˘guzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and Amir Zamir. 3d common corruptions and data\\naugmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pp. 18963–18974, 2022.\\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\\nadversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition , pp. 4401–4410, 2019.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='recognition , pp. 4401–4410, 2019.\\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-\\ning and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition , pp. 8110–8119, 2020.\\nTero Karras, Miika Aittala, Samuli Laine, Erik H ¨ark¨onen, Janne Hellsten, Jaakko Lehtinen, and\\nTimo Aila. Alias-free generative adversarial networks. Advances in Neural Information Process-\\ning Systems , 34:852–863, 2021.\\nBingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Kon-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='rad Schindler. Repurposing diffusion-based image generators for monocular depth estimation.\\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.\\n9492–9502, 2024.\\n14'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco Korner. Evaluation of cnn-based\\nsingle-image depth estimation methods. In Proceedings of the European Conference on Computer\\nVision (ECCV) Workshops , pp. 0–0, 2018.\\nHsin-Ying Lee, Hung-Yu Tseng, and Ming-Hsuan Yang. Exploiting diffusion prior for generalizable\\ndense prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pp. 7861–7871, 2024.\\nJin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale\\nlocal planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326 , 2019.\\nJiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic\\ngaussian fusion from casual videos via 4d motion scaffolds. arXiv preprint arXiv:2405.17421 ,\\n2024.\\nXiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma,\\nSong-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d\\nusing cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pp. 9970–9980, 2024.\\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741 , 2021.\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen,\\nand Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine\\nLearning , pp. 8821–8831. PMLR, 2021.\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125 , 1(2):3, 2022.\\nRen´e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust\\nmonocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transac-\\ntions on pattern analysis and machine intelligence , 44(3):1623–1637, 2020.\\nRen´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.\\nInProceedings of the IEEE/CVF international conference on computer vision , pp. 12179–12188,\\n2021.\\nMike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan\\nPaczan, Russ Webb, and Joshua M Susskind. Hypersim: A photorealistic synthetic dataset for\\nholistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference\\non computer vision , pp. 10912–10922, 2021.\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-\\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\\nence on computer vision and pattern recognition , pp. 10684–10695, 2022.\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-\\nical image segmentation. In Medical image computing and computer-assisted intervention–\\nMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceed-\\nings, part III 18 , pp. 234–241. Springer, 2015.\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\\ntion Processing Systems , 35:36479–36494, 2022.\\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv\\npreprint arXiv:2202.00512 , 2022.\\n15'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nTobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco Korner. Evaluation of cnn-based\\nsingle-image depth estimation methods. In Proceedings of the European Conference on Computer\\nVision (ECCV) Workshops , pp. 0–0, 2018.\\nHsin-Ying Lee, Hung-Yu Tseng, and Ming-Hsuan Yang. Exploiting diffusion prior for generalizable\\ndense prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pp. 7861–7871, 2024.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='Recognition , pp. 7861–7871, 2024.\\nJin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale\\nlocal planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326 , 2019.\\nJiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic\\ngaussian fusion from casual videos via 4d motion scaffolds. arXiv preprint arXiv:2405.17421 ,\\n2024.\\nXiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma,\\nSong-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pp. 9970–9980, 2024.\\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741 , 2021.\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen,\\nand Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='Learning , pp. 8821–8831. PMLR, 2021.\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125 , 1(2):3, 2022.\\nRen´e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust\\nmonocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transac-\\ntions on pattern analysis and machine intelligence , 44(3):1623–1637, 2020.\\nRen´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='InProceedings of the IEEE/CVF international conference on computer vision , pp. 12179–12188,\\n2021.\\nMike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan\\nPaczan, Russ Webb, and Joshua M Susskind. Hypersim: A photorealistic synthetic dataset for\\nholistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference\\non computer vision , pp. 10912–10922, 2021.\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\\nence on computer vision and pattern recognition , pp. 10684–10695, 2022.\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-\\nical image segmentation. In Medical image computing and computer-assisted intervention–\\nMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceed-\\nings, part III 18 , pp. 234–241. Springer, 2015.\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\\ntion Processing Systems , 35:36479–36494, 2022.\\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv\\npreprint arXiv:2202.00512 , 2022.\\n15'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='Thomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc\\nPollefeys, and Andreas Geiger. A multi-view stereo benchmark with high-resolution images and\\nmulti-camera videos. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition , pp. 3260–3269, 2017.\\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\\nopen large-scale dataset for training next generation image-text models. Advances in Neural\\nInformation Processing Systems , 35:25278–25294, 2022.\\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and sup-\\nport inference from rgbd images. In Computer Vision–ECCV 2012: 12th European Conference\\non Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V 12 , pp. 746–760.\\nSpringer, 2012.\\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\\npreprint arXiv:2010.02502 , 2020.\\nYunzhou Song, Jiahui Lei, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Track everything every-\\nwhere fast and robustly, 2024.\\nQianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of\\nmotion: 4d reconstruction from a single video. arXiv preprint arXiv:2407.13764 , 2024.\\nYuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou.\\nSpatialtracker: Tracking any 2d pixels in 3d space. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , 2024.\\nGuangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen,\\nand Chunhua Shen. Diffusion models trained with large data are transferable visual models. arXiv\\npreprint arXiv:2403.06090 , 2024.\\nTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong\\nHe. Attngan: Fine-grained text to image generation with attentional generative adversarial net-\\nworks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\\n1316–1324, 2018.\\nLihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth\\nanything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , pp. 10371–10381, 2024a.\\nLihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang\\nZhao. Depth anything v2. arXiv preprint arXiv:2406.09414 , 2024b.\\nChongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang\\nXiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal.\\narXiv preprint arXiv:2406.16864 , 2024.\\nWei Yin, Yifan Liu, and Chunhua Shen. Virtual normal: Enforcing geometric constraints for ac-\\ncurate and robust depth prediction. IEEE Transactions on Pattern Analysis and Machine Intelli-\\ngence , 44(10):7282–7295, 2021a.\\nWei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua\\nShen. Learning to recover 3d scene shape from a single image. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , pp. 204–213, 2021b.\\nWei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua\\nShen. Metric3d: Towards zero-shot metric 3d prediction from a single image. In Proceedings of\\nthe IEEE/CVF International Conference on Computer Vision , pp. 9043–9053, 2023.\\nWeihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and Ping Tan. Neural window fully-connected\\ncrfs for monocular depth estimation. In Proceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition , pp. 3916–3925, 2022.\\n16'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nThomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc\\nPollefeys, and Andreas Geiger. A multi-view stereo benchmark with high-resolution images and\\nmulti-camera videos. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition , pp. 3260–3269, 2017.\\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='open large-scale dataset for training next generation image-text models. Advances in Neural\\nInformation Processing Systems , 35:25278–25294, 2022.\\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and sup-\\nport inference from rgbd images. In Computer Vision–ECCV 2012: 12th European Conference\\non Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V 12 , pp. 746–760.\\nSpringer, 2012.\\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\\npreprint arXiv:2010.02502 , 2020.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='preprint arXiv:2010.02502 , 2020.\\nYunzhou Song, Jiahui Lei, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Track everything every-\\nwhere fast and robustly, 2024.\\nQianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of\\nmotion: 4d reconstruction from a single video. arXiv preprint arXiv:2407.13764 , 2024.\\nYuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou.\\nSpatialtracker: Tracking any 2d pixels in 3d space. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , 2024.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='on Computer Vision and Pattern Recognition (CVPR) , 2024.\\nGuangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen,\\nand Chunhua Shen. Diffusion models trained with large data are transferable visual models. arXiv\\npreprint arXiv:2403.06090 , 2024.\\nTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong\\nHe. Attngan: Fine-grained text to image generation with attentional generative adversarial net-\\nworks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\\n1316–1324, 2018.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='1316–1324, 2018.\\nLihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth\\nanything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , pp. 10371–10381, 2024a.\\nLihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang\\nZhao. Depth anything v2. arXiv preprint arXiv:2406.09414 , 2024b.\\nChongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='Xiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal.\\narXiv preprint arXiv:2406.16864 , 2024.\\nWei Yin, Yifan Liu, and Chunhua Shen. Virtual normal: Enforcing geometric constraints for ac-\\ncurate and robust depth prediction. IEEE Transactions on Pattern Analysis and Machine Intelli-\\ngence , 44(10):7282–7295, 2021a.\\nWei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua\\nShen. Learning to recover 3d scene shape from a single image. In Proceedings of the IEEE/CVF'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='Conference on Computer Vision and Pattern Recognition , pp. 204–213, 2021b.\\nWei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua\\nShen. Metric3d: Towards zero-shot metric 3d prediction from a single image. In Proceedings of\\nthe IEEE/CVF International Conference on Computer Vision , pp. 9043–9053, 2023.\\nWeihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and Ping Tan. Neural window fully-connected\\ncrfs for monocular depth estimation. In Proceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition , pp. 3916–3925, 2022.\\n16'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 16}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 16}, page_content='Ekim Yurtsever, Jacob Lambert, Alexander Carballo, and Kazuya Takeda. A survey of autonomous\\ndriving: Common practices and emerging technologies. IEEE access , 8:58443–58469, 2020.\\nYuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. In-\\nvestigating the catastrophic forgetting in multimodal large language models. arXiv preprint\\narXiv:2309.10313 , 2023.\\nChi Zhang, Wei Yin, Billzb Wang, Gang Yu, Bin Fu, and Chunhua Shen. Hierarchical normalization\\nfor robust monocular depth estimation. Advances in Neural Information Processing Systems , 35:\\n14128–14139, 2022.\\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dim-\\nitris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative ad-\\nversarial networks. In Proceedings of the IEEE international conference on computer vision , pp.\\n5907–5915, 2017.\\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dim-\\nitris N Metaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial net-\\nworks. IEEE transactions on pattern analysis and machine intelligence , 41(8):1947–1962, 2018.\\nHan Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive\\nlearning for text-to-image generation. In Proceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition , pp. 833–842, 2021.\\n17'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 16}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nEkim Yurtsever, Jacob Lambert, Alexander Carballo, and Kazuya Takeda. A survey of autonomous\\ndriving: Common practices and emerging technologies. IEEE access , 8:58443–58469, 2020.\\nYuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. In-\\nvestigating the catastrophic forgetting in multimodal large language models. arXiv preprint\\narXiv:2309.10313 , 2023.\\nChi Zhang, Wei Yin, Billzb Wang, Gang Yu, Bin Fu, and Chunhua Shen. Hierarchical normalization'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 16}, page_content='for robust monocular depth estimation. Advances in Neural Information Processing Systems , 35:\\n14128–14139, 2022.\\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dim-\\nitris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative ad-\\nversarial networks. In Proceedings of the IEEE international conference on computer vision , pp.\\n5907–5915, 2017.\\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dim-\\nitris N Metaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial net-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 16}, page_content='works. IEEE transactions on pattern analysis and machine intelligence , 41(8):1947–1962, 2018.\\nHan Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive\\nlearning for text-to-image generation. In Proceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition , pp. 833–842, 2021.\\n17'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 0}, page_content='LLaV A-3D: A Simple yet Effective Pathway to Empowering'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 0}, page_content='LMMs with 3D-awareness\\nChenming Zhu1,2Tai Wang2,†Wenwei Zhang2Jiangmiao Pang2Xihui Liu1,†\\n1The University of Hong Kong2Shanghai AI Laboratory\\nhttps://zcmax.github.io/projects/LLaVA-3D\\n†corresponding author\\nFigure 1. Overview of LLaV A-3D. Block (a) shows that LLaV A-3D could perform both 2D and 3D vision-language tasks. The left block (b)\\nshows that compared with previous 3D LMMs, our LLaV A-3D achieves state-of-the-art performance across a wide range of 3D benchmarks\\nwhile maintaining a comparable performance on various 2D benchmarks compared with LLaV A-1.5. The middle block (c) demonstrates that\\nLLaV A-3D is built on the 2D LMM: LLaV A, and leverages 3D patches to endow it with 3D spatial awareness, enabling it to perform various\\n3D vision-and-language tasks in the physical world. The right blocks (d) and (e) highlights the significantly faster convergence and inference\\nspeeds of LLaV A-3D compared to existing 3D LMMs.\\nAbstract\\nRecent advancements in Large Multimodal Models\\n(LMMs) have greatly enhanced their proficiency in 2D visual\\nunderstanding tasks, enabling them to effectively process and\\nunderstand images and videos. However, the development of\\nLMMs with 3D-awareness for 3D scene understanding has\\nbeen hindered by the lack of large-scale 3D vision-language\\ndatasets and powerful 3D encoders. In this paper, we in-troduce a simple yet effective framework called LLaVA-3D .\\nLeveraging the strong 2D understanding priors from LLaVA,\\nour LLaVA-3D efficiently adapts LLaVA for 3D scene under-\\nstanding without compromising 2D understanding capabili-\\nties. To achieve this, we employ a simple yet effective repre-\\nsentation, 3D Patch , which connects 2D CLIP patch features\\nwith their corresponding positions in 3D space. By integrat-\\ning the 3D Patches into 2D LMMs and employing joint 2D\\nand 3D vision-language instruction tuning, we establish aarXiv:2409.18125v1  [cs.CV]  26 Sep 2024'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 0}, page_content='LLaV A-3D: A Simple yet Effective Pathway to Empowering\\nLMMs with 3D-awareness\\nChenming Zhu1,2Tai Wang2,†Wenwei Zhang2Jiangmiao Pang2Xihui Liu1,†\\n1The University of Hong Kong2Shanghai AI Laboratory\\nhttps://zcmax.github.io/projects/LLaVA-3D\\n†corresponding author\\nFigure 1. Overview of LLaV A-3D. Block (a) shows that LLaV A-3D could perform both 2D and 3D vision-language tasks. The left block (b)\\nshows that compared with previous 3D LMMs, our LLaV A-3D achieves state-of-the-art performance across a wide range of 3D benchmarks'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 0}, page_content='while maintaining a comparable performance on various 2D benchmarks compared with LLaV A-1.5. The middle block (c) demonstrates that\\nLLaV A-3D is built on the 2D LMM: LLaV A, and leverages 3D patches to endow it with 3D spatial awareness, enabling it to perform various\\n3D vision-and-language tasks in the physical world. The right blocks (d) and (e) highlights the significantly faster convergence and inference\\nspeeds of LLaV A-3D compared to existing 3D LMMs.\\nAbstract\\nRecent advancements in Large Multimodal Models\\n(LMMs) have greatly enhanced their proficiency in 2D visual'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 0}, page_content='(LMMs) have greatly enhanced their proficiency in 2D visual\\nunderstanding tasks, enabling them to effectively process and\\nunderstand images and videos. However, the development of\\nLMMs with 3D-awareness for 3D scene understanding has\\nbeen hindered by the lack of large-scale 3D vision-language\\ndatasets and powerful 3D encoders. In this paper, we in-troduce a simple yet effective framework called LLaVA-3D .\\nLeveraging the strong 2D understanding priors from LLaVA,\\nour LLaVA-3D efficiently adapts LLaVA for 3D scene under-\\nstanding without compromising 2D understanding capabili-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 0}, page_content='standing without compromising 2D understanding capabili-\\nties. To achieve this, we employ a simple yet effective repre-\\nsentation, 3D Patch , which connects 2D CLIP patch features\\nwith their corresponding positions in 3D space. By integrat-\\ning the 3D Patches into 2D LMMs and employing joint 2D\\nand 3D vision-language instruction tuning, we establish aarXiv:2409.18125v'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='unified architecture for both 2D image understanding and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='3D scene understanding. Experimental results show that\\nLLaVA-3D converges 3.5 ×faster than existing 3D LMMs\\nwhen trained on 3D vision-language datasets. Moreover,\\nLLaVA-3D not only achieves state-of-the-art performance\\nacross various 3D tasks but also maintains comparable 2D\\nimage understanding and vision-language conversation ca-\\npabilities with LLaVA.\\n1. Introduction\\nRecent advancements in Large Multimodal Models\\n(LMMs) [ 2,8,30,43] have significantly enhanced their\\nability to understand and reason over visual and language\\ninputs, leading to remarkable performance in 2D visual tasks,\\nsuch as 2D perception, understanding, and reasoning. De-\\nspite their advanced perceptual and reasoning capabilities,\\nLMMs are primarily confined to virtual interactions through\\nimages or video, lacking the critical ability to interact with\\nthe physical world. To enable their deployment in real-world\\napplications and to facilitate the emergence of new capabili-\\nties through physical interactions, it is imperative to equip\\nLMMs with 3D spatial intelligence.\\nA key aspect of 3D spatial intelligence is the ability to\\nperceive and understand the 3D world. Similar to how 2D\\nLMMs align 2D visual features with language models us-\\ning large-scale 2D vision-language datasets, a common ap-\\nproach to developing 3D LMMs [ 12,19,20] involves inte-\\ngrating 3D features encoded from point clouds into Large\\nLanguage Models (LLMs) and training them on 3D point\\ncloud-language datasets. However, in contrast to the abun-\\ndance of large-scale 2D datasets, 3D datasets remain rela-\\ntively scarce. Meanwhile, there are no powerful pre-trained\\n3D point cloud encoders, akin to CLIP ViT [ 42] in 2D, to\\nserve as a foundational model that can provide strong 3D\\nfeatures to LLMs.\\nSince real-world embodied agents typically rely on ego-\\ncentric, multi-view images as their raw observations, we aim\\nto build a 3D foundation model based on such inputs rather\\nthan 3D point clouds. There have been attempts [ 17,18]\\nto leverage the 2D foundation models, like CLIP, alongside\\nLLMs to advance this goal. These methods resort to 2D\\nobject segmentation results [ 26] to extract and aggregate\\nCLIP features from object-centric image patches, construct-\\ning pixel-aligned 3D scene features [ 23]. However, this\\nmulti-view image object segmentation and object-centric\\nfeature extraction pipeline is inherently complex and compu-\\ntationally intensive. In contrast, 2D LMMs [ 2,8,30,37,38]\\ndirectly leverage CLIP’s image patch features with richer,\\nfine-grained information for effective image understanding\\nand visual reasoning. This naturally leads to the question:\\nCan we directly build a 3D LMM upon the strong 2D priors\\nfrom 2D LMMs, bypassing the obstacles in 3D data scaleand 3D encoders ?\\nIn light of recent progress in 2D LMMs, we propose\\na simple yet effective framework, LLaV A-3D, which ex-\\ntends the well-established 2D LLaV A model to efficiently\\ncomprehend the 3D world while preserving its robust 2D\\nmultimodal perception and reasoning capabilities. The core\\ninnovation in our approach is the introduction of 3D Patch ,\\na new 3D representation that bridges 2D features within a\\n3D spatial context. This representation is derived by sim-\\nply augmenting 2D patch-wise features with 3D positional\\nembeddings, eliminating the need for additional complex\\nprocessing. After enhancing the 2D visual tokens to 3D\\npatch features, we explore various pooling strategies to com-\\npress the 3D patch features across extensive input frames\\nbefore being fed into LLM. Fine-tuned on the existing 3D\\nvision-language datasets, our model converges rapidly and\\nacquires 3D spatial understanding and grounding capabil-\\nities. Furthermore, the unified model architecture allows\\nLLaV A-3D to retain the strong 2D understanding and rea-\\nsoning abilities of LLaV A through joint instruction-tuning\\non 2D vision-language datasets.\\nOur experimental results demonstrate that LLaV A-3D\\nachieves state-of-the-art performance on a wide range of\\n3D tasks and benchmarks [ 3,6,13,39–41,51], including\\n3D captioning, 3D question answering, and 3D grounding\\nwhile requiring significantly less training time and fewer\\nepochs than existing 3D LMMs. Additionally, LLaV A-3D\\nachieves comparable capabilities in 2D image understanding,\\nreasoning, and conversation to LLaV A through joint tuning\\non 2D and 3D vision-language instructions.\\n2. Related Work\\n2D LMMs. Building on the success of recent LLMs, nu-\\nmerous studies [ 2,8,30,35,37,38] explored LMMs that\\ncan jointly process visual and linguistic information. For\\nexample, LLaV A [ 37,38] aligned 2D images with language\\nmodels through an image encoder and a projection layer,\\nwhile BLIP2 [ 30] employed a more sophisticated Q-Former\\narchitecture to guide the compression of visual features using\\ntextual cues. However, most early 2D LMMs were trained\\non single-image datasets, limiting their ability to tackle the\\ncomplexities of multi-image understanding. Recently, there\\nhas been increasing interest in expanding LMMs to handle\\nmulti-image inputs, addressing the demands of real-world\\nscenarios. For video LMMs [ 28,31,34,47], multi-image\\ninput forms the basis for capturing temporal or action-related\\ndynamics across sequences of video frames. On the other\\nhand, multi-view images of the 3D scene can implicitly re-\\nveal 3D spatial relationships and other abstract relations in\\nthe environment. Recent works [ 36,41] explored whether\\n2D LMMs [ 1,44] can leverage multi-view images to perform\\nspatial understanding. However, these methods primarily\\nrelied on implicit learning from the data, without directly'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='unified architecture for both 2D image understanding and\\n3D scene understanding. Experimental results show that\\nLLaVA-3D converges 3.5 ×faster than existing 3D LMMs\\nwhen trained on 3D vision-language datasets. Moreover,\\nLLaVA-3D not only achieves state-of-the-art performance\\nacross various 3D tasks but also maintains comparable 2D\\nimage understanding and vision-language conversation ca-\\npabilities with LLaVA.\\n1. Introduction\\nRecent advancements in Large Multimodal Models\\n(LMMs) [ 2,8,30,43] have significantly enhanced their\\nability to understand and reason over visual and language'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='ability to understand and reason over visual and language\\ninputs, leading to remarkable performance in 2D visual tasks,\\nsuch as 2D perception, understanding, and reasoning. De-\\nspite their advanced perceptual and reasoning capabilities,\\nLMMs are primarily confined to virtual interactions through\\nimages or video, lacking the critical ability to interact with\\nthe physical world. To enable their deployment in real-world\\napplications and to facilitate the emergence of new capabili-\\nties through physical interactions, it is imperative to equip\\nLMMs with 3D spatial intelligence.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='LMMs with 3D spatial intelligence.\\nA key aspect of 3D spatial intelligence is the ability to\\nperceive and understand the 3D world. Similar to how 2D\\nLMMs align 2D visual features with language models us-\\ning large-scale 2D vision-language datasets, a common ap-\\nproach to developing 3D LMMs [ 12,19,20] involves inte-\\ngrating 3D features encoded from point clouds into Large\\nLanguage Models (LLMs) and training them on 3D point\\ncloud-language datasets. However, in contrast to the abun-\\ndance of large-scale 2D datasets, 3D datasets remain rela-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='dance of large-scale 2D datasets, 3D datasets remain rela-\\ntively scarce. Meanwhile, there are no powerful pre-trained\\n3D point cloud encoders, akin to CLIP ViT [ 42] in 2D, to\\nserve as a foundational model that can provide strong 3D\\nfeatures to LLMs.\\nSince real-world embodied agents typically rely on ego-\\ncentric, multi-view images as their raw observations, we aim\\nto build a 3D foundation model based on such inputs rather\\nthan 3D point clouds. There have been attempts [ 17,18]\\nto leverage the 2D foundation models, like CLIP, alongside\\nLLMs to advance this goal. These methods resort to 2D'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='LLMs to advance this goal. These methods resort to 2D\\nobject segmentation results [ 26] to extract and aggregate\\nCLIP features from object-centric image patches, construct-\\ning pixel-aligned 3D scene features [ 23]. However, this\\nmulti-view image object segmentation and object-centric\\nfeature extraction pipeline is inherently complex and compu-\\ntationally intensive. In contrast, 2D LMMs [ 2,8,30,37,38]\\ndirectly leverage CLIP’s image patch features with richer,\\nfine-grained information for effective image understanding\\nand visual reasoning. This naturally leads to the question:'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='and visual reasoning. This naturally leads to the question:\\nCan we directly build a 3D LMM upon the strong 2D priors\\nfrom 2D LMMs, bypassing the obstacles in 3D data scaleand 3D encoders ?\\nIn light of recent progress in 2D LMMs, we propose\\na simple yet effective framework, LLaV A-3D, which ex-\\ntends the well-established 2D LLaV A model to efficiently\\ncomprehend the 3D world while preserving its robust 2D\\nmultimodal perception and reasoning capabilities. The core\\ninnovation in our approach is the introduction of 3D Patch ,\\na new 3D representation that bridges 2D features within a'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='a new 3D representation that bridges 2D features within a\\n3D spatial context. This representation is derived by sim-\\nply augmenting 2D patch-wise features with 3D positional\\nembeddings, eliminating the need for additional complex\\nprocessing. After enhancing the 2D visual tokens to 3D\\npatch features, we explore various pooling strategies to com-\\npress the 3D patch features across extensive input frames\\nbefore being fed into LLM. Fine-tuned on the existing 3D\\nvision-language datasets, our model converges rapidly and\\nacquires 3D spatial understanding and grounding capabil-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='acquires 3D spatial understanding and grounding capabil-\\nities. Furthermore, the unified model architecture allows\\nLLaV A-3D to retain the strong 2D understanding and rea-\\nsoning abilities of LLaV A through joint instruction-tuning\\non 2D vision-language datasets.\\nOur experimental results demonstrate that LLaV A-3D\\nachieves state-of-the-art performance on a wide range of\\n3D tasks and benchmarks [ 3,6,13,39–41,51], including\\n3D captioning, 3D question answering, and 3D grounding\\nwhile requiring significantly less training time and fewer\\nepochs than existing 3D LMMs. Additionally, LLaV A-3D'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='epochs than existing 3D LMMs. Additionally, LLaV A-3D\\nachieves comparable capabilities in 2D image understanding,\\nreasoning, and conversation to LLaV A through joint tuning\\non 2D and 3D vision-language instructions.\\n2. Related Work\\n2D LMMs. Building on the success of recent LLMs, nu-\\nmerous studies [ 2,8,30,35,37,38] explored LMMs that\\ncan jointly process visual and linguistic information. For\\nexample, LLaV A [ 37,38] aligned 2D images with language\\nmodels through an image encoder and a projection layer,\\nwhile BLIP2 [ 30] employed a more sophisticated Q-Former'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='while BLIP2 [ 30] employed a more sophisticated Q-Former\\narchitecture to guide the compression of visual features using\\ntextual cues. However, most early 2D LMMs were trained\\non single-image datasets, limiting their ability to tackle the\\ncomplexities of multi-image understanding. Recently, there\\nhas been increasing interest in expanding LMMs to handle\\nmulti-image inputs, addressing the demands of real-world\\nscenarios. For video LMMs [ 28,31,34,47], multi-image\\ninput forms the basis for capturing temporal or action-related\\ndynamics across sequences of video frames. On the other'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='dynamics across sequences of video frames. On the other\\nhand, multi-view images of the 3D scene can implicitly re-\\nveal 3D spatial relationships and other abstract relations in\\nthe environment. Recent works [ 36,41] explored whether\\n2D LMMs [ 1,44] can leverage multi-view images to perform\\nspatial understanding. However, these methods primarily\\nrelied on implicit learning from the data, without directly'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='modeling the 3D world. In contrast, our LLaV A-3D explic-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='itly models the 3D world from multi-view images, enabling\\nadvanced 3D spatial understanding and grounding capabili-\\nties.\\nInjecting 3D into LLMs. As 2D LMMs achieved substan-\\ntial progress in visual perception, similar efforts have been\\nmade in the 3D domain. For 3D scene-level understanding,\\nrecent works explored ways to integrate 3D inputs such as\\npoint clouds [ 12,19,20] or multi-view images [ 17,18] into\\nLLMs to enable advanced 3D scene understanding and rea-\\nsoning. An important distinction among these methods is\\nhow they construct the 3D scene representation. LL3DA [ 12]\\ndirectly used a scene-level 3D point cloud encoder to extract\\nthe 3D scene representation. LEO [20] and Chat3D v2 [19]\\nfirst segmented 3D objects from the scene point cloud us-\\ning the off-the-shelf 3D instance segmentation model, and\\nthen independently extracted 3D object features with object-\\nlevel 3D encoders to represent the 3D scene. On the other\\nhand, starting from multi-view images, 3D-LLM [ 18] and\\nScene-LLM [ 17] resorted to manually crafted 2D object\\nsegmentation to extract and aggregate CLIP features from\\nobject-centric image patches, constructing pixel-aligned 3D\\npoint representation. Unlike these approaches, our LLaV A-\\n3D directly builds on the well-trained 2D LMM with multi-\\nview images as input. Utilizing the 3D position embeddings,\\nit brings the the 2D patches within a 3D spatial context to\\nconstruct 3D Patches. This 3D representation enables quick\\nadaption of LLaV A for 3D scene understanding while pre-\\nserving its strong 2D image understanding ability.\\n3. Method\\nPrevious 2D LMMs typically consist of a visual encoder\\nto extract 2D image features, which are then aligned with\\nthe LLM via the projection layer for joint visual and lan-\\nguage reasoning tasks. In this section, we introduce how to\\nbridge the 2D image features within 3D spatial context to\\nconstruct 3D patches (Sec. 3.1, 3.2), and then demonstrate\\nthe 3D-aware pooling strategies to compress the 3D patches\\n(Sec. 3.2) and finally present the 3D-aware position encoding\\nand decoding process (Sec. 3.4), as illustrated in Fig. 2.\\n3.1. Preliminary\\nWe choose LLaV A [ 38], a 2D LMM, to explore building a\\n3D LMM based on it. LLaV A uses the pre-trained CLIP\\nencoder to extract the 2D patch features Xv∈Rc×w×h\\nfrom the input image X∈R3×W×H, and then align the 2D\\npatch features Xvinto with LLM space with the projection\\nlayer. A simple multi-view image adaptation [ 29] for LLaV A\\ncould be extracting multi-view 2D Patch features from multi-\\nview images X′\\nv∈RV×c×w×hand sequentially feeding\\nthem into LLM for multi-image understanding. To empowerLLaV A with 3D awareness, we introduce 3D Patch , a novel\\n3D representation that integrates 3D spatial information into\\n2D patch features.\\n3.2. 3D Patch\\nOur 3D Patch representations are built upon the 2D patch\\nfeatures X′\\nvextracted from multi-view images with CLIP\\nvisual encoder to leverage the strong visual-semantic align-\\nment. To construct the 3D Patches, we inject the 3D po-\\nsition information into the aforementioned 2d patches so\\nthat the 3D Patches can explicitly model 3D spatial infor-\\nmation while preserving the semantic information from 2D\\npatches. As illustrated in left block of Fig. 2, given the\\nmulti-view 2D patch features X′\\nv∈RV×c×w×h, we obtain\\ntheir 3D positions P∈RV×3×w×hin the 3D world, using\\nnearest neighbor depth and known camera intrinsic and ex-\\ntrinsic parameters. The 3D positions Pare then encoded\\ninto 3D position embeddings P′∈RV×w×h×dthrough\\na learnable two-layer MLP, which are subsequently added\\nto the 2D patch visual tokens, resulting in the 3D patches\\nX′\\n3D∈RV×w×h×d:\\nX′\\n3D=X′\\nv+MLP (P′) (1)\\n3.3. 3D Patch Pooling\\nWhile the 3D Patches equip 2D patches with 3D spatial\\nawareness, the number of 3D patches scales directly with\\nthe number of input images. Capturing a full 3D scene\\noften necessitates a large set of images, which significantly\\nincreases the computational overhead for large language\\nmodels (LLMs). To address this, we introduce a 3D-aware\\npooling mechanism to reduce the number of 3D patches, as\\nillustrated in the middle block of Fig. 2.\\nIn the 2D image or video domain, pooling is commonly\\napplied along the 2D spatial or temporal dimensions to com-\\npress the number of tokens and extract essential semantic\\ninformation. However, for 3D scene understanding, we must\\npool the 3D patches based on their 3D locations to ensure\\nthese features can cover and preserve the entire scene’s struc-\\nture as completely as possible. We explore two parameter-\\nfree pooling strategies to achieve this:\\nVoxelization Pooling. V oxelization discretizes the 3D\\nspace into a volumetric grid. Within each occupied voxel,\\nthe 3D patches undergo average pooling, resulting in updated\\nvoxel visual tokens. Only the visual tokens from occupied\\nvoxels are passed to the LLM, with the number of tokens\\nvarying across different 3D scenes. While the number of 3D\\npatches scales with the number of images, the number of\\nvoxel patches is only determined by the partition of voxel\\ngrids. We can easily balance the number of visual tokens and\\nthe preservation of fine-grained scene features by controlling\\nthe voxel size.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='modeling the 3D world. In contrast, our LLaV A-3D explic-\\nitly models the 3D world from multi-view images, enabling\\nadvanced 3D spatial understanding and grounding capabili-\\nties.\\nInjecting 3D into LLMs. As 2D LMMs achieved substan-\\ntial progress in visual perception, similar efforts have been\\nmade in the 3D domain. For 3D scene-level understanding,\\nrecent works explored ways to integrate 3D inputs such as\\npoint clouds [ 12,19,20] or multi-view images [ 17,18] into\\nLLMs to enable advanced 3D scene understanding and rea-\\nsoning. An important distinction among these methods is'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='soning. An important distinction among these methods is\\nhow they construct the 3D scene representation. LL3DA [ 12]\\ndirectly used a scene-level 3D point cloud encoder to extract\\nthe 3D scene representation. LEO [20] and Chat3D v2 [19]\\nfirst segmented 3D objects from the scene point cloud us-\\ning the off-the-shelf 3D instance segmentation model, and\\nthen independently extracted 3D object features with object-\\nlevel 3D encoders to represent the 3D scene. On the other\\nhand, starting from multi-view images, 3D-LLM [ 18] and\\nScene-LLM [ 17] resorted to manually crafted 2D object'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='Scene-LLM [ 17] resorted to manually crafted 2D object\\nsegmentation to extract and aggregate CLIP features from\\nobject-centric image patches, constructing pixel-aligned 3D\\npoint representation. Unlike these approaches, our LLaV A-\\n3D directly builds on the well-trained 2D LMM with multi-\\nview images as input. Utilizing the 3D position embeddings,\\nit brings the the 2D patches within a 3D spatial context to\\nconstruct 3D Patches. This 3D representation enables quick\\nadaption of LLaV A for 3D scene understanding while pre-\\nserving its strong 2D image understanding ability.\\n3. Method'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='3. Method\\nPrevious 2D LMMs typically consist of a visual encoder\\nto extract 2D image features, which are then aligned with\\nthe LLM via the projection layer for joint visual and lan-\\nguage reasoning tasks. In this section, we introduce how to\\nbridge the 2D image features within 3D spatial context to\\nconstruct 3D patches (Sec. 3.1, 3.2), and then demonstrate\\nthe 3D-aware pooling strategies to compress the 3D patches\\n(Sec. 3.2) and finally present the 3D-aware position encoding\\nand decoding process (Sec. 3.4), as illustrated in Fig. 2.\\n3.1. Preliminary'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='3.1. Preliminary\\nWe choose LLaV A [ 38], a 2D LMM, to explore building a\\n3D LMM based on it. LLaV A uses the pre-trained CLIP\\nencoder to extract the 2D patch features Xv∈Rc×w×h\\nfrom the input image X∈R3×W×H, and then align the 2D\\npatch features Xvinto with LLM space with the projection\\nlayer. A simple multi-view image adaptation [ 29] for LLaV A\\ncould be extracting multi-view 2D Patch features from multi-\\nview images X′\\nv∈RV×c×w×hand sequentially feeding\\nthem into LLM for multi-image understanding. To empowerLLaV A with 3D awareness, we introduce 3D Patch , a novel'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='3D representation that integrates 3D spatial information into\\n2D patch features.\\n3.2. 3D Patch\\nOur 3D Patch representations are built upon the 2D patch\\nfeatures X′\\nvextracted from multi-view images with CLIP\\nvisual encoder to leverage the strong visual-semantic align-\\nment. To construct the 3D Patches, we inject the 3D po-\\nsition information into the aforementioned 2d patches so\\nthat the 3D Patches can explicitly model 3D spatial infor-\\nmation while preserving the semantic information from 2D\\npatches. As illustrated in left block of Fig. 2, given the\\nmulti-view 2D patch features X′'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='multi-view 2D patch features X′\\nv∈RV×c×w×h, we obtain\\ntheir 3D positions P∈RV×3×w×hin the 3D world, using\\nnearest neighbor depth and known camera intrinsic and ex-\\ntrinsic parameters. The 3D positions Pare then encoded\\ninto 3D position embeddings P′∈RV×w×h×dthrough\\na learnable two-layer MLP, which are subsequently added\\nto the 2D patch visual tokens, resulting in the 3D patches\\nX′\\n3D∈RV×w×h×d:\\nX′\\n3D=X′\\nv+MLP (P′) (1)\\n3.3. 3D Patch Pooling\\nWhile the 3D Patches equip 2D patches with 3D spatial\\nawareness, the number of 3D patches scales directly with'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='awareness, the number of 3D patches scales directly with\\nthe number of input images. Capturing a full 3D scene\\noften necessitates a large set of images, which significantly\\nincreases the computational overhead for large language\\nmodels (LLMs). To address this, we introduce a 3D-aware\\npooling mechanism to reduce the number of 3D patches, as\\nillustrated in the middle block of Fig. 2.\\nIn the 2D image or video domain, pooling is commonly\\napplied along the 2D spatial or temporal dimensions to com-\\npress the number of tokens and extract essential semantic'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='press the number of tokens and extract essential semantic\\ninformation. However, for 3D scene understanding, we must\\npool the 3D patches based on their 3D locations to ensure\\nthese features can cover and preserve the entire scene’s struc-\\nture as completely as possible. We explore two parameter-\\nfree pooling strategies to achieve this:\\nVoxelization Pooling. V oxelization discretizes the 3D\\nspace into a volumetric grid. Within each occupied voxel,\\nthe 3D patches undergo average pooling, resulting in updated\\nvoxel visual tokens. Only the visual tokens from occupied'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='voxel visual tokens. Only the visual tokens from occupied\\nvoxels are passed to the LLM, with the number of tokens\\nvarying across different 3D scenes. While the number of 3D\\npatches scales with the number of images, the number of\\nvoxel patches is only determined by the partition of voxel\\ngrids. We can easily balance the number of visual tokens and\\nthe preservation of fine-grained scene features by controlling\\nthe voxel size.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='Figure 2. LLaV A-3D Architecture. Based on LLaV A, we directly add the corresponding 3D position embeddings to 2D patch visual tokens'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='of multi-view images to construct the 3D Patches, then the 3D Patches will undergo 3D pooling and be sent into the projection layer of\\nLLaV A to map into the LLM space and align with the LLM using 3D-visual-language data.\\nFPS Pooling. Farthest Point Sampling (FPS) is a widely\\nused sampling strategy to select a representative subset of\\npoints from a larger set of points cloud. We apply FPS\\nto sample 3D patches from multi-view images to a fixed\\nnumber of tokens, ensuring that the sampled tokens represent\\nthe entire scene structure. While fixing the number of tokens\\nhelps the LLM efficiently process visual information, it may\\nalso result in loss of scene information.\\nCompared to FPS Pooling, V oxelization Pooling offers equiv-\\nalent efficiency in visual token compression while preserving\\nmore detailed scene information. Furthermore, the explicit\\nconstruction of voxel grids can better handle dynamic 3D\\nscene updates, whereas FPS Pooling excels at preserving the\\noverall 3D scene structure. We conduct quantitative experi-\\nments in Sec. 5.7 to thoroughly assess the effectiveness of\\nthese pooling strategies.\\n3.4. 3D-aware Position Encoding & Decoding\\nIn the previous sections, we detailed the construction of the\\n3D scene representation from multi-view images, establish-\\ning the foundation for further interaction with the 3D scene.\\nBuilding on this, the LLM could process multi-modal inputs\\nsuch as the 3D scene, language instructions, and 3D coordi-\\nnate cues to generate outputs such as language responses and\\n3D bounding boxes, as illustrated in the right block of Fig. 2.\\nIn this section, we introduce how the model is equipped to\\ninterpret 3D coordinate information from inputs and sub-\\nsequently output precise 3D bounding boxes when specific\\nlocation-related task requirements are needed.\\nEncoding of 3D Coordinate Input. In scenarios such\\nas 3D dense object captioning or object-centric question\\nanswering, the language instruction contains 3D coordinates.\\nTo handle such tasks, we introduce the 3D Coordinate Token\\nto allow the model to integrate the provided coordinates\\nas context into its reasoning processes. Specifically, we\\nobtain the 3D coordinate token by feeding the 3D coordinatesthrough a two-layer MLP. These 3D coordinate tokens are\\nfed into LLM together with 3D Patch tokens and text tokens,\\nenabling 3D-aware perception and reasoning.\\nDecoding of 3D Bounding Box Output. The integration\\nof the 3D coordinate token empowers the model to process\\n3D coordinate information from input instructions effec-\\ntively. However, experiments show that directly outputting\\n3D bounding boxes is quite challenging for LLM, and em-\\npirically, the performance is poor. To overcome this, we\\nleverage an approach similar to previous method [ 51], intro-\\nducing a specialized location token that guides the grounding\\nmodule to generate accurate 3D bounding boxes. Specifi-\\ncally, the LLM predicts a special location token to represent\\na 3D box prediction when the task necessitates 3D bounding\\nbox outputs. We then derive the last layer embedding of this\\nlocation token and send it into the grounding module. The\\ngrounding module utilizes the location token embedding and\\nthe 3D scene feature to predict the 3D box coordinates of the\\ntarget object. This process facilitates the precise generation\\nof 3D bounding box outputs. More details can be found in\\nthe appendix.\\n4. Training\\nTo leverage the 2D priors from established 2D LMMs, we\\ntrain our LLaV A-3D model based on the pre-trained LLaV A-\\n1.5. Considering the scarcity of 3D scene-language data, our\\ntraining procedure comprises two stages, each focusing on\\ndifferent training targets of the model.\\nStage 1: 3D Patch Language Alignment. During the\\nfirst training stage, we use the region-level and scene-level\\ncaption data that describe spatial relationships among 3D ob-\\njects to align the 3D patches with the LLM for enhanced 3D\\nspatial comprehension. At this stage, the input multi-view\\nimages used are selected from sequences that correspond'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='Figure 2. LLaV A-3D Architecture. Based on LLaV A, we directly add the corresponding 3D position embeddings to 2D patch visual tokens\\nof multi-view images to construct the 3D Patches, then the 3D Patches will undergo 3D pooling and be sent into the projection layer of\\nLLaV A to map into the LLM space and align with the LLM using 3D-visual-language data.\\nFPS Pooling. Farthest Point Sampling (FPS) is a widely\\nused sampling strategy to select a representative subset of\\npoints from a larger set of points cloud. We apply FPS\\nto sample 3D patches from multi-view images to a fixed'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='to sample 3D patches from multi-view images to a fixed\\nnumber of tokens, ensuring that the sampled tokens represent\\nthe entire scene structure. While fixing the number of tokens\\nhelps the LLM efficiently process visual information, it may\\nalso result in loss of scene information.\\nCompared to FPS Pooling, V oxelization Pooling offers equiv-\\nalent efficiency in visual token compression while preserving\\nmore detailed scene information. Furthermore, the explicit\\nconstruction of voxel grids can better handle dynamic 3D\\nscene updates, whereas FPS Pooling excels at preserving the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='scene updates, whereas FPS Pooling excels at preserving the\\noverall 3D scene structure. We conduct quantitative experi-\\nments in Sec. 5.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='these pooling strategies.\\n3.4. 3D-aware Position Encoding & Decoding\\nIn the previous sections, we detailed the construction of the\\n3D scene representation from multi-view images, establish-\\ning the foundation for further interaction with the 3D scene.\\nBuilding on this, the LLM could process multi-modal inputs\\nsuch as the 3D scene, language instructions, and 3D coordi-\\nnate cues to generate outputs such as language responses and\\n3D bounding boxes, as illustrated in the right block of Fig. 2.\\nIn this section, we introduce how the model is equipped to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='In this section, we introduce how the model is equipped to\\ninterpret 3D coordinate information from inputs and sub-\\nsequently output precise 3D bounding boxes when specific\\nlocation-related task requirements are needed.\\nEncoding of 3D Coordinate Input. In scenarios such\\nas 3D dense object captioning or object-centric question\\nanswering, the language instruction contains 3D coordinates.\\nTo handle such tasks, we introduce the 3D Coordinate Token\\nto allow the model to integrate the provided coordinates\\nas context into its reasoning processes. Specifically, we'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='as context into its reasoning processes. Specifically, we\\nobtain the 3D coordinate token by feeding the 3D coordinatesthrough a two-layer MLP. These 3D coordinate tokens are\\nfed into LLM together with 3D Patch tokens and text tokens,\\nenabling 3D-aware perception and reasoning.\\nDecoding of 3D Bounding Box Output. The integration\\nof the 3D coordinate token empowers the model to process\\n3D coordinate information from input instructions effec-\\ntively. However, experiments show that directly outputting\\n3D bounding boxes is quite challenging for LLM, and em-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='3D bounding boxes is quite challenging for LLM, and em-\\npirically, the performance is poor. To overcome this, we\\nleverage an approach similar to previous method [ 51], intro-\\nducing a specialized location token that guides the grounding\\nmodule to generate accurate 3D bounding boxes. Specifi-\\ncally, the LLM predicts a special location token to represent\\na 3D box prediction when the task necessitates 3D bounding\\nbox outputs. We then derive the last layer embedding of this\\nlocation token and send it into the grounding module. The\\ngrounding module utilizes the location token embedding and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='grounding module utilizes the location token embedding and\\nthe 3D scene feature to predict the 3D box coordinates of the\\ntarget object. This process facilitates the precise generation\\nof 3D bounding box outputs. More details can be found in\\nthe appendix.\\n4. Training\\nTo leverage the 2D priors from established 2D LMMs, we\\ntrain our LLaV A-3D model based on the pre-trained LLaV A-\\n1.5. Considering the scarcity of 3D scene-language data, our\\ntraining procedure comprises two stages, each focusing on\\ndifferent training targets of the model.\\nStage 1: 3D Patch Language Alignment. During the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='Stage 1: 3D Patch Language Alignment. During the\\nfirst training stage, we use the region-level and scene-level\\ncaption data that describe spatial relationships among 3D ob-\\njects to align the 3D patches with the LLM for enhanced 3D\\nspatial comprehension. At this stage, the input multi-view\\nimages used are selected from sequences that correspond'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='Figure 3. LLaV A-3D-Instruct-1M. The hybrid 2D and 3D Dataset'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='Collection. Left: Distribution of data across categories, with the\\nouter circle representing all categories and the inner circle illustrat-\\ning data subset distribution. Right: Detailed dataset quantities.\\nto specific regions or entire scenes. We freeze the vision\\nencoder and LLM parameters, and only train the projection\\nlayer and 3D position embedding layer, encouraging effi-\\ncient alignment between 3D patch features and text space.\\nSince 3D patches are derived from CLIP features augmented\\nwith 3D positional information, the alignment between 3D\\nPatch and LLM converges rapidly.\\nStage 2: Task Instruction Tuning. During the instruction-\\ntuning stage, LLaV A-3D is optimized to respond to complex\\n3D V&L tasks and maintain its inherent 2D image reason-\\ning and instruction-following capabilities. To facilitate this\\ncapability, we collect the LLaV A-3D-Instruct-1M dataset,\\na hybrid collection of 2D and 3D data specifically tailored\\nfor instruction tuning. The overall distribution of the dataset\\ncollection is shown in Fig 3, more details about the instruc-\\ntional tuning datasets are listed in the appendix. The 2D Data\\nof LLaV A-3D-Instruct-1M is derived from existing LLaV A-\\n1.5 instruction tuning data, ensuring the preservation of 2D\\nimage comprehension and vision-language conversation abil-\\nities. When tuning with 2D data, we keep the vision encoder\\nfrozen and jointly train the projection layer and LLM. The\\n3D Data of LLaV A-3D-Instruct-1M, on the other hand, com-\\nprises data from diverse 3D QA, 3D dense captioning, and\\n3D grounding tasks. During the 3D data instruction tuning,\\nthe 3D position embedding layer will be added to jointly train\\nwith the other modules. Additionally, for tasks where the\\ninstruction contains 3D coordinate information or requires\\n3D bounding box outputs, the corresponding encoding and\\ndecoding modules will be trained together. During instruc-\\ntion tuning, the 3D data pathway includes the 3D position\\nembeddings and 3D patches, while the 2D data pathway is\\nthe original LLaV A. All modules except for the 3D position\\nembeddings to construct 3D patches are shared across 2D\\nand 3D data. This training setup ensures that LLaV A-3D\\nis capable of processing both 2D and 3D visual tokens ef-\\nfectively, and is adaptive to various task formulations andTable 1. Quantitative comparison with SOTA models on various\\n3D QA tasks . “C” stands for “CIDEr”, “B-4” for “BLEU-4”, “M”\\nfor “METEOR”, “R” for “ROUGE”, “Sim” for sentence similarity,\\nand “EM@1” for top-1 exact match. Gray indicates evaluation\\nresults with refined exact-match protocol.\\nScanQA (val) SQA3D (test)\\nC B-4 M R EM@1 EM@1\\nTask-specific models\\nScan2Cap [13] - - - - - 41.0†\\nScanRefer+MCAN [49] 55.4 7.9 11.5 30.0 18.6 -\\nClipBERT [27] - - - - - 43.3\\nScanQA [3] 64.9 10.1 13.1 33.3 21.1 47.2\\n3D-VisTA [52] 69.6 10.4 13.9 35.7 22.4 48.5\\nTask-specific fine-tuned 3D LMMs\\n3D-LLM (FlanT5) [18] 69.4 12.0 14.5 35.7 20.5\\nLL3DA [37] 76.8 13.5 15.9 37.3 -\\nChat-3D v2 [19] 87.6 14.0 - - - 54.7\\nLEO [20] 101.4 13.2 20.0 49.2 24.5 (47.6) 50.0 (52.4)\\nScene-LLM [17] 80 12.0 16.6 40.0 27.2 54.2\\nZero-shot 2D LMMs\\nVideoChat2 [32] 49.2 9.6 9.5 28.2 19.2 37.3\\nLLaV A-NeXT-Video [28] 46.2 9.8 9.1 27.8 18.7 34.2\\nGPT-4V 59.6 - 13.5 33.4 - -\\nGemini 68.3 - 11.3 35.4 - -\\nClaude 57.7 - 10.0 29.3 - -\\nLLaV A-3D 91.7 14.5 20.7 50.1 27.0 (45.0) 55.6 (57.6)\\nrequirements.\\n5. Experiments\\nIn this section, we conduct extensive evaluations to examine\\nthe capabilities of LLaV A-3D. To begin with, we introduce\\nthe implementation details (Sec. 5.1). Then, we compare\\nour model’s 3D scene understanding (Sec. 5.2, 5.3, 5.4)and\\n2D image understanding (Sec. 5.5) capability with previous\\nmethods. Finally, we conduct a thorough analysis to vali-\\ndate the effectiveness of the components and designs of our\\nLLaV A-3D (Sec. 5.6, 5.7).\\n5.1. Implementation Details\\nLLaV A-3D is built upon the LLaV A-1.5-7B, utilizing their\\npre-trained weights from the HuggingFace library. For 3D\\ntasks, we add the 3D position embeddings to the 2D patch\\nvisual tokens, and utilize the voxelization pooling strategy\\nto reduce 3D patch number before passing the input visual\\ntokens to the projection layer and LLM. The number of\\nviewsVis set to be 20 and voxel size is set to 0.2m. Due\\nto the LLM context length limitation, the maximum number\\nof 3D patch tokens after 3D-aware pooling is set to 3096.\\nFor 2D tasks, LLaV A-3D functions the same as LLaV A. All\\nexperiments are conducted on 8 ×80G A100 GPUs. We\\ntrain our model for 1 epoch with a learning rate of 1e-3 and\\na batch size of 32 in stage 1, and fine-tune on the collected\\nLLaV A-3D-Instruct-1M dataset, with a learning rate of 2e-5\\nand a batch size of 16 in stage 2.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='Figure 3. LLaV A-3D-Instruct-1M. The hybrid 2D and 3D Dataset\\nCollection. Left: Distribution of data across categories, with the\\nouter circle representing all categories and the inner circle illustrat-\\ning data subset distribution. Right: Detailed dataset quantities.\\nto specific regions or entire scenes. We freeze the vision\\nencoder and LLM parameters, and only train the projection\\nlayer and 3D position embedding layer, encouraging effi-\\ncient alignment between 3D patch features and text space.\\nSince 3D patches are derived from CLIP features augmented'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='Since 3D patches are derived from CLIP features augmented\\nwith 3D positional information, the alignment between 3D\\nPatch and LLM converges rapidly.\\nStage 2: Task Instruction Tuning. During the instruction-\\ntuning stage, LLaV A-3D is optimized to respond to complex\\n3D V&L tasks and maintain its inherent 2D image reason-\\ning and instruction-following capabilities. To facilitate this\\ncapability, we collect the LLaV A-3D-Instruct-1M dataset,\\na hybrid collection of 2D and 3D data specifically tailored\\nfor instruction tuning. The overall distribution of the dataset'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='collection is shown in Fig 3, more details about the instruc-\\ntional tuning datasets are listed in the appendix. The 2D Data\\nof LLaV A-3D-Instruct-1M is derived from existing LLaV A-\\n1.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content=', ensuring the preservation of 2D\\nimage comprehension and vision-language conversation abil-\\nities. When tuning with 2D data, we keep the vision encoder\\nfrozen and jointly train the projection layer and LLM. The\\n3D Data of LLaV A-3D-Instruct-1M, on the other hand, com-\\nprises data from diverse 3D QA, 3D dense captioning, and\\n3D grounding tasks. During the 3D data instruction tuning,\\nthe 3D position embedding layer will be added to jointly train\\nwith the other modules. Additionally, for tasks where the\\ninstruction contains 3D coordinate information or requires'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='instruction contains 3D coordinate information or requires\\n3D bounding box outputs, the corresponding encoding and\\ndecoding modules will be trained together. During instruc-\\ntion tuning, the 3D data pathway includes the 3D position\\nembeddings and 3D patches, while the 2D data pathway is\\nthe original LLaV A. All modules except for the 3D position\\nembeddings to construct 3D patches are shared across 2D\\nand 3D data. This training setup ensures that LLaV A-3D\\nis capable of processing both 2D and 3D visual tokens ef-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='is capable of processing both 2D and 3D visual tokens ef-\\nfectively, and is adaptive to various task formulations andTable 1. Quantitative comparison with SOTA models on various\\n3D QA tasks . “C” stands for “CIDEr”, “B-4” for “BLEU-4”, “M”\\nfor “METEOR”, “R” for “ROUGE”, “Sim” for sentence similarity,\\nand “EM@1” for top-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='. Gray indicates evaluation\\nresults with refined exact-match protocol.\\nScanQA (val) SQA3D (test)\\nC B-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='-specific models\\nScan2Cap [13] - - - - - 41.0†\\nScanRefer+MCAN [49] 55.4 7.9 11.5 30.0 18.6 -\\nClipBERT [27] - - - - - 43.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='-4V 59.6 - 13.5 33.4 - -\\nGemini 68.3 - 11.3 35.4 - -\\nClaude 57.7 - 10.0 29.3 - -\\nLLaV A-3D 91.7 14.5 20.7 50.1 27.0 (45.0) 55.6 (57.6)\\nrequirements.\\n5. Experiments\\nIn this section, we conduct extensive evaluations to examine\\nthe capabilities of LLaV A-3D. To begin with, we introduce\\nthe implementation details (Sec. 5.1). Then, we compare\\nour model’s 3D scene understanding (Sec. 5.2, 5.3, 5.4)and\\n2D image understanding (Sec. 5.5) capability with previous\\nmethods. Finally, we conduct a thorough analysis to vali-\\ndate the effectiveness of the components and designs of our'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='date the effectiveness of the components and designs of our\\nLLaV A-3D (Sec. 5.6, 5.7).\\n5.1. Implementation Details\\nLLaV A-3D is built upon the LLaV A-1.5-7B, utilizing their\\npre-trained weights from the HuggingFace library. For 3D\\ntasks, we add the 3D position embeddings to the 2D patch\\nvisual tokens, and utilize the voxelization pooling strategy\\nto reduce 3D patch number before passing the input visual\\ntokens to the projection layer and LLM. The number of\\nviewsVis set to be'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='0.2m. Due\\nto the LLM context length limitation, the maximum number\\nof 3D patch tokens after 3D-aware pooling is set to 3096.\\nFor 2D tasks, LLaV A-3D functions the same as LLaV A. All\\nexperiments are conducted on 8 ×80G A1'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='Table 2. Quantitative comparison on MMScan QA benchmark . “S.-BERT\", “B-1”, “B-4”, “R.-L.”, “MET.” represents “Sentence-BERT\",'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='“BLEU-1”, “BLEU-4”, “ROUGE-L”, “METEOR”, respectively. Here, we report the top-1 exact match with (the refined exact-match protocol\\nresults) for “EM@1”.\\nMethods Setting OverallSingle-target Inter-targetAdvancedData-driven Metrics Traditional Metrics\\nST-attr ST-space OO-attr OO-space OR SimCSE S.-BERT B-1. B-4. R.-L MET. EM@1\\n3D-LLM [18]\\nZero-Shot28.6 37.8 18.8 13.7 26.3 15.4 20.8 40.4 40.3 13.4 1.5 17.3 6.0 6.2 (19.6)\\nChat3D-v2 [19] 27.9 38.1 18.3 9.3 22.4 13.5 25.4 45.4 46.3 18.0 3.0 22.9 7.5 10.2 (19.6)\\nLL3DA [12] 15.8 15.5 14.7 14.2 25.2 4.3 6.4 40.7 43.6 5.4 2.1 16.4 4.4 8.3 (19.4)\\nLEO [20] 22.2 28.9 17.6 18.1 20.4 15.0 16.3 40.4 41.0 11.0 0.7 17.1 4.9 9.6 (18.7)\\nLL3DA [12]Fine-tuning38.5 40.4 46.2 14.7 47.1 26.4 7.1 65.3 67.0 26.4 8.5 44.3 14.7 30.2 (37.6)\\nLEO [20] 47.8 55.5 49.5 36.1 45.6 32.1 38.4 71.2 72.2 32.0 12.5 52.1 17.7 36.6 (44.5)\\nLLaV A-3D Generalist 52.3 61.2 54.4 28.7 61.2 44.5 43.6 74.6 76.3 38.7 13.1 55.5 19.5 45.2 (51.4)\\nTable 3. Quantitative comparison with SOTA models on\\nOpenEQA benchmark\\nModels Frame Accuracy\\nLLaMA2 [45] 0 28.3\\nGPT-4 [1] 0 33.5\\nClaude3 20 36.3\\nGemini-Pro [44] 15 44.9\\nGPT-4V [1] 15 54.6\\nGPT-4V [1] 50 55.3\\nHuman Full 86.8\\nLLaV A-3D 20 51.2\\n5.2. Evaluation on 3D Question Answering\\n3D Question Answering requires a model to generate re-\\nsponses to the natural language queries questioning to-\\nwards a 3D scene. In this section, we validate LLaV A-\\n3D performance on various 3D question answering bench-\\nmarks: ScanQA [ 3], SQA3D [ 40], MMScan QA [ 39], and\\nOpenEQA [41].\\nSpatial Understanding with ScanQA and SQA3D.\\nScanQA and SQA3D are both built on ScanNet dataset.\\nThe ScanQA dataset consists of 41363 questions about 800\\nscenes, including 32337 unique questions. Its validation set\\ncontains 4675 questions about 71 scenes. SQA3D comprises\\n20.4k descriptions of 6.8k unique situations collected from\\n650 ScanNet scenes and 33.4k questions about these situa-\\ntions. Questions in ScanQA require basic recognition and\\n3D reasoning capabilities, and SQA3D further incorporates\\nthe situation understanding and situated reasoning into em-\\nbodied 3D scene understanding. Following prior works, we\\nadopt BLEU scores, METEOR, ROUHE-L, CIDEr and EM\\n(“exact match”) as our evaluation metrics for ScanQA and\\nEM for SQA3D respectively. As shown in Tab. 1, current\\n2D LMMs fail to achieve competitive performance with the\\nlatest 3D LMMs trained with 3D awareness, which might be\\nattributed to the lack of explicit 3D representation. Besides,\\ncompared with task-specific fine-tuned 3D LMMs that need\\nto be further fine-tuned on the corresponding datasets, ourLLaV A-3D could perform as a generalist and achieve the\\nSOTA performance on these benchmarks.\\nCoordinate Spatial Understanding with MMScan QA.\\nMMScan QA includes 5.2k scans from ScanNet, 3RScan,\\nand Matterport3D, along with 116k training questions and\\n29k validation questions. These questions span existential in-\\nquiries, attribute understanding, and more advanced queries.\\nUnlike ScanQA and SQA3D, some MMScan QA questions\\nrequire 3D reasoning based on object coordinates, rather than\\nrelying solely on text descriptions, demanding the model ca-\\npable of understanding 3D coordinates information. All\\ndata samples are classified into one of the following sub-\\ncategories: ST-attr, ST-space, OO-attr, OO-space, OR, where\\nST stands for Single-target, attr for attribute, OO for Object-\\nObject, and OR for Object Region. Besides, there is a minor\\npart of QA samples for advanced understanding and reason-\\ning, such as situated QA related to everyday life. We present\\nthe results under GPT-4 evaluation, data-driven metrics, and\\ntraditional metrics respectively in Tab. 2. In this benchmark,\\nthe well-trained LL3DA and LEO are further fine-tuned on\\nthe full 1.2M MMScan QA training dataset. Our LLaV A-3D,\\ntrained on LLaV A-3D-Instruct-1M (which includes 440K\\nMMScan QA training samples), achieves significantly better\\nperformance on the MMScan QA benchmark compared to\\nthe specially fine-tuned LL3DA and LEO. The results high-\\nlight the training efficiency of LLaV A-3D and its strong 3D\\nunderstanding ability to serve as the generalist model.\\nEmbodied Question Answering with OpenEQA.\\nOpenEQA is the first open-vocabulary benchmark designed\\nfor spatial understanding and embodied reasoning in\\nembodied question answering, specifically in the era of\\nfoundation models. It features an automated evaluation\\nprotocol powered by LLMs, which shows strong alignment\\nwith human judgment. Our evaluations are conducted\\nusing the EM-EQA data split of OpenEQA, which includes\\nover 1,600 high-quality, human-generated questions from\\ndiverse real-world environments. In EM-EQA, the model\\nis required to generate a textual answer to a question\\nbased on an episode history, which typically consists'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='Table 2. Quantitative comparison on MMScan QA benchmark . “S.-BERT\", “B-1”, “B-4”, “R.-L.”, “MET.” represents “Sentence-BERT\",\\n“BLEU-1”, “BLEU-4”, “ROUGE-L”, “METEOR”, respectively. Here, we report the top-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='(the refined exact-match protocol\\nresults) for “EM@1”.\\nMethods Setting OverallSingle-target Inter-targetAdvancedData-driven Metrics Traditional Metrics\\nST-attr ST-space OO-attr OO-space OR SimCSE S.-BERT B-1. B-4. R.-L MET. EM@1\\n3D-LLM [18]\\nZero-Shot28.6 37.8 18.8 13.7 26.3 15.4 20.8 40.4 40.3 13.4 1.5 17.3 6.0 6.2 (19.6)\\nChat3D-v2 [19] 27.9 38.1 18.3 9.3 22.4 13.5 25.4 45.4 46.3 18.0 3.0 22.9 7.5 10.2 (19.6)\\nLL3DA [12] 15.8 15.5 14.7 14.2 25.2 4.3 6.4 40.7 43.6 5.4 2.1 16.4 4.4 8.3 (19.4)\\nLEO [20] 22.2 28.9 17.6 18.1 20.4 15.0 16.3 40.4 41.0 11.0 0.7 17.1 4.9 9.6 (18.7)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='LL3DA [12]Fine-tuning38.5 40.4 46.2 14.7 47.1 26.4 7.1 65.3 67.0 26.4 8.5 44.3 14.7 30.2 (37.6)\\nLEO [20] 47.8 55.5 49.5 36.1 45.6 32.1 38.4 71.2 72.2 32.0 12.5 52.1 17.7 36.6 (44.5)\\nLLaV A-3D Generalist 52.3 61.2 54.4 28.7 61.2 44.5 43.6 74.6 76.3 38.7 13.1 55.5 19.5 45.2 (51.4)\\nTable 3. Quantitative comparison with SOTA models on\\nOpenEQA benchmark\\nModels Frame Accuracy\\nLLaMA2 [45] 0 28.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='-3D 20 51.2\\n5.2. Evaluation on 3D Question Answering\\n3D Question Answering requires a model to generate re-\\nsponses to the natural language queries questioning to-\\nwards a 3D scene. In this section, we validate LLaV A-\\n3D performance on various 3D question answering bench-\\nmarks: ScanQA [ 3], SQA3D [ 40], MMScan QA [ 39], and\\nOpenEQA [41].\\nSpatial Understanding with ScanQA and SQA3D.\\nScanQA and SQA3D are both built on ScanNet dataset.\\nThe ScanQA dataset consists of 413'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='33.4k questions about these situa-\\ntions. Questions in ScanQA require basic recognition and\\n3D reasoning capabilities, and SQA3D further incorporates\\nthe situation understanding and situated reasoning into em-\\nbodied 3D scene understanding. Following prior works, we\\nadopt BLEU scores, METEOR, ROUHE-L, CIDEr and EM\\n(“exact match”) as our evaluation metrics for ScanQA and\\nEM for SQA3D respectively. As shown in Tab. 1, current\\n2D LMMs fail to achieve competitive performance with the\\nlatest 3D LMMs trained with 3D awareness, which might be'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='latest 3D LMMs trained with 3D awareness, which might be\\nattributed to the lack of explicit 3D representation. Besides,\\ncompared with task-specific fine-tuned 3D LMMs that need\\nto be further fine-tuned on the corresponding datasets, ourLLaV A-3D could perform as a generalist and achieve the\\nSOTA performance on these benchmarks.\\nCoordinate Spatial Understanding with MMScan QA.\\nMMScan QA includes 5.2k scans from ScanNet, 3RScan,\\nand Matterport3D, along with 116k training questions and\\n29k validation questions. These questions span existential in-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='quiries, attribute understanding, and more advanced queries.\\nUnlike ScanQA and SQA3D, some MMScan QA questions\\nrequire 3D reasoning based on object coordinates, rather than\\nrelying solely on text descriptions, demanding the model ca-\\npable of understanding 3D coordinates information. All\\ndata samples are classified into one of the following sub-\\ncategories: ST-attr, ST-space, OO-attr, OO-space, OR, where\\nST stands for Single-target, attr for attribute, OO for Object-\\nObject, and OR for Object Region. Besides, there is a minor\\npart of QA samples for advanced understanding and reason-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='part of QA samples for advanced understanding and reason-\\ning, such as situated QA related to everyday life. We present\\nthe results under GPT-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content=', data-driven metrics, and\\ntraditional metrics respectively in Tab. 2. In this benchmark,\\nthe well-trained LL3DA and LEO are further fine-tuned on\\nthe full 1.2M MMScan QA training dataset. Our LLaV A-3D,\\ntrained on LLaV A-3D-Instruct-1M (which includes 440K\\nMMScan QA training samples), achieves significantly better\\nperformance on the MMScan QA benchmark compared to\\nthe specially fine-tuned LL3DA and LEO. The results high-\\nlight the training efficiency of LLaV A-3D and its strong 3D\\nunderstanding ability to serve as the generalist model.\\nEmbodied Question Answering with OpenEQA.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='Embodied Question Answering with OpenEQA.\\nOpenEQA is the first open-vocabulary benchmark designed\\nfor spatial understanding and embodied reasoning in\\nembodied question answering, specifically in the era of\\nfoundation models. It features an automated evaluation\\nprotocol powered by LLMs, which shows strong alignment\\nwith human judgment. Our evaluations are conducted\\nusing the EM-EQA data split of OpenEQA, which includes\\nover 1,6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='-quality, human-generated questions from\\ndiverse real-world environments. In EM-EQA, the model\\nis required to generate a textual answer to a question\\nbased on an episode history, which typically consists'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='Table 4. Quantitative Comparisons with SOTA models for 3D'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='Dense Captioning on Scan2Cap. “C” stands for “CIDEr”, “B-4”\\nfor “BLEU4”, “M” for “METEOR”, “R” for “ROUGE”, “Sim”\\nfor sentence similarity, and “EM@1” for top-1 exact match. The\\nn-gram metrics for Scan2Cap are governed by IoU@0.5.\\nScan2Cap (Val)\\nC@0.5↑B-4@0.5 ↑M@0.5 ↑R@0.5↑\\nScan2Cap [13] 39.08 23.32 21.97 44.78\\nMORE [24] 40.94 22.93 21.66 44.42\\nSpaCap3D [46] 44.02 25.26 22.33 45.36\\nD3Net [7] 46.07 30.29 24.35 51.67\\nUniT3D [14] 46.69 27.22 21.91 45.98\\n3DJCG [5] 49.48 31.03 24.22 50.80\\n3D-VLP [25] 54.94 32.31 24.83 51.51\\n3D-VisTA [52] 61.60 34.10 26.80 55.00\\nV ote2Cap-DETR [11] 61.81 34.46 26.22 54.40\\nLL3DA [12] 65.19 36.79 25.97 55.06\\nLEO [20] 72.4 38.2 27.9 58.1\\nLLaV A-3D 79.21 41.12 30.21 63.41\\nTable 5. Quantitative comparison with SOTA models on the\\nMMScan Captioning benchmark.\\nmodel Evaluator Type Color Shape Position Function Design Overall\\nLL3DA [12] GPT 10.0 26.3 40.6 38.9 67.5 21.7 33.6\\nLEO [20] GPT 34.9 29.7 63.0 63.7 75.0 42.7 51.3\\nLLaV A-3D GPT 39.9 79.2 89.1 82.2 94.1 88.0 78.8\\nof RGB images, depth information, camera poses, and\\nintrinsic camera data. The results in Tab. 3 demonstrate\\nthat LLaV A-3D surpasses Claude3 and Gemini-Pro, and\\nachieves comparable performance with powerful GPT-4V on\\nthis benchmark with significantly fewer model parameters.\\n5.3. Evaluation on 3D Dense Captioning\\n3D dense captioning requires the model to localize all the\\nobjects in a 3D scene and then generate descriptive sentences\\nfor each object. To evaluate our model on the 3D dense cap-\\ntioning tasks, we utilize the off-the-shelf segmentation model\\nMask3D to generate object proposals. Then we further con-\\nstruct the 3D coordinate tokens based on the 3D object center\\ncoordinates to guide the model to perform the task. Addition-\\nally, we utilize two types of textual instructions that prompt\\nthe model to either describe the object or describe and output\\nthe bounding box of the object. We report the performance\\nof various methods on two 3D dense captioning benchmarks:\\nScan2Cap and MMScan Captioning.\\nScan2Cap. Scan2Cap requires the model to describe the\\nobject’s appearance and the spatial relations with nearby\\nobjects and output the corresponding 3D bounding box. As\\nillustrated in Tab. 4, our method consistently outperforms\\nthe existing method on the Scan2Cap benchmark.\\nMMScan Captioning. MMScan Captioning focuses on\\nidentifying common aspects of 3D objects such as Object\\nType, Color, Shape, Position, Function, and Design. TheTable 6. Quantitative comparison with SOTA models on various\\n3D VG tasks .\\nScanRefer\\nAcc@0.25 Acc@0.5\\nTask-specific models\\nScanRefer [6] 37.3 24.3\\nMVT [21] 40.8 33.3\\n3DVG-Trans [50] 45.9 34.5\\nViL3DRel [10] 47.9 37.7\\nBUTD-DETR [22] 52.2 39.8\\nReGround3D [51] 53.1 41.1\\nTask-specific fine-tuned 3D LLMs\\nLLM-Grounder [48] 17.1 5.3\\n3D-LLM [18] 30.3 -\\nChat3D-v2 [19] 35.9 30.4\\nLLaV A-3D 54.1 42.2\\nbenchmark utilizes GPT4 to assess whether these aspects\\nare correct in the object description. We benchmark var-\\nious methods on the MMScan Captioning benchmark in\\nTab. 5. The results show that our method surpasses existing\\napproaches across all metrics by a substantial margin, espe-\\ncially achieving a 49.5% improvement in the Color score and\\na 43.3% improvement in the Design score. This significant\\nperformance boost can be attributed to our 2D LMM-based\\narchitecture.\\nUniquely, LLaV A-3D takes multi-view images as inputs,\\nenabling a user-friendly feature where users can simply click\\non selected images to generate both 3D object captions and\\n3D bounding boxes. We refer to this capability as 2D Click-\\nbased 3D Dense Captioning . (as illustrated in Fig. 4).\\n5.4. Evaluation on 3D Visual Grounding\\n3D visual grounding aims to localize the target object in the\\n3D scene using natural language descriptions. In this section,\\nwe initially report the performance on the ScanRefer [ 6]\\nbenchmark. For quantitative comparisons, we include both\\ntask-specific approaches and generalist models: the state-\\nof-the-art specialists in 3D VG and generalist models like\\nLLM-Grounder [ 48], 3D-LLM [ 18], and Chat3D-v2 [ 19].\\n3D-LLM [ 18]] uses the location tokens to discretize the 3D\\nscene and predicts the bounding box as the location tokens\\nthat were added to the vocabulary. Chat-3D v2 [ 19] first\\ndetects all the objects and then identifies the object that\\nbest matches the provided description. The results in Tab. 6\\ndemonstrate that by combining with the grounding module,\\nour LLaV A-3D could achieve even better performance com-\\npared with the task-specific specialists model as a generalist.\\nSuch a paradigm allows LLaV A-3D to integrate with various\\npowerful 3D grounding modules and inject the 3D under-\\nstanding capability and world knowledge into the grounding\\nprocess, as illustrated in Fig. 7.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='Table 4. Quantitative Comparisons with SOTA models for 3D\\nDense Captioning on Scan2Cap. “C” stands for “CIDEr”, “B-4”\\nfor “BLEU4”, “M” for “METEOR”, “R” for “ROUGE”, “Sim”\\nfor sentence similarity, and “EM@1” for top-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='. The\\nn-gram metrics for Scan2Cap are governed by IoU@0.5.\\nScan2Cap (Val)\\nC@0.5↑B-4@0.5 ↑M@0.5 ↑R@0.5↑\\nScan2Cap [13] 39.08 23.32 21.97 44.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='3D [14] 46.69 27.22 21.91 45.98\\n3DJCG [5] 49.48 31.03 24.22 50.80\\n3D-VLP [25] 54.94 32.31 24.83 51.51\\n3D-VisTA [52] 61.60 34.10 26.80 55.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='5. Quantitative comparison with SOTA models on the\\nMMScan Captioning benchmark.\\nmodel Evaluator Type Color Shape Position Function Design Overall\\nLL3DA [12] GPT 10.0 26.3 40.6 38.9 67.5 21.7 33.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='-Pro, and\\nachieves comparable performance with powerful GPT-4V on\\nthis benchmark with significantly fewer model parameters.\\n5.3. Evaluation on 3D Dense Captioning\\n3D dense captioning requires the model to localize all the\\nobjects in a 3D scene and then generate descriptive sentences\\nfor each object. To evaluate our model on the 3D dense cap-\\ntioning tasks, we utilize the off-the-shelf segmentation model\\nMask3D to generate object proposals. Then we further con-\\nstruct the 3D coordinate tokens based on the 3D object center\\ncoordinates to guide the model to perform the task. Addition-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='ally, we utilize two types of textual instructions that prompt\\nthe model to either describe the object or describe and output\\nthe bounding box of the object. We report the performance\\nof various methods on two 3D dense captioning benchmarks:\\nScan2Cap and MMScan Captioning.\\nScan2Cap. Scan2Cap requires the model to describe the\\nobject’s appearance and the spatial relations with nearby\\nobjects and output the corresponding 3D bounding box. As\\nillustrated in Tab. 4, our method consistently outperforms\\nthe existing method on the Scan2Cap benchmark.\\nMMScan Captioning. MMScan Captioning focuses on'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='MMScan Captioning. MMScan Captioning focuses on\\nidentifying common aspects of 3D objects such as Object\\nType, Color, Shape, Position, Function, and Design. TheTable 6. Quantitative comparison with SOTA models on various\\n3D VG tasks .\\nScanRefer\\nAcc@0.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='are correct in the object description. We benchmark var-\\nious methods on the MMScan Captioning benchmark in\\nTab. 5. The results show that our method surpasses existing\\napproaches across all metrics by a substantial margin, espe-\\ncially achieving a 49.5% improvement in the Color score and\\na 43.3% improvement in the Design score. This significant\\nperformance boost can be attributed to our 2D LMM-based\\narchitecture.\\nUniquely, LLaV A-3D takes multi-view images as inputs,\\nenabling a user-friendly feature where users can simply click\\non selected images to generate both 3D object captions and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='on selected images to generate both 3D object captions and\\n3D bounding boxes. We refer to this capability as 2D Click-\\nbased 3D Dense Captioning . (as illustrated in Fig. 4).\\n5.4. Evaluation on 3D Visual Grounding\\n3D visual grounding aims to localize the target object in the\\n3D scene using natural language descriptions. In this section,\\nwe initially report the performance on the ScanRefer [ 6]\\nbenchmark. For quantitative comparisons, we include both\\ntask-specific approaches and generalist models: the state-\\nof-the-art specialists in 3D VG and generalist models like'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='of-the-art specialists in 3D VG and generalist models like\\nLLM-Grounder [ 48], 3D-LLM [ 18], and Chat3D-v2 [ 19].\\n3D-LLM [ 18]] uses the location tokens to discretize the 3D\\nscene and predicts the bounding box as the location tokens\\nthat were added to the vocabulary. Chat-3D v2 [ 19] first\\ndetects all the objects and then identifies the object that\\nbest matches the provided description. The results in Tab.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content=',\\nour LLaV A-3D could achieve even better performance com-\\npared with the task-specific specialists model as a generalist.\\nSuch a paradigm allows LLaV A-3D to integrate with various\\npowerful 3D grounding modules and inject the 3D under-\\nstanding capability and world knowledge into the grounding\\nprocess, as illustrated in Fig. 7.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='Figure 4. LLaV A-3D enables the user-friendly interaction with the 3D scene across various 3D understanding and reasoning tasks. It allows'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='the users to just click on the 2D images or the video frame to simply conduct the interactive 3D question answering and 3D dense captioning.\\nTable 7. Quantitative Comparisons with SOTA models on zero-\\nshot 2D benchmarks .\\nMethod LLM Res. VQATMMB MME MM-Vet\\nMobileVLM [15] MLLaMA 2.7B 336 47.5 59.6 1289 -\\nInstructBLIP [16] Vicuna-7B 224 50.1 36.0 – 26.2\\nInstructBLIP [16] Vicuna-13B 224 50.7 – 1213 25.6\\nQwen-VL [4] Qwen-7B 448 63.8∗38.2 – –\\nQwen-VL-Chat [4] Qwen-7B 448 61.5∗60.6 1488 –\\nShikra [9] Vicuna-13B 224 – 58.8 – –\\nLLaMA-VID [33] Vicuna-7B 336 – 65.1 1521 –\\nLLaMA-VID [33] Vicuna-13B 336 – 66.6 1542 –\\nLLaV A-1.5 [38] Vicuna-7B 336 58.2 65.2 1511 31.1\\nLLaV A-1.5 [38] Vicuna-13B 336 61.3 69.2 1531/295 36.1\\nLLaV A-3D Vicuna-7B 336 57.8 65.0 1502 30.9\\nTable 8. Architecture Comparison on various 3D V&L Bench-\\nmark.\\n3D Feature Connector LLM / LMM ScanQA SQA3D Inference time\\n(a) (SAM + CLIP) w / PE Q-Former Vicuna-7B 21.9 49.3 900s\\n(b) (SAM + CLIP) w / PE Pooling + MLP Vicuna-7B 22.1 49.2 900s\\n(c) CLIP w / PE Pooling + MLP Vicuna-7B 23.4 51.2 0.2s\\n(d) CLIP w / PE Pooling + MLP LLaV A-1.5 27.0 55.6 0.2s\\n(e) CLIP w / PE Pooling + MLP PLLaV A 27.9 56.2 0.2s\\n5.5. Evaluation on 2D benchmarks\\nSince our model is trained on a mix of 2D and 3D datasets,\\nwe evaluate it across various 2D benchmarks to ensure it\\nretains the 2D image understanding capabilities of the origi-\\nnal LLaV A. As demonstrated in Tab. 7, LLaV A-3D achieves\\nperformance comparable to that of LLaV A-1.5 across sev-\\neral 2D image understanding benchmarks. This performance\\nunderscores the architectural superiority of our model com-\\npared to other 3D LMMs.\\n5.6. Architecture Analysis\\nIn this section, we delve deeper into the architectural bene-\\nfits and efficacy of adapting a 2D large multimodal modelTable 9. Effectiveness of 3D Patch Representation . Training\\nusing the instruction tuning datasets, the only difference between\\nmulti-image LLaV A and LLaV A-3D is the patch type.\\nMethod Patch Type ScanQA SQA3D MMScan QA Scan2Cap\\nmulti-image LLaV A 2D 24.1 52.3 32.7 29.1\\nLLaV A-3D 3D 27.0 (+2.9) 55.6 (+3.3) 41.5 (+8.8) 79.2 (+50.1)\\n(LMM) to a 3D LMM, as opposed to developing a 3D LMM\\nsolely from LLMs. A notable approach in the latter category\\nis 3D-LLM, which leverages foundational 2D visual models\\nsuch as SAM and CLIP for feature extraction. This method\\nthen employs 3D multi-view fusion to generate 3D point\\nfeatures, followed by the use of a Q-Former to condense\\nthese point features into a fixed number of tokens.\\nArchitecture Comparison. To ensure the fairness of the\\nexperiment as much as possible, starting from LLM, we\\nfirst ablate different 3D feature encoders, and 3D-language\\nconnectors using the same instruction tuning datasets. As\\nshown in Tab. 8, the Q-Former (a) and Pooling + MLP\\n(b) share a similar performance on 3D V&L benchmarks.\\nNotably, using CLIP (c) alone instead of SAM + CLIP (b)\\nachieves better performance and significantly reduces 3D\\nfeature extraction computation time from 900s to 0.2s.\\nEffectiveness of 3D Patch. Once we don’t decorate the 2D\\npatches with corresponding 3D position embedding, LLaV A-\\n3D will degenerate into 2D multi-image LMMs. To further\\nascertain the efficacy of our proposed 3D representation: 3D\\nPatch, we conduct additional experiments across a variety\\nof 3D question answering and 3D dense captioning bench-\\nmarks. As shown in Tab. 9, integrating 2D patches within\\na 3D context enhances the model’s 3D spatial understand-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='Figure 4. LLaV A-3D enables the user-friendly interaction with the 3D scene across various 3D understanding and reasoning tasks. It allows\\nthe users to just click on the 2D images or the video frame to simply conduct the interactive 3D question answering and 3D dense captioning.\\nTable 7. Quantitative Comparisons with SOTA models on zero-\\nshot 2D benchmarks .\\nMethod LLM Res. VQATMMB MME MM-Vet\\nMobileVLM [15] MLLaMA 2.7B 336 47.5 59.6 1289 -\\nInstructBLIP [16] Vicuna-7B 224 50.1 36.0 – 26.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='-VL [4] Qwen-7B 448 63.8∗38.2 – –\\nQwen-VL-Chat [4] Qwen-7B 448 61.5∗60.6 1488 –\\nShikra [9] Vicuna-13B 224 – 58.8 – –\\nLLaMA-VID [33] Vicuna-7B 336 – 65.1 1521 –\\nLLaMA-VID [33] Vicuna-13B 336 – 66.6 1542 –\\nLLaV A-1.5 [38] Vicuna-7B 336 58.2 65.2 1511 31.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='8. Architecture Comparison on various 3D V&L Bench-\\nmark.\\n3D Feature Connector LLM / LMM ScanQA SQA3D Inference time\\n(a) (SAM + CLIP) w / PE Q-Former Vicuna-7B 21.9 49.3 900s\\n(b) (SAM + CLIP) w / PE Pooling + MLP Vicuna-7B 22.1 49.2 900s\\n(c) CLIP w / PE Pooling + MLP Vicuna-7B 23.4 51.2 0.2s\\n(d) CLIP w / PE Pooling + MLP LLaV A-1.5 27.0 55.6 0.2s\\n(e) CLIP w / PE Pooling + MLP PLLaV A 27.9 56.2 0.2s\\n5.5. Evaluation on 2D benchmarks\\nSince our model is trained on a mix of 2D and 3D datasets,\\nwe evaluate it across various 2D benchmarks to ensure it'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='we evaluate it across various 2D benchmarks to ensure it\\nretains the 2D image understanding capabilities of the origi-\\nnal LLaV A. As demonstrated in Tab. 7, LLaV A-3D achieves\\nperformance comparable to that of LLaV A-1.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='-\\neral 2D image understanding benchmarks. This performance\\nunderscores the architectural superiority of our model com-\\npared to other 3D LMMs.\\n5.6. Architecture Analysis\\nIn this section, we delve deeper into the architectural bene-\\nfits and efficacy of adapting a 2D large multimodal modelTable 9. Effectiveness of 3D Patch Representation . Training\\nusing the instruction tuning datasets, the only difference between\\nmulti-image LLaV A and LLaV A-3D is the patch type.\\nMethod Patch Type ScanQA SQA3D MMScan QA Scan2Cap\\nmulti-image LLaV A 2D 24.1 52.3 32.7 29.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='-3D 3D 27.0 (+2.9) 55.6 (+3.3) 41.5 (+8.8) 79.2 (+50.1)\\n(LMM) to a 3D LMM, as opposed to developing a 3D LMM\\nsolely from LLMs. A notable approach in the latter category\\nis 3D-LLM, which leverages foundational 2D visual models\\nsuch as SAM and CLIP for feature extraction. This method\\nthen employs 3D multi-view fusion to generate 3D point\\nfeatures, followed by the use of a Q-Former to condense\\nthese point features into a fixed number of tokens.\\nArchitecture Comparison. To ensure the fairness of the\\nexperiment as much as possible, starting from LLM, we'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='experiment as much as possible, starting from LLM, we\\nfirst ablate different 3D feature encoders, and 3D-language\\nconnectors using the same instruction tuning datasets. As\\nshown in Tab. 8, the Q-Former (a) and Pooling + MLP\\n(b) share a similar performance on 3D V&L benchmarks.\\nNotably, using CLIP (c) alone instead of SAM + CLIP (b)\\nachieves better performance and significantly reduces 3D\\nfeature extraction computation time from 900s to 0.2s.\\nEffectiveness of 3D Patch. Once we don’t decorate the 2D\\npatches with corresponding 3D position embedding, LLaV A-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='patches with corresponding 3D position embedding, LLaV A-\\n3D will degenerate into 2D multi-image LMMs. To further\\nascertain the efficacy of our proposed 3D representation: 3D\\nPatch, we conduct additional experiments across a variety\\nof 3D question answering and 3D dense captioning bench-\\nmarks. As shown in Tab. 9, integrating 2D patches within\\na 3D context enhances the model’s 3D spatial understand-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='Table 10. Comparsion on different pooling strategies .'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='Pooling Stratgey V oxel Size Token Number ScanQA SQA3D\\nV oxelization 0.4 Dynamic 24.1 53.2\\nV oxelization 0.3 Dynamic 25.9 54.8\\nV oxelization 0.2 Dynamic 27.0 55.6\\nFPS - 576 25.7 54.9\\nFPS - 1024 26.3 55.2\\ning and reasoning capabilities, resulting in 2.9% and 3.3%\\nimprovements on the ScanQA and SQA3D benchmarks, re-\\nspectively. Additionally, 3D patches are crucial for tasks that\\nrequire explicit 3D world modeling, leading to significant\\nimprovements of 8.8% on the MMScan QA benchmark and\\nan impressive 50.1% on the Scan2Cap benchmark.\\nBenefits from pre-trained 2D LMM. Leveraging our\\nfoundation in 2D LMMs, our framework benefits signifi-\\ncantly from the robust pre-training on extensive image/video-\\ntext datasets. In Tab. 8, we explore the advantages of ini-\\ntializing from a pre-trained 2D LMM compared to start-\\ning directly from an LLM. The experimental results con-\\nsistently demonstrate that starting with a well-tuned 2D\\nLMM substantially could enhance performance in 3D spa-\\ntial understanding tasks. Besides, we observe that stronger\\nbase 2D LMM could lead to better 3D spatial understand-\\ning performance. Besides, we find that initializing from\\na pre-trained 2D LMM could achieve 3.5 ×faster training\\nconvergence speed of LLaV A-3D compared with existing\\n3D LMMs [ 12,20]. More details could be found in our\\nappendix.\\n5.7. More Analysis\\nTo better understand the impact of different components of\\nour LLaV A-3D, we conduct a thorough ablation study on\\nthe ScanQA and SQA3D benchmarks.\\nImpact of Pooling Strategy. Given the maximum token\\nlength limitation of LLMs, we apply pooling to the 3D patch\\ntokens extracted from multi-view images to reduce the num-\\nber of tokens. However, this pooling process inevitably leads\\nto some information loss. To understand its impact on per-\\nformance, we conducted experiments to evaluate the effects\\nof pooling. As shown in Tab. 10, the voxelization pooling\\nstrategy outperforms the FPS pooling method on 3D QA\\nbenchmarks. Reducing the voxel size in the voxelization\\npooling or increasing the number of 3D patch tokens ob-\\ntained through FPS pooling can both enhance the model’s\\nperformance to some extent.\\nMulti-View Images Sampling Strategy. Due to the re-\\ndundancy of information among multi-view images and con-\\nsidering computational costs, we sample Vviews from theTable 11. Comparison on performance on 3D QA tasks under\\ndifferent number of multi-view images .\\nNumber of Views Number of Tokens ScanQA SQA3D\\n16 9216 26.2 55.1\\n20 11520 27.0 55.6\\n24 13824 27.0 55.4\\n40 23040 26.7 55.2\\negocentric images of the 3D scene in our experiment. In this\\nsection, we explore the effect of different image sampling\\nstrategies during the inference stage: 1) Uniform Sampling :\\nTo achieve comprehensive coverage of the entire 3D scene,\\na straightforward approach is uniform sampling, which sam-\\nples images evenly. 2) Text-Guided Sampling : such sampling\\nstrategy uses CLIP to select the frames related to the text\\ninstruction during inference based on the image-text CLIP\\nsimilarity score. Our experiments demonstrate that these two\\nsampling strategies share similar performance on ScanQA\\nand SQA3D, so we choose uniform sampling for simplicity.\\nNumber of Views. An intuitive assumption is that sam-\\npling more views from the 3D scene will preserve more\\ninformation about the 3D scene. We conduct a comparative\\nexperiment varying the number of views sampled from 3D\\nscenes. Tab. 11 presents the Exact Match (EM) scores on\\nScanQA and SQA3D across different settings, revealing that\\nthe increase in EM score is marginal as the number of views\\nincreases. Additionally, the experimental results indicate\\nthat exceeding a certain number of views can degrade the\\nmodel’s performance.\\n6. Conclusion\\nWe propose LLaV A-3D, a simple yet effective framework\\nbuilt upon the well-established 2D LLaV A model. LLaV A-\\n3D extends the LLaV A’s capabilities to understand the 3D\\nworld by using 3D patches to bridge 2D features within a\\n3D space, enabling spatial understanding while efficiently\\npreserving the original 2D image understanding and reason-\\ning capability. Experimental results show that our method\\nsets new state-of-the-art performance on a wide range of 3D\\ntasks and benchmarks. We hope that our model will inspire\\nnew ideas for building 3D LMMs, and in the future, we plan\\nto explore the application of LLaV A-3D in more downstream\\nscenarios, such as robot manipulation and navigation.\\nReferences\\n[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,\\nIlge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko\\nAltenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\\ntechnical report. arXiv preprint arXiv:2303.08774 , 2023. 2, 6\\n[2]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='Table 10. Comparsion on different pooling strategies .\\nPooling Stratgey V oxel Size Token Number ScanQA SQA3D\\nV oxelization 0.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content=', resulting in 2.9% and 3.3%\\nimprovements on the ScanQA and SQA3D benchmarks, re-\\nspectively. Additionally, 3D patches are crucial for tasks that\\nrequire explicit 3D world modeling, leading to significant\\nimprovements of 8.8% on the MMScan QA benchmark and\\nan impressive 50.1% on the Scan2Cap benchmark.\\nBenefits from pre-trained 2D LMM. Leveraging our\\nfoundation in 2D LMMs, our framework benefits signifi-\\ncantly from the robust pre-training on extensive image/video-\\ntext datasets. In Tab. 8, we explore the advantages of ini-\\ntializing from a pre-trained 2D LMM compared to start-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='tializing from a pre-trained 2D LMM compared to start-\\ning directly from an LLM. The experimental results con-\\nsistently demonstrate that starting with a well-tuned 2D\\nLMM substantially could enhance performance in 3D spa-\\ntial understanding tasks. Besides, we observe that stronger\\nbase 2D LMM could lead to better 3D spatial understand-\\ning performance. Besides, we find that initializing from\\na pre-trained 2D LMM could achieve 3.5 ×faster training\\nconvergence speed of LLaV A-3D compared with existing\\n3D LMMs [ 12,20]. More details could be found in our\\nappendix.\\n5.7. More Analysis'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='appendix.\\n5.7. More Analysis\\nTo better understand the impact of different components of\\nour LLaV A-3D, we conduct a thorough ablation study on\\nthe ScanQA and SQA3D benchmarks.\\nImpact of Pooling Strategy. Given the maximum token\\nlength limitation of LLMs, we apply pooling to the 3D patch\\ntokens extracted from multi-view images to reduce the num-\\nber of tokens. However, this pooling process inevitably leads\\nto some information loss. To understand its impact on per-\\nformance, we conducted experiments to evaluate the effects\\nof pooling. As shown in Tab. 10, the voxelization pooling'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='of pooling. As shown in Tab. 10, the voxelization pooling\\nstrategy outperforms the FPS pooling method on 3D QA\\nbenchmarks. Reducing the voxel size in the voxelization\\npooling or increasing the number of 3D patch tokens ob-\\ntained through FPS pooling can both enhance the model’s\\nperformance to some extent.\\nMulti-View Images Sampling Strategy. Due to the re-\\ndundancy of information among multi-view images and con-\\nsidering computational costs, we sample Vviews from theTable 11. Comparison on performance on 3D QA tasks under\\ndifferent number of multi-view images .'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='different number of multi-view images .\\nNumber of Views Number of Tokens ScanQA SQA3D\\n16 9216 26.2 55.1\\n20 11520 27.0 55.6\\n24 13824 27.0 55.4\\n40 23040 26.7 55.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='3D scene in our experiment. In this\\nsection, we explore the effect of different image sampling\\nstrategies during the inference stage: 1) Uniform Sampling :\\nTo achieve comprehensive coverage of the entire 3D scene,\\na straightforward approach is uniform sampling, which sam-\\nples images evenly. 2) Text-Guided Sampling : such sampling\\nstrategy uses CLIP to select the frames related to the text\\ninstruction during inference based on the image-text CLIP\\nsimilarity score. Our experiments demonstrate that these two\\nsampling strategies share similar performance on ScanQA'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='sampling strategies share similar performance on ScanQA\\nand SQA3D, so we choose uniform sampling for simplicity.\\nNumber of Views. An intuitive assumption is that sam-\\npling more views from the 3D scene will preserve more\\ninformation about the 3D scene. We conduct a comparative\\nexperiment varying the number of views sampled from 3D\\nscenes. Tab.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='(EM) scores on\\nScanQA and SQA3D across different settings, revealing that\\nthe increase in EM score is marginal as the number of views\\nincreases. Additionally, the experimental results indicate\\nthat exceeding a certain number of views can degrade the\\nmodel’s performance.\\n6. Conclusion\\nWe propose LLaV A-3D, a simple yet effective framework\\nbuilt upon the well-established 2D LLaV A model. LLaV A-\\n3D extends the LLaV A’s capabilities to understand the 3D\\nworld by using 3D patches to bridge 2D features within a\\n3D space, enabling spatial understanding while efficiently'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='3D space, enabling spatial understanding while efficiently\\npreserving the original 2D image understanding and reason-\\ning capability. Experimental results show that our method\\nsets new state-of-the-art performance on a wide range of 3D\\ntasks and benchmarks. We hope that our model will inspire\\nnew ideas for building 3D LMMs, and in the future, we plan\\nto explore the application of LLaV A-3D in more downstream\\nscenarios, such as robot manipulation and navigation.\\nReferences\\n[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,\\nIlge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko\\nAltenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='. arXiv preprint arXiv:2303.08774 , 2023. 2, 6\\n[2]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='Katherine Millican, Malcolm Reynolds, et al. Flamingo: a'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='visual language model for few-shot learning. Advances in\\nNeural Information Processing Systems , 35:23716–23736,\\n2022. 2\\n[3]Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki\\nKawanabe. Scanqa: 3d question answering for spatial scene\\nunderstanding. In proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition , pages 19129–\\n19139, 2022. 2, 5, 6\\n[4]Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. Qwen-vl: A frontier large vision-language model with\\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 8\\n[5]Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong\\nXu. 3djcg: A unified framework for joint dense captioning\\nand visual grounding on 3d point clouds. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 16464–16473, 2022. 7\\n[6]Dave Zhenyu Chen, Angel X Chang, and Matthias Nießner.\\nScanrefer: 3d object localization in rgb-d scans using natural\\nlanguage. In European conference on computer vision , pages\\n202–221. Springer, 2020. 2, 7\\n[7]Dave Zhenyu Chen, Qirui Wu, Matthias Nießner, and An-\\ngel X Chang. D 3 net: A unified speaker-listener architecture\\nfor 3d dense captioning and visual grounding. In European\\nConference on Computer Vision , pages 487–505. Springer,\\n2022. 7\\n[8]Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\\nLiu, Pengchuan Zhang, Raghuraman Krishnamoorthi,\\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\\nMinigpt-v2: large language model as a unified interface\\nfor vision-language multi-task learning. arXiv preprint\\narXiv:2310.09478 , 2023. 2\\n[9]Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng\\nZhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s\\nreferential dialogue magic. arXiv preprint arXiv:2306.15195 ,\\n2023. 8\\n[10] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi,\\nCordelia Schmid, and Ivan Laptev. Language conditioned\\nspatial relation reasoning for 3d object grounding. Advances\\nin Neural Information Processing Systems , 35:20522–20535,\\n2022. 7\\n[11] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu,\\nand Tao Chen. End-to-end 3d dense captioning with vote2cap-\\ndetr. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition , pages 11124–11133,\\n2023. 7\\n[12] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu,\\nHao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da:\\nVisual interactive instruction tuning for omni-3d understand-\\ning reasoning and planning. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 26428–26438, 2024. 2, 3, 6, 7, 9\\n[13] Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel X\\nChang. Scan2cap: Context-aware dense captioning in rgb-\\nd scans. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition , pages 3193–3203,\\n2021. 2, 5, 7[14] Zhenyu Chen, Ronghang Hu, Xinlei Chen, Matthias Nießner,\\nand Angel X Chang. Unit3d: A unified transformer for 3d\\ndense captioning and visual grounding. In Proceedings of\\nthe IEEE/CVF international conference on computer vision ,\\npages 18109–18119, 2023. 7\\n[15] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu,\\nYang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang,\\nXiaolin Wei, et al. Mobilevlm: A fast, reproducible and strong\\nvision language assistant for mobile devices. arXiv preprint\\narXiv:2312.16886 , 2023. 8\\n[16] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung,\\nand Steven Hoi. Instructblip: Towards general-purpose vision-\\nlanguage models with instruction tuning, 2023. 8\\n[17] Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wen-\\nhan Xiong. Scene-llm: Extending language model for\\n3d visual understanding and reasoning. arXiv preprint\\narXiv:2403.11401 , 2024. 2, 3, 5\\n[18] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng,\\nYilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting\\nthe 3d world into large language models. arXiv preprint\\narXiv:2307.12981 , 2023. 2, 3, 5, 6, 7\\n[19] Haifeng Huang, Zehan Wang, Rongjie Huang, Luping Liu,\\nXize Cheng, Yang Zhao, Tao Jin, and Zhou Zhao. Chat-3d\\nv2: Bridging 3d scene and large language models with object\\nidentifiers. arXiv preprint arXiv:2312.08168 , 2023. 2, 3, 5, 6,\\n7\\n[20] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun\\nLinghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baox-\\niong Jia, and Siyuan Huang. An embodied generalist agent in\\n3d world. arXiv preprint arXiv:2311.12871 , 2023. 2, 3, 5, 6,\\n7, 9\\n[21] Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multi-\\nview transformer for 3d visual grounding. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 15524–15533, 2022. 7\\n[22] Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Kate-\\nrina Fragkiadaki. Bottom up top down detection transformers\\nfor language grounding in images and point clouds. In Com-\\nputer Vision–ECCV 2022: 17th European Conference, Tel\\nAviv, Israel, October 23–27, 2022, Proceedings, Part XXXVI ,\\npages 417–433. Springer, 2022. 7\\n[23] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao\\nGu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer,\\nSoroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al. Con-\\nceptfusion: Open-set multimodal 3d mapping. arXiv preprint\\narXiv:2302.07241 , 2023. 2\\n[24] Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin\\nMa, and Yu-Gang Jiang. More: Multi-order relation mining\\nfor dense captioning in 3d scenes. In European Conference\\non Computer Vision , pages 528–545. Springer, 2022. 7\\n[25] Zhao Jin, Munawar Hayat, Yuwei Yang, Yulan Guo, and\\nYinjie Lei. Context-aware alignment and mutual masking for\\n3d-language pre-training. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 10984–10994, 2023. 7\\n[26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='Katherine Millican, Malcolm Reynolds, et al. Flamingo: a\\nvisual language model for few-shot learning. Advances in\\nNeural Information Processing Systems , 35:23716–23736,\\n2022. 2\\n[3]Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki\\nKawanabe. Scanqa: 3d question answering for spatial scene\\nunderstanding. In proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition , pages 19129–\\n19139, 2022. 2, 5, 6\\n[4]Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. Qwen-vl: A frontier large vision-language model with\\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 8\\n[5]Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong\\nXu. 3djcg: A unified framework for joint dense captioning\\nand visual grounding on 3d point clouds. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 16464–16473, 2022. 7\\n[6]Dave Zhenyu Chen, Angel X Chang, and Matthias Nießner.\\nScanrefer: 3d object localization in rgb-d scans using natural'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='language. In European conference on computer vision , pages\\n202–221. Springer, 2020. 2, 7\\n[7]Dave Zhenyu Chen, Qirui Wu, Matthias Nießner, and An-\\ngel X Chang. D'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content=': A unified speaker-listener architecture\\nfor 3d dense captioning and visual grounding. In European\\nConference on Computer Vision , pages 487–505. Springer,\\n2022. 7\\n[8]Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\\nLiu, Pengchuan Zhang, Raghuraman Krishnamoorthi,\\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\\nMinigpt-v2: large language model as a unified interface\\nfor vision-language multi-task learning. arXiv preprint\\narXiv:2310.09478 , 2023. 2\\n[9]Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng\\nZhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s\\nreferential dialogue magic. arXiv preprint arXiv:2306.15195 ,\\n2023. 8\\n[10] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi,\\nCordelia Schmid, and Ivan Laptev. Language conditioned\\nspatial relation reasoning for 3d object grounding. Advances\\nin Neural Information Processing Systems , 35:20522–20535,\\n2022. 7\\n[11] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu,\\nand Tao Chen. End-to-end 3d dense captioning with vote2cap-\\ndetr. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition , pages 11124–11133,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='puter Vision and Pattern Recognition , pages 11124–11133,\\n2023. 7\\n[12] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu,\\nHao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da:\\nVisual interactive instruction tuning for omni-3d understand-\\ning reasoning and planning. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 26428–26438, 2024. 2, 3, 6, 7, 9\\n[13] Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel X\\nChang. Scan2cap: Context-aware dense captioning in rgb-\\nd scans. In Proceedings of the IEEE/CVF conference on'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='d scans. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition , pages 3193–3203,\\n2021. 2, 5, 7[14] Zhenyu Chen, Ronghang Hu, Xinlei Chen, Matthias Nießner,\\nand Angel X Chang. Unit3d: A unified transformer for 3d\\ndense captioning and visual grounding. In Proceedings of\\nthe IEEE/CVF international conference on computer vision ,\\npages 18109–18119, 2023. 7\\n[15] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu,\\nYang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang,\\nXiaolin Wei, et al. Mobilevlm: A fast, reproducible and strong'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='vision language assistant for mobile devices. arXiv preprint\\narXiv:2312.16886 , 2023. 8\\n[16] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung,\\nand Steven Hoi. Instructblip: Towards general-purpose vision-\\nlanguage models with instruction tuning, 2023. 8\\n[17] Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wen-\\nhan Xiong. Scene-llm: Extending language model for\\n3d visual understanding and reasoning. arXiv preprint\\narXiv:2403.11401 , 2024. 2, 3, 5\\n[18] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='[18] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng,\\nYilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting\\nthe 3d world into large language models. arXiv preprint\\narXiv:2307.12981 , 2023. 2, 3, 5, 6, 7\\n[19] Haifeng Huang, Zehan Wang, Rongjie Huang, Luping Liu,\\nXize Cheng, Yang Zhao, Tao Jin, and Zhou Zhao. Chat-3d\\nv2: Bridging 3d scene and large language models with object\\nidentifiers. arXiv preprint arXiv:2312.08168 , 2023. 2, 3, 5, 6,\\n7\\n[20] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun\\nLinghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baox-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baox-\\niong Jia, and Siyuan Huang. An embodied generalist agent in\\n3d world. arXiv preprint arXiv:2311.12871 , 2023. 2, 3, 5, 6,\\n7, 9\\n[21] Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multi-\\nview transformer for 3d visual grounding. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 15524–15533, 2022. 7\\n[22] Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Kate-\\nrina Fragkiadaki. Bottom up top down detection transformers\\nfor language grounding in images and point clouds. In Com-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='for language grounding in images and point clouds. In Com-\\nputer Vision–ECCV 2022: 17th European Conference, Tel\\nAviv, Israel, October 23–27, 2022, Proceedings, Part XXXVI ,\\npages 417–433. Springer, 2022. 7\\n[23] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao\\nGu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer,\\nSoroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al. Con-\\nceptfusion: Open-set multimodal 3d mapping. arXiv preprint\\narXiv:2302.07241 , 2023. 2\\n[24] Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin\\nMa, and Yu-Gang Jiang. More: Multi-order relation mining'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='Ma, and Yu-Gang Jiang. More: Multi-order relation mining\\nfor dense captioning in 3d scenes. In European Conference\\non Computer Vision , pages 528–545. Springer, 2022. 7\\n[25] Zhao Jin, Munawar Hayat, Yuwei Yang, Yulan Guo, and\\nYinjie Lei. Context-aware alignment and mutual masking for\\n3d-language pre-training. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 10984–10994, 2023. 7\\n[26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='thing. In Proceedings of the IEEE/CVF International Confer-\\nence on Computer Vision , pages 4015–4026, 2023. 2\\n[27] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg,\\nMohit Bansal, and Jingjing Liu. Less is more: Clipbert for\\nvideo-and-language learning via sparse sampling. In Proceed-\\nings of the IEEE/CVF conference on computer vision and\\npattern recognition , pages 7331–7341, 2021. 5\\n[28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li,\\nHao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chun-\\nyuan Li. Llava-onevision: Easy visual task transfer. arXiv\\npreprint arXiv:2408.03326 , 2024. 2, 5\\n[29] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li,\\nWei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave:\\nTackling multi-image, video, and 3d in large multimodal\\nmodels. arXiv preprint arXiv:2407.07895 , 2024. 3\\n[30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-\\n2: Bootstrapping language-image pre-training with frozen\\nimage encoders and large language models. arXiv preprint\\narXiv:2301.12597 , 2023. 2\\n[31] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai\\nWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\\nVideochat: Chat-centric video understanding. arXiv preprint\\narXiv:2305.06355 , 2023. 2\\n[32] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,\\nYi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.\\nMvbench: A comprehensive multi-modal video understand-\\ning benchmark. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 22195–\\n22206, 2024. 5\\n[33] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An\\nimage is worth 2 tokens in large language models. arXiv\\npreprint arXiv:2311.17043 , 2023. 8\\n[34] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and\\nLi Yuan. Video-llava: Learning united visual represen-\\ntation by alignment before projection. arXiv preprint\\narXiv:2311.10122 , 2023. 2\\n[35] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad\\nShoeybi, and Song Han. Vila: On pre-training for visual\\nlanguage models. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition , pages\\n26689–26699, 2024. 2\\n[36] Benlin Liu, Yuhao Dong, Yiqin Wang, Yongming Rao, Yan-\\nsong Tang, Wei-Chiu Ma, and Ranjay Krishna. Coarse corre-\\nspondence elicit 3d spacetime understanding in multimodal\\nlanguage model. arXiv preprint arXiv:2408.00754 , 2024. 2\\n[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2, 5\\n[38] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pages 26296–26306, 2024. 2, 3, 8\\n[39] Ruiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao,\\nYilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu,\\nDahua Lin, et al. Mmscan: A multi-modal 3d scene dataset\\nwith hierarchical grounded language annotations. arXiv\\npreprint arXiv:2406.09401 , 2024. 2, 6[40] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao\\nLiang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Sit-\\nuated question answering in 3d scenes. arXiv preprint\\narXiv:2210.07474 , 2022. 6\\n[41] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta,\\nSriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mc-\\nvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa:\\nEmbodied question answering in the era of foundation models.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 16488–16498, 2024.\\n2, 6\\n[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748–8763. PMLR, 2021. 2\\n[43] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-\\nactor: A multi-task transformer for robotic manipulation. In\\nConference on Robot Learning , pages 785–799. PMLR, 2023.\\n2\\n[44] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui\\nWu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\\nSchalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a\\nfamily of highly capable multimodal models. arXiv preprint\\narXiv:2312.11805 , 2023. 2, 6\\n[45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-\\njad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\\nBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\\nOpen foundation and fine-tuned chat models. arXiv preprint\\narXiv:2307.09288 , 2023. 6\\n[46] Heng Wang, Chaoyi Zhang, Jianhui Yu, and Weidong Cai.\\nSpatiality-guided transformer for 3d dense captioning on\\npoint clouds. arXiv preprint arXiv:2204.10688 , 2022. 7\\n[47] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng,\\nand Jiashi Feng. Pllava: Parameter-free llava extension from\\nimages to videos for video dense captioning. arXiv preprint\\narXiv:2404.16994 , 2024. 2\\n[48] Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan,\\nMadhavan Iyengar, David F Fouhey, and Joyce Chai. Llm-\\ngrounder: Open-vocabulary 3d visual grounding with large\\nlanguage model as an agent. arXiv preprint arXiv:2309.12311 ,\\n2023. 7\\n[49] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep\\nmodular co-attention networks for visual question answering.\\nInProceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition , pages 6281–6290, 2019. 5\\n[50] Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvg-\\ntransformer: Relation modeling for visual grounding on point\\nclouds. In Proceedings of the IEEE/CVF International Con-\\nference on Computer Vision , pages 2928–2937, 2021. 7\\n[51] Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, and\\nXihui Liu. Empowering 3d visual grounding with reasoning\\ncapabilities. arXiv preprint arXiv:2407.01525 , 2024. 2, 4, 7\\n[52] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan\\nHuang, and Qing Li. 3d-vista: Pre-trained transformer for 3d\\nvision and text alignment. arXiv preprint arXiv:2308.04352 ,\\n2023. 5, 7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\\nthing. In Proceedings of the IEEE/CVF International Confer-\\nence on Computer Vision , pages 4015–4026, 2023. 2\\n[27] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg,\\nMohit Bansal, and Jingjing Liu. Less is more: Clipbert for\\nvideo-and-language learning via sparse sampling. In Proceed-\\nings of the IEEE/CVF conference on computer vision and\\npattern recognition , pages 7331–7341, 2021. 5\\n[28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li,\\nHao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chun-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chun-\\nyuan Li. Llava-onevision: Easy visual task transfer. arXiv\\npreprint arXiv:2408.03326 , 2024. 2, 5\\n[29] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li,\\nWei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave:\\nTackling multi-image, video, and 3d in large multimodal\\nmodels. arXiv preprint arXiv:2407.07895 , 2024. 3\\n[30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-\\n2: Bootstrapping language-image pre-training with frozen\\nimage encoders and large language models. arXiv preprint\\narXiv:2301.12597 , 2023. 2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='arXiv:2301.12597 , 2023. 2\\n[31] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai\\nWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\\nVideochat: Chat-centric video understanding. arXiv preprint\\narXiv:2305.06355 , 2023. 2\\n[32] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,\\nYi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.\\nMvbench: A comprehensive multi-modal video understand-\\ning benchmark. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 22195–\\n22206, 2024. 5\\n[33] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An\\nimage is worth'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='. arXiv\\npreprint arXiv:2311.17043 , 2023. 8\\n[34] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and\\nLi Yuan. Video-llava: Learning united visual represen-\\ntation by alignment before projection. arXiv preprint\\narXiv:2311.10122 , 2023. 2\\n[35] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad\\nShoeybi, and Song Han. Vila: On pre-training for visual\\nlanguage models. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition , pages\\n26689–26699, 2024. 2\\n[36] Benlin Liu, Yuhao Dong, Yiqin Wang, Yongming Rao, Yan-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='[36] Benlin Liu, Yuhao Dong, Yiqin Wang, Yongming Rao, Yan-\\nsong Tang, Wei-Chiu Ma, and Ranjay Krishna. Coarse corre-\\nspondence elicit 3d spacetime understanding in multimodal\\nlanguage model. arXiv preprint arXiv:2408.00754 , 2024. 2\\n[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2, 5\\n[38] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='ceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pages 26296–26306, 2024. 2, 3, 8\\n[39] Ruiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao,\\nYilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu,\\nDahua Lin, et al. Mmscan: A multi-modal 3d scene dataset\\nwith hierarchical grounded language annotations. arXiv\\npreprint arXiv:2406.09401 , 2024. 2, 6[40] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao\\nLiang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Sit-\\nuated question answering in 3d scenes. arXiv preprint\\narXiv:2210.07474 , 2022. 6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='arXiv:2210.07474 , 2022. 6\\n[41] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta,\\nSriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mc-\\nvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa:\\nEmbodied question answering in the era of foundation models.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 16488–16498, 2024.\\n2, 6\\n[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748–8763. PMLR, 2021. 2\\n[43] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-\\nactor: A multi-task transformer for robotic manipulation. In\\nConference on Robot Learning , pages 785–799. PMLR, 2023.\\n2\\n[44] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui\\nWu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\\nSchalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a\\nfamily of highly capable multimodal models. arXiv preprint\\narXiv:2312.11805 , 2023. 2, 6\\n[45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-\\njad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\\nBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\\nOpen foundation and fine-tuned chat models. arXiv preprint\\narXiv:2307.09288 , 2023. 6\\n[46] Heng Wang, Chaoyi Zhang, Jianhui Yu, and Weidong Cai.\\nSpatiality-guided transformer for 3d dense captioning on\\npoint clouds. arXiv preprint arXiv:2204.10688 , 2022. 7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='point clouds. arXiv preprint arXiv:2204.10688 , 2022. 7\\n[47] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng,\\nand Jiashi Feng. Pllava: Parameter-free llava extension from\\nimages to videos for video dense captioning. arXiv preprint\\narXiv:2404.16994 , 2024. 2\\n[48] Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan,\\nMadhavan Iyengar, David F Fouhey, and Joyce Chai. Llm-\\ngrounder: Open-vocabulary 3d visual grounding with large\\nlanguage model as an agent. arXiv preprint arXiv:2309.12311 ,\\n2023. 7\\n[49] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='modular co-attention networks for visual question answering.\\nInProceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition , pages 6281–6290, 2019. 5\\n[50] Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvg-\\ntransformer: Relation modeling for visual grounding on point\\nclouds. In Proceedings of the IEEE/CVF International Con-\\nference on Computer Vision , pages 2928–2937, 2021. 7\\n[51] Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, and\\nXihui Liu. Empowering 3d visual grounding with reasoning\\ncapabilities. arXiv preprint arXiv:2407.01525 , 2024. 2, 4, 7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='[52] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan\\nHuang, and Qing Li. 3d-vista: Pre-trained transformer for 3d\\nvision and text alignment. arXiv preprint arXiv:2308.04352 ,\\n2023. 5, 7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 11}, page_content='Figure 5. LLaV A-3D could perform 2D Click-based 3D dense captioning, generating the corresponding object caption and 3D bounding box.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 11}, page_content='Figure 5. LLaV A-3D could perform 2D Click-based 3D dense captioning, generating the corresponding object caption and 3D bounding box.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 12}, page_content='Figure 6. LLaV A-3D could perform 2D Click-based 3D question answering, now users could click on the 2D images and ask the question.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 12}, page_content='Figure 6. LLaV A-3D could perform 2D Click-based 3D question answering, now users could click on the 2D images and ask the question.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 13}, page_content='Figure 7. LLaV A-3D exhibits powerful 3D visual grounding capability, enabling accurate 3D bounding boxes output.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 13}, page_content='Figure 7. LLaV A-3D exhibits powerful 3D visual grounding capability, enabling accurate 3D bounding boxes output.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 0}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 0}, page_content='EGOLM: M ULTI -MODAL LANGUAGE MODEL OF EGO-\\nCENTRIC MOTIONS\\nFangzhou Hong1,2, Vladimir Guzov1,3, Hyo Jin Kim1, Yuting Ye1\\nRichard Newcombe1, Ziwei Liu2 \\x00, Lingni Ma1 \\x00\\n1Meta Reality Labs Research,2S-Lab, Nanyang Technological University\\n3University of Tuebingen\\n“The person is standing straight as she puts the piece of clothing on the hanger.”“The person turns around then walks out of the bedroom.”Input#1:Sparse Motion Sensors\\nInput#2:Egocentric VideosTask#1:Motion Tracking\\nTask#2:Motion Understanding\\nEgoLMab\\nab\\nFigure 1: We propose EgoLM , a multi-modal language model that unifies egocentric motion track-\\ning and understanding from wearable sensor data, e.g., sparse motion sensors and egocentric videos.\\nABSTRACT\\nAs the prevalence of wearable devices, learning egocentric motions becomes es-\\nsential to develop contextual AI. In this work, we present EgoLM , a versatile\\nframework that tracks andunderstands egocentric motions from multi-modal\\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich con-\\ntexts for the disambiguation of egomotion tracking and understanding, which\\nare ill-posed under single modality conditions. To facilitate the versatile and\\nmulti-modal framework, our key insight is to model the joint distribution of\\negocentric motions and natural languages using large language models (LLM).\\nMulti-modal sensor inputs are encoded and projected to the joint latent space\\nof language models, and used to prompt motion generation or text generation\\nfor egomotion tracking or understanding, respectively. Extensive experiments\\non large-scale multi-modal human motion dataset validate the effectiveness of\\nEgoLM as a generalist model for universal egocentric learning. Project Page:\\nhttps://hongfz16.github.io/projects/EgoLM .\\n1 I NTRODUCTION\\nWith the recent explosive advancement of large language models, their values as intelligent agents\\nhave been thoroughly studied (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; Tou-\\nvron et al., 2023a;b). To better play the role of everyday smart assistant, the contextualization of\\nAI is proposed and studied (Vercauteren et al., 2019; Deepika et al., 2020). Agents are expected\\nto interact with users in a context-aware style, through multi-modal sensors on wearable devices,\\ne.g., smartwatches, smart glasses (Somasundaram et al., 2023). Human motions play an important\\nrole in the user-agent interaction (Plizzari et al., 2023), which requires egocentric human motion\\nlearning (Li et al., 2015).\\nIn this work, we propose a versatile framework EgoLM that approaches human motions learning\\nfrom egocentric perspective. Specifically, EgoLM unifies two aspects of egocentric motion learning,\\ni.e., tracking and understanding. a)Egocentric motion tracking aims to recover full-body motions\\n1arXiv:2409.18127v1  [cs.CV]  26 Sep 2024'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 0}, page_content='Technical Report\\nEGOLM: M ULTI -MODAL LANGUAGE MODEL OF EGO-\\nCENTRIC MOTIONS\\nFangzhou Hong1,2, Vladimir Guzov1,3, Hyo Jin Kim1, Yuting Ye'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 0}, page_content='1, Ziwei Liu2 \\x00, Lingni Ma1 \\x00\\n1Meta Reality Labs Research,2S-Lab, Nanyang Technological University\\n3University of Tuebingen\\n“The person is standing straight as she puts the piece of clothing on the hanger.”“The person turns around then walks out of the bedroom.”Input#1:Sparse Motion Sensors\\nInput#2:Egocentric VideosTask#1:Motion Tracking\\nTask#2:Motion Understanding\\nEgoLMab\\nab\\nFigure 1: We propose EgoLM , a multi-modal language model that unifies egocentric motion track-\\ning and understanding from wearable sensor data, e.g., sparse motion sensors and egocentric videos.\\nABSTRACT'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 0}, page_content='ABSTRACT\\nAs the prevalence of wearable devices, learning egocentric motions becomes es-\\nsential to develop contextual AI. In this work, we present EgoLM , a versatile\\nframework that tracks andunderstands egocentric motions from multi-modal\\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich con-\\ntexts for the disambiguation of egomotion tracking and understanding, which\\nare ill-posed under single modality conditions. To facilitate the versatile and\\nmulti-modal framework, our key insight is to model the joint distribution of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 0}, page_content='egocentric motions and natural languages using large language models (LLM).\\nMulti-modal sensor inputs are encoded and projected to the joint latent space\\nof language models, and used to prompt motion generation or text generation\\nfor egomotion tracking or understanding, respectively. Extensive experiments\\non large-scale multi-modal human motion dataset validate the effectiveness of\\nEgoLM as a generalist model for universal egocentric learning. Project Page:\\nhttps://hongfz16.github.io/projects/EgoLM .'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 0}, page_content='With the recent explosive advancement of large language models, their values as intelligent agents\\nhave been thoroughly studied (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; Tou-\\nvron et al., 2023a;b). To better play the role of everyday smart assistant, the contextualization of\\nAI is proposed and studied (Vercauteren et al., 2019; Deepika et al., 2020). Agents are expected\\nto interact with users in a context-aware style, through multi-modal sensors on wearable devices,\\ne.g., smartwatches, smart glasses (Somasundaram et al., 2023). Human motions play an important'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 0}, page_content='role in the user-agent interaction (Plizzari et al., 2023), which requires egocentric human motion\\nlearning (Li et al., 2015).\\nIn this work, we propose a versatile framework EgoLM that approaches human motions learning\\nfrom egocentric perspective. Specifically, EgoLM unifies two aspects of egocentric motion learning,\\ni.e., tracking and understanding. a)Egocentric motion tracking aims to recover full-body motions\\n1arXiv:2409.18127v'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='from sparse motion sensors, e.g., three-points (head and both wrists) 6-DoF poses (Jiang et al., 2022;\\nCastillo et al., 2023; Du et al., 2023; Jiang et al., 2023) or one-point (only head) 6-DoF poses (Li\\net al., 2023). b)Egocentric motion understanding aims to recognize or describe human motions\\nfrom wearable sensors, e.g., egocentric videos (Damen et al., 2021; 2022; 2018; Nagarajan et al.,\\n2024; Xue et al., 2023; Escobar et al., 2022; Grauman et al., 2022; Rodin et al., 2021; Yonetani\\net al., 2016; Del Molino et al., 2016; Chen et al., 2023). Both tasks are highly challenging due to the\\nincomplete observation from egocentric perspectives . To this end, we propose to approach the\\nchallenges in an unified way by incorporating multiple modalities andmulti-task training , which\\nare elaborated below.\\nEgocentric motion tracking from sparse sensors is an ill-posed problem. Three-points (Jiang et al.,\\n2022) and one-point inputs (Li et al., 2023) miss information of the lower body parts and even\\nhand positions, making it a one-to-many mapping problem. In order to disambiguate the tracking\\nof unobserved body parts, we explore the environment contexts by egocentric videos captured from\\nhead-mounted cameras. Although other body parts are not always visible from egocentric videos,\\nthe semantics of the environment provides valuable clues to disambiguate full body motions.\\nFor egocentric motion understanding, the common input setting is the egocentric video (Jia et al.,\\n2022). However, egocentric videos lacks the accurate information of full-body motion, for their\\nrestricted viewing angles. For better understanding of human motion, sparse motion sensor data is\\nvaluable in terms of providing accurate body part positions (Tan et al., 2024). Therefore, we unify\\nthe input conditions as egocentric videos and sparse sensors for both tracking and understanding.\\nFurther unifying the training of both tasks can also be beneficial, especially for the understanding\\npart. The supervision signals of full-body motions from motion tracking training can contribute to\\nmotion understanding.\\nIn summary, we aim at a multi-modal multi-tasking generative framework. As shown in Fig. 1,\\nEgoLM takes sparse motion sensor data (three-points or one-point) and egocentric videos as inputs.\\nThen motion and natural languages are generated for motion tracking and understanding, respec-\\ntively. To facilitate this versatile framework, there are two main challenges in the framework design,\\nwhich are large modality gaps andlarge task gaps . To that end, our key insight is to use a lan-\\nguage model to handle the multi-modal inputs and multi-task training .\\nUnlike recent VLMs (Liu et al., 2023b;a), our setting is more complex and challenging, where four\\nmodalities are involved, including sparse motion sensor data, egocentric videos, motion represen-\\ntations, and texts. These modalities provide different granularity of information. Human motions\\nand sparse motion sensor data are low-level and contiguous representations with physical meanings.\\nNatural languages, on the other hand, are unstructured and discrete representations. To bridge the\\ngap, we adopt three strategies: a)Treat motions as languages. A motion VQ-V AE is trained to\\ntokenize motions, which can be generated autoregressively by a language model. b)Unify differ-\\nent inputs to the language model space. Sparse sensor data and egocentric videos are encoded and\\nprojected by light-weight temporal encoders. c)Use instruction tuning for multi-task joint training.\\nTo validate the proposed framework, we perform extensive experiments on a large-scale motion\\ndataset, Nymeria (Ma et al., 2024). Compared with previous motion tracking and understanding\\nmethods, our newly proposed multi-modal setup shows its advantages. Our contributions are sum-\\nmarized below.\\n1)We propose a versatile multi-modal generative framework, EgoLM, that unifies egocentric motion\\ntracking and understanding tasks with a language model.\\n2)A new egocentric motion tracking setup is proposed. We combine sparse sensor inputs with\\negocentric videos to provide more contexts that disambiguate this ill-posed problems.\\n3)We propose a practical paradigm of motion understanding by combining sparse sensor data and\\negocentric videos, which provides more accurate full-body motion narration.\\n4)Extensive experiments and studies are performed to show the effectiveness of the proposed frame-\\nwork. Our setup achieves the best performance compared with the state-of-the-art methods.\\n2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='Technical Report\\nfrom sparse motion sensors, e.g., three-points (head and both wrists) 6-DoF poses (Jiang et al., 2022;\\nCastillo et al., 2023; Du et al., 2023; Jiang et al., 2023) or one-point (only head) 6-DoF poses (Li\\net al., 2023). b)Egocentric motion understanding aims to recognize or describe human motions\\nfrom wearable sensors, e.g., egocentric videos (Damen et al., 2021; 2022; 2018; Nagarajan et al.,\\n2024; Xue et al., 2023; Escobar et al., 2022; Grauman et al., 2022; Rodin et al., 2021; Yonetani'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='et al., 2016; Del Molino et al., 2016; Chen et al., 2023). Both tasks are highly challenging due to the\\nincomplete observation from egocentric perspectives . To this end, we propose to approach the\\nchallenges in an unified way by incorporating multiple modalities andmulti-task training , which\\nare elaborated below.\\nEgocentric motion tracking from sparse sensors is an ill-posed problem. Three-points (Jiang et al.,\\n2022) and one-point inputs (Li et al., 2023) miss information of the lower body parts and even'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='hand positions, making it a one-to-many mapping problem. In order to disambiguate the tracking\\nof unobserved body parts, we explore the environment contexts by egocentric videos captured from\\nhead-mounted cameras. Although other body parts are not always visible from egocentric videos,\\nthe semantics of the environment provides valuable clues to disambiguate full body motions.\\nFor egocentric motion understanding, the common input setting is the egocentric video (Jia et al.,\\n2022). However, egocentric videos lacks the accurate information of full-body motion, for their'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='restricted viewing angles. For better understanding of human motion, sparse motion sensor data is\\nvaluable in terms of providing accurate body part positions (Tan et al., 2024). Therefore, we unify\\nthe input conditions as egocentric videos and sparse sensors for both tracking and understanding.\\nFurther unifying the training of both tasks can also be beneficial, especially for the understanding\\npart. The supervision signals of full-body motions from motion tracking training can contribute to\\nmotion understanding.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='motion understanding.\\nIn summary, we aim at a multi-modal multi-tasking generative framework. As shown in Fig. 1,\\nEgoLM takes sparse motion sensor data (three-points or one-point) and egocentric videos as inputs.\\nThen motion and natural languages are generated for motion tracking and understanding, respec-\\ntively. To facilitate this versatile framework, there are two main challenges in the framework design,\\nwhich are large modality gaps andlarge task gaps . To that end, our key insight is to use a lan-\\nguage model to handle the multi-modal inputs and multi-task training .'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='Unlike recent VLMs (Liu et al., 2023b;a), our setting is more complex and challenging, where four\\nmodalities are involved, including sparse motion sensor data, egocentric videos, motion represen-\\ntations, and texts. These modalities provide different granularity of information. Human motions\\nand sparse motion sensor data are low-level and contiguous representations with physical meanings.\\nNatural languages, on the other hand, are unstructured and discrete representations. To bridge the\\ngap, we adopt three strategies: a)Treat motions as languages. A motion VQ-V AE is trained to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='tokenize motions, which can be generated autoregressively by a language model. b)Unify differ-\\nent inputs to the language model space. Sparse sensor data and egocentric videos are encoded and\\nprojected by light-weight temporal encoders. c)Use instruction tuning for multi-task joint training.\\nTo validate the proposed framework, we perform extensive experiments on a large-scale motion\\ndataset, Nymeria (Ma et al., 2024). Compared with previous motion tracking and understanding\\nmethods, our newly proposed multi-modal setup shows its advantages. Our contributions are sum-\\nmarized below.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='marized below.\\n1)We propose a versatile multi-modal generative framework, EgoLM, that unifies egocentric motion\\ntracking and understanding tasks with a language model.\\n2)A new egocentric motion tracking setup is proposed. We combine sparse sensor inputs with\\negocentric videos to provide more contexts that disambiguate this ill-posed problems.\\n3)We propose a practical paradigm of motion understanding by combining sparse sensor data and\\negocentric videos, which provides more accurate full-body motion narration.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='4)Extensive experiments and studies are performed to show the effectiveness of the proposed frame-\\nwork. Our setup achieves the best performance compared with the state-of-the-art methods.\\n2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='2 R ELATED WORK\\nMotion Regression. Large amounts of efforts are devoted to detect and track 2D or 3D keypoints\\nfrom human images and videos (Toshev & Szegedy, 2014; Martinez et al., 2017; Pavllo et al., 2019).\\nTo incorporate more human structure prior, parametric human models, e.g., SMPL (Loper et al.,\\n2023), are used as the regression target (Bogo et al., 2016; Kanazawa et al., 2018). Other than\\nthe cameras, wearable motion sensors are straight-forward in terms of motion capture (Ponton et al.,\\n2023; Mollyn et al., 2023; Milef et al., 2023; Yi et al., 2023; Jiang et al., 2023). Recent advancements\\nin VR/AR devices and applications have developed a new setup for motion tracking, i.e., three-points\\nbody tracking (Du et al., 2023; Jiang et al., 2022; Castillo et al., 2023). EgoEgo (Li et al., 2023)\\nproposes to track motions from only head poses. In this work, we also target motion tracking from\\nsparse sensors. The difference is that we propose to use egocentric videos to disambiguate ill-posed\\nscenarios in this setup.\\nMotion Generation. There have been many efforts in generating motions from various conditions,\\ni.e., action labels (Petrovich et al., 2021; Guo et al., 2020), natural languages (Zhang et al., 2024;\\nTevet et al., 2022; Punnakkal et al., 2021; Guo et al., 2022a; Zhang et al., 2023b; Guo et al., 2022b).\\nRecently, researchers take advantage of powerful LLMs to model the joint motion-language dis-\\ntribution for text-to-motion generation (Jiang et al., 2024; Zhang et al., 2023c; Zhou et al., 2023).\\nIn this work, we also adopt the similar idea of modeling motion together with language models.\\nAs a by-product, we can also perform text-to-motion generation. But our main focus is on motion\\ntracking and understanding from multi-modal inputs.\\nMotion Understanding. There have been many different setups in motion understanding. From\\nthe input side, human videos, either from third-person view (Soomro et al., 2012; Kuehne et al.,\\n2011; Tran et al., 2015; Wang et al., 2016; Yan et al., 2018) or first-person view (Damen et al.,\\n2021; 2022; 2018), are used for this task. From the output side, action recognition/classification\\nhas been a classic task definition (Soomro et al., 2012; Damen et al., 2018). More recently, with\\nthe development of language models, some researches also propose to use natural languages as\\noutput (Jia et al., 2022; Xu et al., 2024; Grauman et al., 2022; Xue et al., 2023; Chen et al., 2023).\\nIn our work, from the input side, we propose to combine egocentric videos, which provide high-\\nlevel semantic information, with motion sensor inputs, which carries low-level motion clues, for\\nmore holistic motion understanding. For the output, we use natural language responses for more\\nversatility and diversity.\\nLanguage Models. Language models have been a huge success in recent years with the large-scale\\npre-training (Radford et al., 2019; Brown et al., 2020) and alignment (cha, 2022; Achiam et al.,\\n2023). To take advantage of the powerful text generation ability, image (Liu et al., 2023b;a) or\\nvideo understanding (Zhang et al., 2023a) are defined as conditional text generation. LLaV A (Liu\\net al., 2023b) proposes to encode images with powerful pre-trained vision encoders (Radford et al.,\\n2021) and inject the features to language models. By tuning from a powerful LLM (Touvron et al.,\\n2023a), LLaV A achieves wonderful abilities of vision question answering. In this work, we also\\nadopt the similar idea of encoding and injecting features of other modalities in the language model\\nand unifying different tasks with instruction tuning.\\n3 M ETHOD\\nThe overview of EgoLM is demonstrated in Fig. 2. There are three steps in EgoLM training. In the\\nfirst step, we train a motion VQ-V AE as the motion tokenizer (Sec. 3.2). The second step is motion\\npre-training for motion distribution learning (Sec. 3.3). The last step is multi-modal instruction\\ntuning to guide the model to perform motion tracking and understanding (Sec. 3.4).\\n3.1 P RELIMINARIES\\nLanguage Model. Language models model the distribution of natural languages. Recent\\nbreakthroughs in language models suggest the effectiveness of the transformer-based architec-\\nture (Vaswani et al., 2017). The language model consists of three parts. The first is a look-up table\\n(LM embedding) that stores the embeddings for each text token. The second part is the transformer\\n3'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='Motion Regression. Large amounts of efforts are devoted to detect and track 2D or 3D keypoints\\nfrom human images and videos (Toshev & Szegedy, 2014; Martinez et al., 2017; Pavllo et al., 2019).\\nTo incorporate more human structure prior, parametric human models, e.g., SMPL (Loper et al.,\\n2023), are used as the regression target (Bogo et al., 2016; Kanazawa et al., 2018). Other than\\nthe cameras, wearable motion sensors are straight-forward in terms of motion capture (Ponton et al.,\\n2023; Mollyn et al., 2023; Milef et al., 2023; Yi et al., 2023; Jiang et al., 2023). Recent advancements'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='in VR/AR devices and applications have developed a new setup for motion tracking, i.e., three-points\\nbody tracking (Du et al., 2023; Jiang et al., 2022; Castillo et al., 2023). EgoEgo (Li et al., 2023)\\nproposes to track motions from only head poses. In this work, we also target motion tracking from\\nsparse sensors. The difference is that we propose to use egocentric videos to disambiguate ill-posed\\nscenarios in this setup.\\nMotion Generation. There have been many efforts in generating motions from various conditions,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='i.e., action labels (Petrovich et al., 2021; Guo et al., 2020), natural languages (Zhang et al., 2024;\\nTevet et al., 2022; Punnakkal et al., 2021; Guo et al., 2022a; Zhang et al., 2023b; Guo et al., 2022b).\\nRecently, researchers take advantage of powerful LLMs to model the joint motion-language dis-\\ntribution for text-to-motion generation (Jiang et al., 2024; Zhang et al., 2023c; Zhou et al., 2023).\\nIn this work, we also adopt the similar idea of modeling motion together with language models.\\nAs a by-product, we can also perform text-to-motion generation. But our main focus is on motion'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='tracking and understanding from multi-modal inputs.\\nMotion Understanding. There have been many different setups in motion understanding. From\\nthe input side, human videos, either from third-person view (Soomro et al., 2012; Kuehne et al.,\\n2011; Tran et al., 2015; Wang et al., 2016; Yan et al., 2018) or first-person view (Damen et al.,\\n2021; 2022; 2018), are used for this task. From the output side, action recognition/classification\\nhas been a classic task definition (Soomro et al., 2012; Damen et al., 2018). More recently, with'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='the development of language models, some researches also propose to use natural languages as\\noutput (Jia et al., 2022; Xu et al., 2024; Grauman et al., 2022; Xue et al., 2023; Chen et al., 2023).\\nIn our work, from the input side, we propose to combine egocentric videos, which provide high-\\nlevel semantic information, with motion sensor inputs, which carries low-level motion clues, for\\nmore holistic motion understanding. For the output, we use natural language responses for more\\nversatility and diversity.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='versatility and diversity.\\nLanguage Models. Language models have been a huge success in recent years with the large-scale\\npre-training (Radford et al., 2019; Brown et al., 2020) and alignment (cha, 2022; Achiam et al.,\\n2023). To take advantage of the powerful text generation ability, image (Liu et al., 2023b;a) or\\nvideo understanding (Zhang et al., 2023a) are defined as conditional text generation. LLaV A (Liu\\net al., 2023b) proposes to encode images with powerful pre-trained vision encoders (Radford et al.,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='2021) and inject the features to language models. By tuning from a powerful LLM (Touvron et al.,\\n2023a), LLaV A achieves wonderful abilities of vision question answering. In this work, we also\\nadopt the similar idea of encoding and injecting features of other modalities in the language model\\nand unifying different tasks with instruction tuning.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='The overview of EgoLM is demonstrated in Fig. 2. There are three steps in EgoLM training. In the\\nfirst step, we train a motion VQ-V AE as the motion tokenizer (Sec. 3.2). The second step is motion\\npre-training for motion distribution learning (Sec. 3.3). The last step is multi-modal instruction\\ntuning to guide the model to perform motion tracking and understanding (Sec. 3.4).\\n3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='Language Model. Language models model the distribution of natural languages. Recent\\nbreakthroughs in language models suggest the effectiveness of the transformer-based architec-\\nture (Vaswani et al., 2017). The language model consists of three parts. The first is a look-up table\\n(LM embedding) that stores the embeddings for each text token. The second part is the transformer\\n3'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='Motion VQ-VA E\\n1) Motions Tokenization\\nLanguage Model\\n<BOS>\\n<EOS>2) Motion Pre-Training\\nEgoLM3) Multi-Modal Instruction Tuning\\nEncoder\\nVision EncoderInstructions\\nMotion Narrations\\nFigure 2: Overview of EgoLM. Three steps are designed for the training of EgoLM, i.e., motion\\ntokenizer training, motion pre-training and multi-modal instruction tuning.\\nbackbone that takes text embeddings as inputs. The output features are mapped to probabilities of\\nthe next tokens by the third part of LM head.\\nMotion Representation. Human motions are represented as sequences of poses, global translations\\nand rotations defined on the root joint. Each frame of pose is represented by joint angles, defined\\non a kinematic tree. For better learning of motion dynamics, we also include joint angle velocity in\\nthe representation. To avoid the normalization of global translation, we use the translation velocity\\nVr\\nt∈R3for each frame, which can be integrated back to global translations. To ease the regression\\ndifficulty of rotation angles, we use 6D rotation representations (Hempel et al., 2022) for the root\\nrotation Rr\\nt∈R6, root rotation velocity Rrv\\nt∈R6, joint angles Rj\\nt∈R22×6, and joint angle\\nvelocity Rjv\\nt∈R22×6. Formally, we represent human motions with Tframes as M={Pt}T\\nt=1,\\nwhere Pt= [Vr\\nt;Rr\\nt;Rrv\\nt;Rj\\nt;Rjv\\nt]∈R279. Forward kinematics (FK) together with integration of\\nroot velocity can be used to recover the joint positions J=FK(M)∈R23×3.\\n3.2 M OTION TOKENIZER\\nTo treat the motion as a foreign language and train with language models, we first need a motion\\ntokenizer, which can be realized by VQ-V AE (Oord et al., 2017). The motion VQ-V AE consists of\\na fully convolutional encoder Eand decoder D. The fully convolutional design enables processing\\nmotions with arbitrary lengths. The encoder embeds raw motion representation to latent features\\nfm=E(M), where fm∈RT/r×c,M∈RT×279.ris the down-sample rate.\\nThen, codebooks are learned to quantize the motion latent features. We use three techniques in the\\nquantization process, which are 1) exponential moving average (EMA), 2) codebook reset (Dhariwal\\net al., 2020), 3) product quantization (Jegou et al., 2010; Lucas et al., 2022). The first two techniques\\nincrease the usage rate of codebooks. Product quantization increases the codebook expressiveness\\nby decomposing the latent space into a Cartesian product of sub-spaces with lower dimensions.\\nSpecifically, the latent feature fmis split equally into Ntrucks {fm\\nn}N\\nn=1, which are quantized\\nseparately by Ncodebooks {Zn}N\\nn=1. Each codebook with Kentries is defined as Zn={zi}K\\ni=1,\\nwhere zi∈Rc/N. The quantization process for feature fm\\ntnat frame tand trunk nis formulated as\\nitn=Q(fm\\ntn) = arg min\\nzi∈Zn∥fm\\ntn−zi∥2. (1)\\nThe resulting indices itnare flattened and used as motion token sequences W={[(in)N\\nn=1]t}T/r\\nt=1,\\nwhich has the length of LW=N×(T/r). After quantization, we obtain the corresponding code-\\nbook entry for the motion latent feature ˆfm={ˆfm\\nt}T/r\\nt=1={zit}T/r\\nt=1. It is input into the decoder D\\nto decode raw motion representation ˆM=D(ˆfm).\\nFor the training of VQ-V AE, two types of training losses are used. The first is the commitment loss\\nLc=∥fm−ˆfm∥2for the codebook learning. The second is motion reconstruction loss Lr, which\\nconsists of raw representation loss Lm, joint position loss Lj, rotation velocity loss Lv, which are\\ndefined as\\nLr=λmLm+λjLj+λvLv=λm∥M−ˆM∥1+λj∥FK(M)−FK(ˆM)∥1 (2)\\n+λv∥Rrv\\n1:T−1−(Rr\\n1:T−1)−1Rr\\n2:T∥1+λv∥Rjv\\n1:T−1−(Rj\\n1:T−1)−1Rj\\n2:T∥1. (3)\\n4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='Technical Report\\nMotion VQ-VA E\\n1) Motions Tokenization\\nLanguage Model\\n<BOS>\\n<EOS>2) Motion Pre-Training\\nEgoLM3) Multi-Modal Instruction Tuning\\nEncoder\\nVision EncoderInstructions\\nMotion Narrations\\nFigure 2: Overview of EgoLM. Three steps are designed for the training of EgoLM, i.e., motion\\ntokenizer training, motion pre-training and multi-modal instruction tuning.\\nbackbone that takes text embeddings as inputs. The output features are mapped to probabilities of\\nthe next tokens by the third part of LM head.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='the next tokens by the third part of LM head.\\nMotion Representation. Human motions are represented as sequences of poses, global translations\\nand rotations defined on the root joint. Each frame of pose is represented by joint angles, defined\\non a kinematic tree. For better learning of motion dynamics, we also include joint angle velocity in\\nthe representation. To avoid the normalization of global translation, we use the translation velocity\\nVr\\nt∈R3for each frame, which can be integrated back to global translations. To ease the regression'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='difficulty of rotation angles, we use 6D rotation representations (Hempel et al., 2022) for the root\\nrotation Rr\\nt∈R6, root rotation velocity Rrv\\nt∈R6, joint angles Rj\\nt∈R22×6, and joint angle\\nvelocity Rjv\\nt∈R22×6. Formally, we represent human motions with Tframes as M={Pt}T\\nt=1,\\nwhere Pt= [Vr\\nt;Rr\\nt;Rrv\\nt;Rj\\nt;Rjv\\nt]∈R279. Forward kinematics (FK) together with integration of\\nroot velocity can be used to recover the joint positions J=FK(M)∈R23×3.\\n3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='To treat the motion as a foreign language and train with language models, we first need a motion\\ntokenizer, which can be realized by VQ-V AE (Oord et al., 2017). The motion VQ-V AE consists of\\na fully convolutional encoder Eand decoder D. The fully convolutional design enables processing\\nmotions with arbitrary lengths. The encoder embeds raw motion representation to latent features\\nfm=E(M), where fm∈RT/r×c,M∈RT×279.ris the down-sample rate.\\nThen, codebooks are learned to quantize the motion latent features. We use three techniques in the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='quantization process, which are 1) exponential moving average (EMA), 2) codebook reset (Dhariwal\\net al., 2020), 3) product quantization (Jegou et al., 2010; Lucas et al., 2022). The first two techniques\\nincrease the usage rate of codebooks. Product quantization increases the codebook expressiveness\\nby decomposing the latent space into a Cartesian product of sub-spaces with lower dimensions.\\nSpecifically, the latent feature fmis split equally into Ntrucks {fm\\nn}N\\nn=1, which are quantized\\nseparately by Ncodebooks {Zn}N\\nn=1. Each codebook with Kentries is defined as Zn={zi}K\\ni=1,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='i=1,\\nwhere zi∈Rc/N. The quantization process for feature fm\\ntnat frame tand trunk nis formulated as\\nitn=Q(fm\\ntn) = arg min\\nzi∈Zn∥fm\\ntn−zi∥2. (1)\\nThe resulting indices itnare flattened and used as motion token sequences W={[(in)N\\nn=1]t}T/r\\nt=1,\\nwhich has the length of LW=N×(T/r). After quantization, we obtain the corresponding code-\\nbook entry for the motion latent feature ˆfm={ˆfm\\nt}T/r\\nt=1={zit}T/r\\nt=1. It is input into the decoder D\\nto decode raw motion representation ˆM=D(ˆfm).\\nFor the training of VQ-V AE, two types of training losses are used. The first is the commitment loss'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='Lc=∥fm−ˆfm∥2for the codebook learning. The second is motion reconstruction loss Lr, which\\nconsists of raw representation loss Lm, joint position loss Lj, rotation velocity loss Lv, which are\\ndefined as\\nLr=λmLm+λjLj+λvLv=λm∥M−ˆM∥1+λj∥FK(M)−FK(ˆM)∥1 (2)\\n+λv∥Rrv\\n1:T−1−(Rr\\n1:T−1)−1Rr\\n2:T∥1+λv∥Rjv\\n1:T−1−(Rj\\n1:T−1)−1Rj\\n2:T∥1. (3)\\n4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='“<s>Perform ... based on the given ... Input CLIP embeddings: <CLIP_Placeholder>. Input three-points: <TP_Placeholder>”\\n1\\n27313\\n2729\\n373\\n278\\n10567\\n24492\\n29925\\n8297\\n10567\\n2211\\n29899\\n...\\n...\\nCLIP Encoder\\nText Tokenizer\\nText Tokenizer\\nLinear Layer\\nLM Embedding\\nLM Embedding\\nTP Encoder\\nConcatenate\\nFigure 3: Details of Multi-Modal Instruction Tuning. Different modalities are encoded separately.\\nTheir features are concatenated in the order of the instruction template and input into the transformer\\nlayers of the language model.\\nWe define the smoothed L1 loss as ∥ · ∥ 1. In summary, the training loss of the motion VQ-V AE is\\nLvq=λcLc+λrLr, where λ∗are manually adjusted weights.\\n3.3 M OTION PRE-TRAINING\\nAs discussed before, we build the motion learning framework on a pre-trained language model.\\nHowever, the pre-trained language models only model the distribution of natural languages. There-\\nfore, to empower them to generate motions, we perform motion pre-training to learn motion distri-\\nbutions. The motion pre-training is conducted similarly to language model pre-training.\\nBefore we can start training the language model, two modifications to the model are needed. Firstly,\\nsince the pre-trained language model only contains embeddings for text tokens, we expand the em-\\nbeddings in accordance with the motion codebook size. Secondly, the output shape of the language\\nmodel head is also expanded for the same reason. The language model is ready for motion pre-\\ntraining after the above preparations. Using the motion tokenizer described above, motion represen-\\ntations Mcan be encoded into a sequence of motion tokens W={wi}LW\\ni=1. They are fed into the\\nlanguage model to learn the motion token distribution by conducting the classic next-token predic-\\ntion (Radford et al., 2019). The loss function of this stage Lpreis formulated as\\nLpre=−LWX\\ni=2P(wi|w1...wi−1; Θ), (4)\\nwhere we maximize the log-likelihood of the next-token probability given the previous token inputs\\nand network parameter Θ.\\nAfter the training of this stage, as a by-product, we obtain an unconditional motion generator. Given\\na leading motion sequence as the prompt, it can autoregressively sample an arbitrary length of rea-\\nsonable human motion that continues the given motion. More importantly, the language model\\nlearns the distribution of human motions and has the ability of sampling plausible human motions,\\nwhich lays a solid foundation for the next stage.\\n3.4 M ULTI -MODAL INSTRUCTION TUNING\\nInspired by recent advancements in LLMs (Achiam et al., 2023; cha, 2022; Zheng et al., 2023),\\nto squeeze the power out of generative pre-training models, instruction tuning is adopted to guide\\nmodels with instructions to perform specific tasks. The instruction template usually consists of 1)\\ninstructions that specify which tasks to perform; 2) inputs of the task; 3) outputs. We also envision\\nour model accepting multi-modal sensor data as inputs. However, even with motion pre-training,\\nthe model only accepts text or motion tokens as inputs. It is not practical or necessary to design\\ntokenizers and perform pre-training for all the involved modalities. Therefore, we draw inspiration\\nfrom vision language models (Liu et al., 2023b;a), where they directly map vision data to LLM\\nfeature space to enable visual question answering.\\nSpecifically, we consider two input modalities other than motion and natural languages, which are\\negocentric videos and motion sensor inputs. Motion sensor inputs can be three-points (head and\\nwrists) 6-DoF poses or one-point (only head) 6-DoF poses. Both are encoded with positions, veloc-\\nity, rotation and angular velocity. Below, we use three-points as examples. We unify both motion\\ntracking and motion understanding using the following templates.\\n5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='Technical Report\\n“<s>Perform ... based on the given ... Input CLIP embeddings: <CLIP_Placeholder>. Input three-points: <TP_Placeholder>”\\n1\\n27313\\n2729\\n373\\n278\\n10567\\n24492\\n29925\\n8297\\n10567\\n2211\\n29899\\n...\\n...\\nCLIP Encoder\\nText Tokenizer\\nText Tokenizer\\nLinear Layer\\nLM Embedding\\nLM Embedding\\nTP Encoder\\nConcatenate\\nFigure 3: Details of Multi-Modal Instruction Tuning. Different modalities are encoded separately.\\nTheir features are concatenated in the order of the instruction template and input into the transformer\\nlayers of the language model.\\nWe define the smoothed L'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='∥ · ∥ 1. In summary, the training loss of the motion VQ-V AE is\\nLvq=λcLc+λrLr, where λ∗are manually adjusted weights.\\n3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='-TRAINING\\nAs discussed before, we build the motion learning framework on a pre-trained language model.\\nHowever, the pre-trained language models only model the distribution of natural languages. There-\\nfore, to empower them to generate motions, we perform motion pre-training to learn motion distri-\\nbutions. The motion pre-training is conducted similarly to language model pre-training.\\nBefore we can start training the language model, two modifications to the model are needed. Firstly,\\nsince the pre-trained language model only contains embeddings for text tokens, we expand the em-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='beddings in accordance with the motion codebook size. Secondly, the output shape of the language\\nmodel head is also expanded for the same reason. The language model is ready for motion pre-\\ntraining after the above preparations. Using the motion tokenizer described above, motion represen-\\ntations Mcan be encoded into a sequence of motion tokens W={wi}LW\\ni=1. They are fed into the\\nlanguage model to learn the motion token distribution by conducting the classic next-token predic-\\ntion (Radford et al., 2019). The loss function of this stage Lpreis formulated as\\nLpre=−LWX'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='Lpre=−LWX\\ni=2P(wi|w1...wi−1; Θ), (4)\\nwhere we maximize the log-likelihood of the next-token probability given the previous token inputs\\nand network parameter Θ.\\nAfter the training of this stage, as a by-product, we obtain an unconditional motion generator. Given\\na leading motion sequence as the prompt, it can autoregressively sample an arbitrary length of rea-\\nsonable human motion that continues the given motion. More importantly, the language model\\nlearns the distribution of human motions and has the ability of sampling plausible human motions,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='which lays a solid foundation for the next stage.\\n3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='-MODAL INSTRUCTION TUNING\\nInspired by recent advancements in LLMs (Achiam et al., 2023; cha, 2022; Zheng et al., 2023),\\nto squeeze the power out of generative pre-training models, instruction tuning is adopted to guide\\nmodels with instructions to perform specific tasks. The instruction template usually consists of 1)\\ninstructions that specify which tasks to perform; 2) inputs of the task; 3) outputs. We also envision\\nour model accepting multi-modal sensor data as inputs. However, even with motion pre-training,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='the model only accepts text or motion tokens as inputs. It is not practical or necessary to design\\ntokenizers and perform pre-training for all the involved modalities. Therefore, we draw inspiration\\nfrom vision language models (Liu et al., 2023b;a), where they directly map vision data to LLM\\nfeature space to enable visual question answering.\\nSpecifically, we consider two input modalities other than motion and natural languages, which are\\negocentric videos and motion sensor inputs. Motion sensor inputs can be three-points (head and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='wrists) 6-DoF poses or one-point (only head) 6-DoF poses. Both are encoded with positions, veloc-\\nity, rotation and angular velocity. Below, we use three-points as examples. We unify both motion\\ntracking and motion understanding using the following templates.\\n5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='Task: Motion Tracking\\nInstruction: Perform motion tracking based on\\nthe given three-points and CLIP embeddings.\\nInput: Input CLIP embeddings:\\n<CLIP Placeholder> . Input three-\\npoints feature: <TPPlaceholder>\\nOutput: <Motion Placeholder>Task: Motion Understanding\\nInstruction: Describe the human motion based on\\nthe given three-points and CLIP embeddings.\\nInput: Input CLIP embeddings:\\n<CLIP Placeholder> . Input three-points\\nfeature: <TPPlaceholder>\\nOutput: <Narration Placeholder>\\nThe encoded three-points 6-DoF poses would replace <TPPlaceholder> .\\n<CLIP Placeholder> is the placeholder for egocentric video features. Motion tokens\\nare filled in <Motion Placeholder> .<Narration Placeholder> is the placeholder for\\ncorresponding motion narration. A detailed illustration of how we organize different modalities\\nof data is shown in Fig. 3. Texts are tokenized and translated to feature vectors through LM\\nembedding. Egocentric videos are first encoded by CLIP image encoder (Radford et al., 2021) per\\nframe, which are further projected by linear layers to the language model feature space. Similarly,\\nmotion sensor data, e.g., sequences of three-points 6-DoF poses, is encoded by a fully convolutional\\nencoder. Lastly, all the encoded features are concatenated and input into the transformer layers of\\nthe language model.\\nFor the training of motion understanding, to better learn the joint distribution of motion and natural\\nlanguages, we also include two auxiliary tasks in the joint instruction training, which are motion-\\nto-text and text-to-motion generation. They are also defined with the templates similar to the above\\nones. In summary, we train the four tasks jointly as the last step. The loss function is the same\\nnext-token prediction loss, as defined in Eq. 4.\\n4 E XPERIMENTS\\n4.1 E XPERIMENTAL SETUP\\nDataset. We use the Nymeria dataset Ma et al. (2024) to train and validate our method. The dataset\\nprovides a)full body motions, captured by the Xsens Mocap system (Roetenberg et al., 2009),\\nb)egocentric videos, captured by Aria glasses (Somasundaram et al., 2023), and c)narrations of\\nmotions written by human annotators. Three-points 6-DoF poses are taken from ground truth joints.\\nFor motion tracking, the training set consists of 147.89h of data and the test set has 41.93h of data.\\nFor motion understanding, the training set has 16673 segments, each lasting for 3-5seconds, adding\\nup to 15.77h. The test set consists of 7468 segments, 6.76h of data.\\nTraining Details. Motion VQ-V AE has two codebooks, each having 8192 entries and code dimen-\\nsion of 64. The down-sample rate is r= 4. For motion tracking, all experiments are conducted with\\nwindow size of 60frames, which is 1second. Random rotation augmentation is applied on motions.\\nWe choose to use GPT2-Medium (Radford et al., 2019) as the language backbone.\\nEvaluation Protocols. For motion tracking, we calculate joint position errors (for full, upper and\\nlower body), joint angle errors (for full body and root joint). For motion understanding, the outputs\\nare natural languages. Therefore, we adopt NLP metrics, including BERT (Zhang et al., 2019),\\nBLEU (Papineni et al., 2002), and ROUGE (Lin, 2004) scores.\\n4.2 M OTION TRACKING\\nQuantitative Results. We report quantitative results of motion tracking in Tab. 1. All methods\\nare evaluated with batch inference, meaning that every 60frames are inferenced independently.\\nWe evaluate several different input combinations of three modalities, which are three-points 6-DoF\\nposes (“3pts”), one-point 6-DoF poses (“1pt”) and egocentric videos (“Vid”). For the 3pts-only and\\n1pt-only settings, EgoLM achieves comparable performance with baseline methods. This show the\\neffectiveness of using language models to perform precise motion tracking tasks. Moreover, we\\nalso use egocentric videos to provide environment contexts for motion tracking. For three-points\\ntracking, the additional modality brings 10mm improvement in full body joints error. For the one-\\npoint tracking, adding egocentric videos improves joints error by 20mm. It shows the effectiveness\\nof using egocentric videos as context information for disambiguation of the ill-posed problem.\\n6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='Technical Report\\nTask: Motion Tracking\\nInstruction: Perform motion tracking based on\\nthe given three-points and CLIP embeddings.\\nInput: Input CLIP embeddings:\\n<CLIP Placeholder> . Input three-\\npoints feature: <TPPlaceholder>\\nOutput: <Motion Placeholder>Task: Motion Understanding\\nInstruction: Describe the human motion based on\\nthe given three-points and CLIP embeddings.\\nInput: Input CLIP embeddings:\\n<CLIP Placeholder> . Input three-points\\nfeature: <TPPlaceholder>\\nOutput: <Narration Placeholder>\\nThe encoded three-points 6-DoF poses would replace <TPPlaceholder> .'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='<CLIP Placeholder> is the placeholder for egocentric video features. Motion tokens\\nare filled in <Motion Placeholder> .<Narration Placeholder> is the placeholder for\\ncorresponding motion narration. A detailed illustration of how we organize different modalities\\nof data is shown in Fig. 3. Texts are tokenized and translated to feature vectors through LM\\nembedding. Egocentric videos are first encoded by CLIP image encoder (Radford et al., 2021) per\\nframe, which are further projected by linear layers to the language model feature space. Similarly,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='motion sensor data, e.g., sequences of three-points 6-DoF poses, is encoded by a fully convolutional\\nencoder. Lastly, all the encoded features are concatenated and input into the transformer layers of\\nthe language model.\\nFor the training of motion understanding, to better learn the joint distribution of motion and natural\\nlanguages, we also include two auxiliary tasks in the joint instruction training, which are motion-\\nto-text and text-to-motion generation. They are also defined with the templates similar to the above'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='ones. In summary, we train the four tasks jointly as the last step. The loss function is the same\\nnext-token prediction loss, as defined in Eq. 4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='Dataset. We use the Nymeria dataset Ma et al. (2024) to train and validate our method. The dataset\\nprovides a)full body motions, captured by the Xsens Mocap system (Roetenberg et al., 2009),\\nb)egocentric videos, captured by Aria glasses (Somasundaram et al., 2023), and c)narrations of\\nmotions written by human annotators. Three-points 6-DoF poses are taken from ground truth joints.\\nFor motion tracking, the training set consists of 147.89h of data and the test set has 41.93h of data.\\nFor motion understanding, the training set has 166'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='-\\nsion of 64. The down-sample rate is r= 4. For motion tracking, all experiments are conducted with\\nwindow size of 60frames, which is 1second. Random rotation augmentation is applied on motions.\\nWe choose to use GPT2-Medium (Radford et al., 2019) as the language backbone.\\nEvaluation Protocols. For motion tracking, we calculate joint position errors (for full, upper and\\nlower body), joint angle errors (for full body and root joint). For motion understanding, the outputs\\nare natural languages. Therefore, we adopt NLP metrics, including BERT (Zhang et al., 2019),'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='BLEU (Papineni et al., 2002), and ROUGE (Lin, 2004) scores.\\n4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='Quantitative Results. We report quantitative results of motion tracking in Tab. 1. All methods\\nare evaluated with batch inference, meaning that every 60frames are inferenced independently.\\nWe evaluate several different input combinations of three modalities, which are three-points 6-DoF\\nposes (“3pts”), one-point 6-DoF poses (“1pt”) and egocentric videos (“Vid”). For the 3pts-only and\\n1pt-only settings, EgoLM achieves comparable performance with baseline methods. This show the\\neffectiveness of using language models to perform precise motion tracking tasks. Moreover, we'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='also use egocentric videos to provide environment contexts for motion tracking. For three-points\\ntracking, the additional modality brings 10mm improvement in full body joints error. For the one-\\npoint tracking, adding egocentric videos improves joints error by 20mm. It shows the effectiveness\\nof using egocentric videos as context information for disambiguation of the ill-posed problem.\\n6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 6}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 6}, page_content='Table 1: Quantitative Results of Motion Tracking. “Full”, “Upper”, “Lower” are joint position\\nerrors in mm. “J.A.”, “Root” are joint angle errors for full body and root joint in degree.†We\\ndirectly replace three-points with one-point to train AvatarPoser.\\nMethodInput ModalityFull Upper Lower J.A. Root3pts 1pt Vid.\\nAvatarPoser (Jiang et al., 2022) ✓ 85.89 52.78 165.18 12.41 14.78\\nBodiffusion (Castillo et al., 2023) ✓ 79.80 52.79 152.68 12.74 13.09\\nOurs ✓ 83.88 54.06 148.37 13.31 14.13\\nOurs ✓ ✓ 73.38 49.67 124.58 12.48 13.23\\nAvatarPoser†(Jiang et al., 2022) ✓ 129.23 94.19 192.34 16.55 21.60\\nEgoEgo (Li et al., 2023) ✓ 132.16 100.02 190.32 18.90 21.80\\nOurs ✓ 127.45 97.87 174.92 16.97 20.57\\nOurs ✓ ✓ 106.95 83.73 141.26 14.67 19.04\\nAvatarPoserBoDiffusion\\nOurs\\nGT\\n0mm200mmEgocentricVideo\\nFigure 4: Qualitative Results of Three-Points Motion Tracking. Skeletons are color-coded by the\\njoint position errors. Baseline methods only use three-points as inputs. Ours uses three-points and\\negocentric videos as inputs.\\nQualitative Results. Three-points motion tracking results and comparisons are shown in Fig. 4.\\nDue to the ambiguity of three-points, AvatarPoser mistakenly generates standing poses for squatting\\nsequences (right example). BoDiffusion, for its generative nature, can sample correct results in\\nsome cases, e.g., the squatting example. But it also suffers from the ambiguity issue, as shown in\\nthe bending down sequence (left example). They show the importance of considering contexts in\\nthe motion tracking task for the purpose of disambiguation. Our full model can reliably perform\\nthree-points body tracking for the shown challenging cases.\\nOne-point motion tracking results are shown in Fig. 5. It is a more challenging task especially\\nfor upper body. As shown in the left example, the upper body motions generated by EgoEgo are\\ncompletely different from the ground truth. In the right example, EgoEgo wrongly generates sitting\\nposes for standing frames and standing poses for sitting frames, which is caused by the ambiguity\\nproblem. Egocentric videos in this task not only help to eliminate the ambiguity, but also provide\\nsome clues about the hand position. In the left example, when hands are visible in the frames, our\\nmodel captures this information through CLIP embeddings and generates correct arm movements.\\n7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 6}, page_content='Technical Report\\nTable 1: Quantitative Results of Motion Tracking. “Full”, “Upper”, “Lower” are joint position\\nerrors in mm. “J.A.”, “Root” are joint angle errors for full body and root joint in degree.†We\\ndirectly replace three-points with one-point to train AvatarPoser.\\nMethodInput ModalityFull Upper Lower J.A. Root3pts 1pt Vid.\\nAvatarPoser (Jiang et al., 2022) ✓ 85.89 52.78 165.18 12.41 14.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 6}, page_content='Ours\\nGT\\n0mm200mmEgocentricVideo\\nFigure 4: Qualitative Results of Three-Points Motion Tracking. Skeletons are color-coded by the\\njoint position errors. Baseline methods only use three-points as inputs. Ours uses three-points and\\negocentric videos as inputs.\\nQualitative Results. Three-points motion tracking results and comparisons are shown in Fig. 4.\\nDue to the ambiguity of three-points, AvatarPoser mistakenly generates standing poses for squatting\\nsequences (right example). BoDiffusion, for its generative nature, can sample correct results in'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 6}, page_content='some cases, e.g., the squatting example. But it also suffers from the ambiguity issue, as shown in\\nthe bending down sequence (left example). They show the importance of considering contexts in\\nthe motion tracking task for the purpose of disambiguation. Our full model can reliably perform\\nthree-points body tracking for the shown challenging cases.\\nOne-point motion tracking results are shown in Fig. 5. It is a more challenging task especially\\nfor upper body. As shown in the left example, the upper body motions generated by EgoEgo are'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 6}, page_content='completely different from the ground truth. In the right example, EgoEgo wrongly generates sitting\\nposes for standing frames and standing poses for sitting frames, which is caused by the ambiguity\\nproblem. Egocentric videos in this task not only help to eliminate the ambiguity, but also provide\\nsome clues about the hand position. In the left example, when hands are visible in the frames, our\\nmodel captures this information through CLIP embeddings and generates correct arm movements.\\n7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='EgoEgoOursEgocentricVideo0mm200mmGT\\nFigure 5: Qualitative Results of One-Point Motion Tracking. Skeletons are color-coded by joint\\nposition errors. EgoEgo only uses one-point as inputs. Ours includes egocentric videos as inputs.\\n4.3 M OTION UNDERSTANDING\\nQuantitative Results. We report quantitative results of motion understanding in Tab. 2. For\\nthis task, we tested three input modalities, i.e., three-points (“3pts”), motions, and egocentric\\nvideos (“Vid”). Different combinations of these modalities are evaluated. We first test and com-\\npare with two motion understanding methods that only take motion as inputs, TM2T (Guo et al.,\\n2022b) and MotionGPT (Jiang et al., 2024). TM2T trains language generation from scratch, which\\nexplains its poor performance. MotionGPT uses a pre-trained T5 model (Raffel et al., 2020).\\nEgoLM(M2T&T2M) achieves the best performance for the scalability advantage brought by the\\ndecoder-only architecture.\\nUsing motion as inputs requires precise motion tracking, which is not always available. So, we\\nexplored using sensor inputs instead. We tested two variants: three-points-only (TP2T) and egocen-\\ntric videos only (V2T). The TP2T variant showed a noticeable drop in performance compared to the\\nmotion-only version, as three-points provide limited information about body motion. In contrast, the\\nV2T variant outperformed the motion-only version because egocentric videos capture environmen-\\ntal context relevant to our motion narrations. This highlights the importance of egocentric videos in\\nunderstanding motion.\\nWe then test our proposed setup of combing three-points and egocentric videos for motion under-\\nstanding. There are three ways of achieving this setup. The first one is to combine two existing\\nsetups: 1) three-points motion tracking and 2) motion-to-text generation (TPV2M +MV2T). The\\nperformance of this variant slightly drops compared with MV2T variant, due to error accumulation.\\nThe second way is directly training three-points plus egocentric video to text generation (TPV2T)\\nwith the proposed multi-modal instruction tuning. It is better than only using egocentric videos or\\nmotions. However, it still falls behind MV2T variant for the missing lower body information. To\\nsolve that, we propose to also include three-points motion tracking in training to actively establish\\nthe connection between three-points and motion narrations. Joint training improves motion under-\\nstanding from three-points plus egocentric video, which proves the effectiveness of using motion as\\na bridge between different modalities.\\nQualitative Results. We show four examples of motion understanding in Fig. 6. TM2T and Mo-\\ntionGPT use full body motions as inputs. Ours is the full version with three-points and egocentric\\nvideos as inputs. TM2T’s language generation part is trained from scratch. Therefore, it often makes\\nmistakes about motions and even generates texts that does not make sense. MotionGPT can generate\\nreasonable descriptions for the motions. In the lower left example, just from the motions, “removing\\na piece of clothing from the hanger” is a reasonable answer. However, our target motion narration\\n8'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='Technical Report\\nEgoEgoOursEgocentricVideo0mm200mmGT\\nFigure 5: Qualitative Results of One-Point Motion Tracking. Skeletons are color-coded by joint\\nposition errors. EgoEgo only uses one-point as inputs. Ours includes egocentric videos as inputs.\\n4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='Quantitative Results. We report quantitative results of motion understanding in Tab. 2. For\\nthis task, we tested three input modalities, i.e., three-points (“3pts”), motions, and egocentric\\nvideos (“Vid”). Different combinations of these modalities are evaluated. We first test and com-\\npare with two motion understanding methods that only take motion as inputs, TM2T (Guo et al.,\\n2022b) and MotionGPT (Jiang et al., 2024). TM2T trains language generation from scratch, which\\nexplains its poor performance. MotionGPT uses a pre-trained T'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='(Raffel et al., 2020).\\nEgoLM(M2T&T2M) achieves the best performance for the scalability advantage brought by the\\ndecoder-only architecture.\\nUsing motion as inputs requires precise motion tracking, which is not always available. So, we\\nexplored using sensor inputs instead. We tested two variants: three-points-only (TP2T) and egocen-\\ntric videos only (V2T). The TP2T variant showed a noticeable drop in performance compared to the\\nmotion-only version, as three-points provide limited information about body motion. In contrast, the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='V2T variant outperformed the motion-only version because egocentric videos capture environmen-\\ntal context relevant to our motion narrations. This highlights the importance of egocentric videos in\\nunderstanding motion.\\nWe then test our proposed setup of combing three-points and egocentric videos for motion under-\\nstanding. There are three ways of achieving this setup. The first one is to combine two existing\\nsetups: 1) three-points motion tracking and 2) motion-to-text generation (TPV2M +MV2T). The'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='performance of this variant slightly drops compared with MV2T variant, due to error accumulation.\\nThe second way is directly training three-points plus egocentric video to text generation (TPV2T)\\nwith the proposed multi-modal instruction tuning. It is better than only using egocentric videos or\\nmotions. However, it still falls behind MV2T variant for the missing lower body information. To\\nsolve that, we propose to also include three-points motion tracking in training to actively establish\\nthe connection between three-points and motion narrations. Joint training improves motion under-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='standing from three-points plus egocentric video, which proves the effectiveness of using motion as\\na bridge between different modalities.\\nQualitative Results. We show four examples of motion understanding in Fig. 6. TM2T and Mo-\\ntionGPT use full body motions as inputs. Ours is the full version with three-points and egocentric\\nvideos as inputs. TM2T’s language generation part is trained from scratch. Therefore, it often makes\\nmistakes about motions and even generates texts that does not make sense. MotionGPT can generate'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='reasonable descriptions for the motions. In the lower left example, just from the motions, “removing\\na piece of clothing from the hanger” is a reasonable answer. However, our target motion narration\\n8'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content=\"Table 2: Quantitative Results of Motion Understanding. Different input modality combinations\\nare tested. All metrics are higher the better.\\nMethodInput ModalityBert↑Bleu@1 ↑Bleu@4 ↑RougeL ↑3pts Motion Vid.\\nTM2T (Guo et al., 2022b) ✓ 11.08 40.11 8.99 30.70\\nMotionGPT (Jiang et al., 2024) ✓ 14.09 42.22 10.31 32.33\\nOurs (M2T&T2M) ✓ 15.90 42.68 11.06 33.71\\nOurs (TP2T) ✓ 11.94 41.70 9.85 31.47\\nOurs (V2T) ✓ 16.62 43.03 11.34 33.13\\nOurs (TPV2M +MV2T) ✓ ✓ 19.97 45.41 12.81 35.04\\nOurs (TPV2T) ✓ ✓ 18.38 44.55 12.12 33.80\\nOurs (Joint Training) ✓ ✓ 19.40 45.45 12.72 34.82\\nOurs: The human is standing in the bedroomto fold the piece of clothing. The human is folding the piece of clothing with his left and right hand. The person is resting his left and right foot on the floor.\\nOurs:The person bends down as she plankson the floor. The human extends both of her arms on the floor to support her body. The person extends both of her legs while tiptoeing both of her feet.\\nOurs: The person is standing still in the living room while talking to her peer. The human lifts both of his arms and then moves both hands in circular motion as she gesticulates. The human rests both of his feet on the ground.\\nOurs: The person is standing by the refrigerator while putting the pack of food inside the freezer. The human puts the pack of food inside the freezer with her right hand as her left hand holds the refrigerator door. The human is standing with both feed fixed on the floor.MotionGPT: The person is standing still in front of the sofawhile holding a piece of clothing. The human's left arm is bent and raised upward with his left hand holding a piece of clothing. The human is standing with both legs apart and both feet resting on the floor.TM2T:The person is sitting at the table as he lays her body on the sofa then leans backwards while talking and looking at her colleague. The person is resting both of her arms on her lap, lifts and bends both of her arms as she sits down on the sofa. The person is sitting on the sofa with both legs bent and slightly spread apart.\\nMotionGPT: The human bends down while kneeling in the living area. The person extends both of her arms on the floor to support her body. The human extends both of her legs on the ground.TM2T: The person lowers her thigh as she lays down on the floor while kneeling on the floor. The person extends both her arms as she moves her right elbow on the floor to support her body.MotionGPT: The person is standing straight at the living room … The human has both arms naturally hanging at her sides then she bends, extends and raises her right arm and throws the object on the living room with her right hand. … The human has both feet fixed on the floor with both legs stretched upright then she slightly bends and spreads both of her legs widely apart.TM2T:The person is standing still in front of the cabinet while making a hanger. The person bends and raises her left hand then lays the hanger on her side of her chest then spreads both arms on her side below her chest. The person stands with both legs stretched upright and both feet fixed on the floor.\\nMotionGPT:The person stands in front of the cabinet to remove the clothes from the hanger. the human raises both of his arms to remove a piece of clothing from the hanger. the human stands with both feet fixed on the floor.TM2T: the person stands up straight as she holds the pillow and place them on the table. the person then arrange the pillow in the middle of the room with her right hand and places it on the table, while her left arm is slightly bent in front as she holds and arrange the pillow in the direction of the table.\\nFigure 6: Qualitative Results of Motion Understanding. We use green to highlight correct parts\\nand red for mistakes.\\nis highly related to environments. TM2T and MotionGPT fail to generate correct narrations for the\\nlack of vision signals. For our model, even though we do not directly use motions as inputs, EgoLM\\njointly model the distributions of different modalities and can generate correct narrations according\\nto different scenarios.\\n4.4 A BLATION STUDY\\nWindow Size of Motion Tracking. As shown in Tab. 3, we increase the window size for three-\\npoints motion tracking from 60to120frames, which brings an improvement of 4.2mm in joint\\nposition errors. This is reasonable since increasing the window size brings more contexts, which\\nhelps the disambiguation. If we further include egocentric videos in the inputs, the improvement\\nof increasing window size is not as large. Moreover, using 60frames plus egocentric video shows\\nbetter performance than only using 120frames. This indicates that the context of egocentric video\\nmight be more effective than increasing window size.\\n9\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='Technical Report\\nTable 2: Quantitative Results of Motion Understanding. Different input modality combinations\\nare tested. All metrics are higher the better.\\nMethodInput ModalityBert↑Bleu@1 ↑Bleu@4 ↑RougeL ↑3pts Motion Vid.\\nTM2T (Guo et al., 2022b) ✓ 11.08 40.11 8.99 30.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content=': The human is standing in the bedroomto fold the piece of clothing. The human is folding the piece of clothing with his left and right hand. The person is resting his left and right foot on the floor.\\nOurs:The person bends down as she plankson the floor. The human extends both of her arms on the floor to support her body. The person extends both of her legs while tiptoeing both of her feet.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='Ours: The person is standing still in the living room while talking to her peer. The human lifts both of his arms and then moves both hands in circular motion as she gesticulates. The human rests both of his feet on the ground.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content=\"Ours: The person is standing by the refrigerator while putting the pack of food inside the freezer. The human puts the pack of food inside the freezer with her right hand as her left hand holds the refrigerator door. The human is standing with both feed fixed on the floor.MotionGPT: The person is standing still in front of the sofawhile holding a piece of clothing. The human's left arm is bent and raised upward with his left hand holding a piece of clothing. The human is standing with both legs apart and both feet resting on the floor.TM2T:The person is sitting at the table as he lays her\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='person is sitting at the table as he lays her body on the sofa then leans backwards while talking and looking at her colleague. The person is resting both of her arms on her lap, lifts and bends both of her arms as she sits down on the sofa. The person is sitting on the sofa with both legs bent and slightly spread apart.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='MotionGPT: The human bends down while kneeling in the living area. The person extends both of her arms on the floor to support her body. The human extends both of her legs on the ground.TM2T: The person lowers her thigh as she lays down on the floor while kneeling on the floor. The person extends both her arms as she moves her right elbow on the floor to support her body.MotionGPT: The person is standing straight at the living room … The human has both arms naturally hanging at her sides then she bends, extends and raises her right arm and throws the object on the living room with her right'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='arm and throws the object on the living room with her right hand. … The human has both feet fixed on the floor with both legs stretched upright then she slightly bends and spreads both of her legs widely apart.TM2T:The person is standing still in front of the cabinet while making a hanger. The person bends and raises her left hand then lays the hanger on her side of her chest then spreads both arms on her side below her chest. The person stands with both legs stretched upright and both feet fixed on the floor.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='MotionGPT:The person stands in front of the cabinet to remove the clothes from the hanger. the human raises both of his arms to remove a piece of clothing from the hanger. the human stands with both feet fixed on the floor.TM2T: the person stands up straight as she holds the pillow and place them on the table. the person then arrange the pillow in the middle of the room with her right hand and places it on the table, while her left arm is slightly bent in front as she holds and arrange the pillow in the direction of the table.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='Figure 6: Qualitative Results of Motion Understanding. We use green to highlight correct parts\\nand red for mistakes.\\nis highly related to environments. TM2T and MotionGPT fail to generate correct narrations for the\\nlack of vision signals. For our model, even though we do not directly use motions as inputs, EgoLM\\njointly model the distributions of different modalities and can generate correct narrations according\\nto different scenarios.\\n4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='Window Size of Motion Tracking. As shown in Tab. 3, we increase the window size for three-\\npoints motion tracking from 60to120frames, which brings an improvement of 4.2mm in joint\\nposition errors. This is reasonable since increasing the window size brings more contexts, which\\nhelps the disambiguation. If we further include egocentric videos in the inputs, the improvement\\nof increasing window size is not as large. Moreover, using 60frames plus egocentric video shows\\nbetter performance than only using 120frames. This indicates that the context of egocentric video'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='might be more effective than increasing window size.\\n9'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='Table 3: Ablation Study\\non Window Size for Motion\\nTracking.\\nWin Vid Full Upper Lower J.A.\\n60 83.88 54.06 148.37 13.31\\n120 79.61 52.66 138.87 13.01\\n60✓ 73.38 49.67 124.58 12.48\\n120✓ 72.76 49.20 123.09 12.52Table 4: Ablation Study on Recon-\\nstruction Results of Motion VQ-\\nV AE. [ mm]\\nPQ CB Dim MPJPE PA-MPJPE ACCEL\\n1 2048 512 51.60 37.55 1.09\\n2 2048 512 39.63 29.77 0.71\\n2 16384 256 39.13 29.78 1.08\\n2 16384 64 34.49 26.83 0.67Table 5: Ablation on\\nthe LM size. Medium:\\n345M; Large: 1.5B\\nGPT-2 Size Medium Large\\nBert↑ 18.38 19.56\\nBleu@1 ↑ 44.55 44.48\\nBleu@4 ↑ 12.12 12.49\\nRougeL ↑ 33.80 35.21\\nInput Prompt:The human leans forward and then turns right while walking towards the kitchen sink. The person holds and close the kitchen drawer with her left hand while the right arm rest beside her. The person bends her both legs and then steps backward.Input Prompt:The person walks toward the kitchen gas range and then grabs the fork while her left arm rest beside her. The person is walking forward to kitchen gas range with her both feet and then steps sideward with her right and left footrespectively.b) Motion Prediction Results\\na) Text-to-Motion Generation Results\\nFigure 7: More Analysis on EgoLM. a) Qualitative results of text-to-motion generation. b)Quali-\\ntative results of motion prediction.\\nMotion VQ-VAE. Ablation studies on motion VQ-V AE are reported in Tab. 4. “PQ” is the number\\nof codebooks. “CB” is the total number of entries in codebooks. The first two lines shows that\\nlarge improvements can be achieved by simply using product quantization. Moreover, increasing\\nthe number of codes and decreasing code dimensions bring further improvement.\\nLarger Language Model. We use GPT-2 Medium (345M) to conduct most of our experiments\\nfor efficiency. To examine the potential of using larger LM, we train with GPT-2 Large (1.5B) and\\nreport performance on TPV2T in Tab. 5. The improved scores suggest EgoLM’s scalability as a\\nversatile framework.\\n4.5 M ORE APPLICATIONS\\nText-to-Motion Generation. As part of our joint training, EgoLM is capable of generating motions\\nfrom texts, as shown in Fig. 7 a). Even with long prompts separately describing upper body and\\nlower body, our model is able to generate motions that match the inputs.\\nMotion Prediction. As a by-product of the motion pre-training, EgoLM can function as a motion\\npredictor. As shown in Fig. 7 b), given motion prompts (the red skeleton in the left), subsequent\\nmotions can be randomly sampled. We show three different samples in different colors.\\n5 D ISCUSSION\\nWe propose EgoLM, a multi-modal language model for egocentric motion tracking and understand-\\ning. A three-steps paradigm, including motion tokenization, motion pre-training and multi-modal\\ninstruction tuning, is proposed to facilitate the training. In contrast to previous works, the proposed\\nframework unifies the egocentric motion tasks with a language model, and incorporates multi-modal\\nsensor data as context information, which is proven effective for both tasks.\\nLimitations. Firstly, our motion tokenizer is a VQ-V AE, which carries reconstruction errors. It\\nsets an upper bound for motion tracking. Moreover, for the motion tracking training, the loss is\\ncalculated on discrete motion tokens, instead of raw motion representations, which might also harm\\nthe performance of motion tracking. Secondly, for motion understanding, since each egocentric\\nvideo frame is compressed by the CLIP encoder to a one-dimensional vector, it is hard for models\\nto precisely name the object that the person is interacting with. Moreover, as is commonly observed\\nin language models (Ji et al., 2023), EgoLM also suffers from the hallucination problem.\\nPotential Societal Impact. While contextual AI offers opportunities for efficiency improvement\\nand societal advancement, the collection and analysis of human data could lead to privacy issues for\\nboth users and people around.\\n10'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='Technical Report\\nTable 3: Ablation Study\\non Window Size for Motion\\nTracking.\\nWin Vid Full Upper Lower J.A.\\n60 83.88 54.06 148.37 13.31\\n120 79.61 52.66 138.87 13.01\\n60✓ 73.38 49.67 124.58 12.48\\n120✓ 72.76 49.20 123.09 12.52Table 4: Ablation Study on Recon-\\nstruction Results of Motion VQ-\\nV AE. [ mm]\\nPQ CB Dim MPJPE PA-MPJPE ACCEL\\n1 2048 512 51.60 37.55 1.09\\n2 2048 512 39.63 29.77 0.71\\n2 16384 256 39.13 29.78 1.08\\n2 16384 64 34.49 26.83 0.67Table 5: Ablation on\\nthe LM size. Medium:\\n345M; Large: 1.5B\\nGPT-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content=':The human leans forward and then turns right while walking towards the kitchen sink. The person holds and close the kitchen drawer with her left hand while the right arm rest beside her. The person bends her both legs and then steps backward.Input Prompt:The person walks toward the kitchen gas range and then grabs the fork while her left arm rest beside her. The person is walking forward to kitchen gas range with her both feet and then steps sideward with her right and left footrespectively.b) Motion Prediction Results\\na) Text-to-Motion Generation Results'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='a) Text-to-Motion Generation Results\\nFigure 7: More Analysis on EgoLM. a) Qualitative results of text-to-motion generation. b)Quali-\\ntative results of motion prediction.\\nMotion VQ-VAE. Ablation studies on motion VQ-V AE are reported in Tab. 4. “PQ” is the number\\nof codebooks. “CB” is the total number of entries in codebooks. The first two lines shows that\\nlarge improvements can be achieved by simply using product quantization. Moreover, increasing\\nthe number of codes and decreasing code dimensions bring further improvement.\\nLarger Language Model. We use GPT-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='(345M) to conduct most of our experiments\\nfor efficiency. To examine the potential of using larger LM, we train with GPT-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='(1.5B) and\\nreport performance on TPV2T in Tab. 5. The improved scores suggest EgoLM’s scalability as a\\nversatile framework.\\n4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='Text-to-Motion Generation. As part of our joint training, EgoLM is capable of generating motions\\nfrom texts, as shown in Fig.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='). Even with long prompts separately describing upper body and\\nlower body, our model is able to generate motions that match the inputs.\\nMotion Prediction. As a by-product of the motion pre-training, EgoLM can function as a motion\\npredictor. As shown in Fig.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='), given motion prompts (the red skeleton in the left), subsequent\\nmotions can be randomly sampled. We show three different samples in different colors.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='We propose EgoLM, a multi-modal language model for egocentric motion tracking and understand-\\ning. A three-steps paradigm, including motion tokenization, motion pre-training and multi-modal\\ninstruction tuning, is proposed to facilitate the training. In contrast to previous works, the proposed\\nframework unifies the egocentric motion tasks with a language model, and incorporates multi-modal\\nsensor data as context information, which is proven effective for both tasks.\\nLimitations. Firstly, our motion tokenizer is a VQ-V AE, which carries reconstruction errors. It'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='sets an upper bound for motion tracking. Moreover, for the motion tracking training, the loss is\\ncalculated on discrete motion tokens, instead of raw motion representations, which might also harm\\nthe performance of motion tracking. Secondly, for motion understanding, since each egocentric\\nvideo frame is compressed by the CLIP encoder to a one-dimensional vector, it is hard for models\\nto precisely name the object that the person is interacting with. Moreover, as is commonly observed\\nin language models (Ji et al., 2023), EgoLM also suffers from the hallucination problem.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='Potential Societal Impact. While contextual AI offers opportunities for efficiency improvement\\nand societal advancement, the collection and analysis of human data could lead to privacy issues for\\nboth users and people around.\\n10'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='REFERENCES\\nNov 2022. URL https://openai.com/blog/chatgpt .\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\\nman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical\\nreport. arXiv preprint arXiv:2303.08774 , 2023.\\nFederica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J.\\nBlack. Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image.\\nInComputer Vision – ECCV 2016 , Lecture Notes in Computer Science. Springer International\\nPublishing, October 2016.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.\\nAngela Castillo, Maria Escobar, Guillaume Jeanneret, Albert Pumarola, Pablo Arbel ´aez, Ali Thabet,\\nand Artsiom Sanakoyeu. Bodiffusion: Diffusing sparse observations for full-body human motion\\nsynthesis. arXiv preprint arXiv:2304.11118 , 2023.\\nYi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and\\nXihui Liu. Egoplan-bench: Benchmarking egocentric embodied planning with multimodal large\\nlanguage models. arXiv preprint arXiv:2312.06722 , 2023.\\nDima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos\\nKazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scal-\\ning egocentric vision: The epic-kitchens dataset. In European Conference on Computer Vision\\n(ECCV) , 2018.\\nDima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos\\nKazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. The\\nepic-kitchens dataset: Collection, challenges and baselines. IEEE Transactions on Pattern Anal-\\nysis and Machine Intelligence (TPAMI) , 43(11):4125–4141, 2021. doi: 10.1109/TPAMI.2020.\\n2991965.\\nDima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evange-\\nlos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray.\\nRescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. Interna-\\ntional Journal of Computer Vision (IJCV) , 130:33–55, 2022. URL https://doi.org/10.\\n1007/s11263-021-01531-2 .\\nKanakamedala Deepika, Veeranki Tilekya, Jatroth Mamatha, and T Subetha. Jollity chatbot-a con-\\ntextual ai assistant. In 2020 Third International Conference on Smart Systems and Inventive\\nTechnology (ICSSIT) , pp. 1196–1200. IEEE, 2020.\\nAna Garcia Del Molino, Cheston Tan, Joo-Hwee Lim, and Ah-Hwee Tan. Summarization of ego-\\ncentric videos: A comprehensive survey. IEEE Transactions on Human-Machine Systems , 47(1):\\n65–76, 2016.\\nPrafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever.\\nJukebox: A generative model for music. arXiv preprint arXiv:2005.00341 , 2020.\\nYuming Du, Robin Kips, Albert Pumarola, Sebastian Starke, Ali Thabet, and Artsiom Sanakoyeu.\\nAvatars grow legs: Generating smooth human motion from sparse tracking inputs with diffusion\\nmodel. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\\npp. 481–490, 2023.\\nMaria Escobar, Laura Daza, Cristina Gonz ´alez, Jordi Pont-Tuset, and Pablo Arbel ´aez. Video\\nswin transformers for egocentric video understanding@ ego4d challenges 2022. arXiv preprint\\narXiv:2207.11329 , 2022.\\n11'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='Technical Report\\nREFERENCES\\nNov 2022. URL https://openai.com/blog/chatgpt .\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\\nman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='report. arXiv preprint arXiv:2303.08774 , 2023.\\nFederica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J.\\nBlack. Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image.\\nInComputer Vision – ECCV 2016 , Lecture Notes in Computer Science. Springer International\\nPublishing, October 2016.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.\\nAngela Castillo, Maria Escobar, Guillaume Jeanneret, Albert Pumarola, Pablo Arbel ´aez, Ali Thabet,\\nand Artsiom Sanakoyeu. Bodiffusion: Diffusing sparse observations for full-body human motion\\nsynthesis. arXiv preprint arXiv:2304.11118 , 2023.\\nYi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and\\nXihui Liu. Egoplan-bench: Benchmarking egocentric embodied planning with multimodal large\\nlanguage models. arXiv preprint arXiv:2312.06722 , 2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='language models. arXiv preprint arXiv:2312.06722 , 2023.\\nDima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos\\nKazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scal-\\ning egocentric vision: The epic-kitchens dataset. In European Conference on Computer Vision\\n(ECCV) , 2018.\\nDima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos\\nKazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. The'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='epic-kitchens dataset: Collection, challenges and baselines. IEEE Transactions on Pattern Anal-\\nysis and Machine Intelligence (TPAMI) , 43(11):4125–4141, 2021. doi: 10.1109/TPAMI.2020.\\n2991965.\\nDima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evange-\\nlos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray.\\nRescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. Interna-\\ntional Journal of Computer Vision (IJCV) , 130:33–55, 2022. URL https://doi.org/10.\\n1007/s11263-021-01531-2 .'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='1007/s11263-021-01531-2 .\\nKanakamedala Deepika, Veeranki Tilekya, Jatroth Mamatha, and T Subetha. Jollity chatbot-a con-\\ntextual ai assistant. In 20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='Technology (ICSSIT) , pp. 1196–1200. IEEE, 2020.\\nAna Garcia Del Molino, Cheston Tan, Joo-Hwee Lim, and Ah-Hwee Tan. Summarization of ego-\\ncentric videos: A comprehensive survey. IEEE Transactions on Human-Machine Systems , 47(1):\\n65–76, 2016.\\nPrafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever.\\nJukebox: A generative model for music. arXiv preprint arXiv:2005.00341 , 2020.\\nYuming Du, Robin Kips, Albert Pumarola, Sebastian Starke, Ali Thabet, and Artsiom Sanakoyeu.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='Avatars grow legs: Generating smooth human motion from sparse tracking inputs with diffusion\\nmodel. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\\npp. 481–490, 2023.\\nMaria Escobar, Laura Daza, Cristina Gonz ´alez, Jordi Pont-Tuset, and Pablo Arbel ´aez. Video\\nswin transformers for egocentric video understanding@ ego4d challenges 2022. arXiv preprint\\narXiv:2207.11329 , 2022.\\n11'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Gird-\\nhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in\\n3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pp. 18995–19012, 2022.\\nChuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and\\nLi Cheng. Action2motion: Conditioned generation of 3d human motions. In Proceedings of the\\n28th ACM International Conference on Multimedia , pp. 2021–2029, 2020.\\nChuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating\\ndiverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pp. 5152–5161, June 2022a.\\nChuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for\\nthe reciprocal generation of 3d human motions and texts. In European Conference on Computer\\nVision , pp. 580–597. Springer, 2022b.\\nThorsten Hempel, Ahmed A Abdelrahman, and Ayoub Al-Hamadi. 6d rotation representation for\\nunconstrained head pose estimation. In 2022 IEEE International Conference on Image Processing\\n(ICIP) , pp. 2496–2500. IEEE, 2022.\\nCatalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3. 6m: Large scale\\ndatasets and predictive methods for 3d human sensing in natural environments. IEEE transactions\\non pattern analysis and machine intelligence , 36(7):1325–1339, 2013.\\nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor\\nsearch. IEEE transactions on pattern analysis and machine intelligence , 33(1):117–128, 2010.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM\\nComputing Surveys , 55(12):1–38, 2023.\\nBaoxiong Jia, Ting Lei, Song-Chun Zhu, and Siyuan Huang. Egotaskqa: Understanding human\\ntasks in egocentric videos. Advances in Neural Information Processing Systems , 35:3343–3360,\\n2022.\\nBiao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as\\na foreign language. Advances in Neural Information Processing Systems , 36, 2024.\\nJiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa Laich, Patrick Snape, and Christian\\nHolz. Avatarposer: Articulated full-body pose tracking from sparse motion sensing. In European\\nConference on Computer Vision , pp. 443–460. Springer, 2022.\\nJiaxi Jiang, Paul Streli, Manuel Meier, Andreas Fender, and Christian Holz. Egoposer: Robust\\nreal-time ego-body pose estimation in large scenes. arXiv preprint arXiv:2308.06493 , 2023.\\nAngjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of\\nhuman shape and pose. In Computer Vision and Pattern Recognition (CVPR) , 2018.\\nHildegard Kuehne, Hueihan Jhuang, Est ´ıbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a\\nlarge video database for human motion recognition. In 2011 International conference on computer\\nvision , pp. 2556–2563. IEEE, 2011.\\nJiaman Li, Karen Liu, and Jiajun Wu. Ego-body pose estimation via ego-head pose estimation.\\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.\\n17142–17151, 2023.\\nYin Li, Zhefan Ye, and James M Rehg. Delving into egocentric actions. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition , pp. 287–295, 2015.\\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\\nbranches out , pp. 74–81, 2004.\\n12'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='Technical Report\\nKristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Gird-\\nhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in\\n3,0'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pp. 18995–19012, 2022.\\nChuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and\\nLi Cheng. Action2motion: Conditioned generation of 3d human motions. In Proceedings of the\\n28th ACM International Conference on Multimedia , pp. 2021–2029, 2020.\\nChuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating\\ndiverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='Computer Vision and Pattern Recognition (CVPR) , pp. 5152–5161, June 2022a.\\nChuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for\\nthe reciprocal generation of 3d human motions and texts. In European Conference on Computer\\nVision , pp. 580–597. Springer, 2022b.\\nThorsten Hempel, Ahmed A Abdelrahman, and Ayoub Al-Hamadi. 6d rotation representation for\\nunconstrained head pose estimation. In 20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='(ICIP) , pp. 2496–2500. IEEE, 2022.\\nCatalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3. 6m: Large scale\\ndatasets and predictive methods for 3d human sensing in natural environments. IEEE transactions\\non pattern analysis and machine intelligence , 36(7):1325–1339, 2013.\\nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor\\nsearch. IEEE transactions on pattern analysis and machine intelligence , 33(1):117–128, 2010.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM\\nComputing Surveys , 55(12):1–38, 2023.\\nBaoxiong Jia, Ting Lei, Song-Chun Zhu, and Siyuan Huang. Egotaskqa: Understanding human\\ntasks in egocentric videos. Advances in Neural Information Processing Systems , 35:3343–3360,\\n2022.\\nBiao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as\\na foreign language. Advances in Neural Information Processing Systems , 36, 2024.\\nJiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa Laich, Patrick Snape, and Christian'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='Holz. Avatarposer: Articulated full-body pose tracking from sparse motion sensing. In European\\nConference on Computer Vision , pp. 443–460. Springer, 2022.\\nJiaxi Jiang, Paul Streli, Manuel Meier, Andreas Fender, and Christian Holz. Egoposer: Robust\\nreal-time ego-body pose estimation in large scenes. arXiv preprint arXiv:2308.06493 , 2023.\\nAngjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of\\nhuman shape and pose. In Computer Vision and Pattern Recognition (CVPR) , 2018.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='Hildegard Kuehne, Hueihan Jhuang, Est ´ıbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a\\nlarge video database for human motion recognition. In 20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='vision , pp. 2556–2563. IEEE, 2011.\\nJiaman Li, Karen Liu, and Jiajun Wu. Ego-body pose estimation via ego-head pose estimation.\\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.\\n17142–17151, 2023.\\nYin Li, Zhefan Ye, and James M Rehg. Delving into egocentric actions. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition , pp. 287–295, 2015.\\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\\nbranches out , pp. 74–81, 2004.\\n12'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\\ntuning, 2023a.\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS ,\\n2023b.\\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl:\\nA skinned multi-person linear model. In Seminal Graphics Papers: Pushing the Boundaries,\\nVolume 2 , pp. 851–866. 2023.\\nThomas Lucas, Fabien Baradel, Philippe Weinzaepfel, and Gr ´egory Rogez. Posegpt: Quantization-\\nbased 3d human motion generation and forecasting. In European Conference on Computer Vision ,\\npp. 417–435. Springer, 2022.\\nLingni Ma, Yuting Ye, Fangzhou Hong, Vladimir Guzov, Yifeng Jiang, Rowan Postyeni, Luis\\nPesqueira, Alexander Gamino, Vijay Baiyya, Hyo Jin Kim, et al. Nymeria: A massive collec-\\ntion of multimodal egocentric daily motion in the wild. arXiv preprint arXiv:2406.09905 , 2024.\\nJulieta Martinez, Rayat Hossain, Javier Romero, and James J. Little. A simple yet effective baseline\\nfor 3d human pose estimation. In ICCV , 2017.\\nNicholas Milef, Shinjiro Sueda, and N Khademi Kalantari. Variational pose prediction with dynamic\\nsample selection from sparse tracking signals. In Computer Graphics Forum , volume 42, pp. 359–\\n369. Wiley Online Library, 2023.\\nVimal Mollyn, Riku Arakawa, Mayank Goel, Chris Harrison, and Karan Ahuja. Imuposer: Full-\\nbody pose estimation using imus in phones, watches, and earbuds. In Proceedings of the 2023\\nCHI Conference on Human Factors in Computing Systems , pp. 1–12, 2023.\\nTushar Nagarajan, Santhosh Kumar Ramakrishnan, Ruta Desai, James Hillis, and Kristen Grauman.\\nEgoenv: Human-centric environment representations from egocentric video. Advances in Neural\\nInformation Processing Systems , 36, 2024.\\nAaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn-\\ning. arXiv preprint arXiv:1711.00937 , 2017.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\\nfor Computational Linguistics , pp. 311–318, 2002.\\nDario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3d human pose es-\\ntimation in video with temporal convolutions and semi-supervised training. In Conference on\\nComputer Vision and Pattern Recognition (CVPR) , 2019.\\nMathis Petrovich, Michael J Black, and G ¨ul Varol. Action-conditioned 3d human motion synthesis\\nwith transformer vae. In Proceedings of the IEEE/CVF International Conference on Computer\\nVision , pp. 10985–10995, 2021.\\nChiara Plizzari, Gabriele Goletto, Antonino Furnari, Siddhant Bansal, Francesco Ragusa, Gio-\\nvanni Maria Farinella, Dima Damen, and Tatiana Tommasi. An outlook into the future of egocen-\\ntric vision. arXiv preprint arXiv:2308.07123 , 2023.\\nJose Luis Ponton, Haoran Yun, Andreas Aristidou, Carlos Andujar, and Nuria Pelechano. Sparse-\\nposer: Real-time full-body motion reconstruction from sparse data. ACM Transactions on Graph-\\nics, 43(1):1–14, 2023.\\nAbhinanda R. Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez,\\nand Michael J. Black. BABEL: Bodies, action and behavior with english labels. In Proceedings\\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , pp. 722–731, June 2021.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\\nmodels are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\\n13'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='Technical Report\\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\\ntuning, 2023a.\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS ,\\n2023b.\\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl:\\nA skinned multi-person linear model. In Seminal Graphics Papers: Pushing the Boundaries,\\nVolume 2 , pp. 851–866. 2023.\\nThomas Lucas, Fabien Baradel, Philippe Weinzaepfel, and Gr ´egory Rogez. Posegpt: Quantization-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='based 3d human motion generation and forecasting. In European Conference on Computer Vision ,\\npp. 417–435. Springer, 2022.\\nLingni Ma, Yuting Ye, Fangzhou Hong, Vladimir Guzov, Yifeng Jiang, Rowan Postyeni, Luis\\nPesqueira, Alexander Gamino, Vijay Baiyya, Hyo Jin Kim, et al. Nymeria: A massive collec-\\ntion of multimodal egocentric daily motion in the wild. arXiv preprint arXiv:2406.09905 , 2024.\\nJulieta Martinez, Rayat Hossain, Javier Romero, and James J. Little. A simple yet effective baseline\\nfor 3d human pose estimation. In ICCV , 2017.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='for 3d human pose estimation. In ICCV , 2017.\\nNicholas Milef, Shinjiro Sueda, and N Khademi Kalantari. Variational pose prediction with dynamic\\nsample selection from sparse tracking signals. In Computer Graphics Forum , volume 42, pp. 359–\\n369. Wiley Online Library, 2023.\\nVimal Mollyn, Riku Arakawa, Mayank Goel, Chris Harrison, and Karan Ahuja. Imuposer: Full-\\nbody pose estimation using imus in phones, watches, and earbuds. In Proceedings of the 20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content=', pp. 1–12, 2023.\\nTushar Nagarajan, Santhosh Kumar Ramakrishnan, Ruta Desai, James Hillis, and Kristen Grauman.\\nEgoenv: Human-centric environment representations from egocentric video. Advances in Neural\\nInformation Processing Systems , 36, 2024.\\nAaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn-\\ning. arXiv preprint arXiv:1711.00937 , 2017.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='for Computational Linguistics , pp. 311–318, 2002.\\nDario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3d human pose es-\\ntimation in video with temporal convolutions and semi-supervised training. In Conference on\\nComputer Vision and Pattern Recognition (CVPR) , 2019.\\nMathis Petrovich, Michael J Black, and G ¨ul Varol. Action-conditioned 3d human motion synthesis\\nwith transformer vae. In Proceedings of the IEEE/CVF International Conference on Computer\\nVision , pp. 10985–10995, 2021.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='Vision , pp. 10985–10995, 2021.\\nChiara Plizzari, Gabriele Goletto, Antonino Furnari, Siddhant Bansal, Francesco Ragusa, Gio-\\nvanni Maria Farinella, Dima Damen, and Tatiana Tommasi. An outlook into the future of egocen-\\ntric vision. arXiv preprint arXiv:2308.07123 , 2023.\\nJose Luis Ponton, Haoran Yun, Andreas Aristidou, Carlos Andujar, and Nuria Pelechano. Sparse-\\nposer: Real-time full-body motion reconstruction from sparse data. ACM Transactions on Graph-\\nics, 43(1):1–14, 2023.\\nAbhinanda R. Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='and Michael J. Black. BABEL: Bodies, action and behavior with english labels. In Proceedings\\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , pp. 722–731, June 2021.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\\nmodels are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\\n13'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\\nmodels from natural language supervision. In International conference on machine learning , pp.\\n8748–8763. PMLR, 2021.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\\ntransformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.\\nIvan Rodin, Antonino Furnari, Dimitrios Mavroeidis, and Giovanni Maria Farinella. Predicting the\\nfuture from first person (egocentric) vision: A survey. Computer Vision and Image Understanding ,\\n211:103252, 2021.\\nDaniel Roetenberg, Henk Luinge, and Per Slycke. Xsens mvn: Full 6dof human motion tracking\\nusing miniature inertial sensors. Xsens Motion Technol. BV Tech. Rep. , 3, 01 2009.\\nKiran Somasundaram, Jing Dong, Huixuan Tang, Julian Straub, Mingfei Yan, Michael Goesele,\\nJakob Julian Engel, Renzo De Nardi, and Richard Newcombe. Project aria: A new tool for\\negocentric multi-modal ai research. arXiv preprint arXiv:2308.13561 , 2023.\\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions\\nclasses from videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.\\nShuhan Tan, Tushar Nagarajan, and Kristen Grauman. Egodistill: Egocentric head motion distilla-\\ntion for efficient video understanding. Advances in Neural Information Processing Systems , 36,\\n2024.\\nGuy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano.\\nHuman motion diffusion model. arXiv preprint arXiv:2209.14916 , 2022.\\nAlexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep neural net-\\nworks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\\n1653–1660, 2014.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee\\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\\nefficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023a.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023b.\\nDu Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spa-\\ntiotemporal features with 3d convolutional networks. In Proceedings of the IEEE International\\nConference on Computer Vision (ICCV) , December 2015.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\\ntion processing systems , 30, 2017.\\nTom Vercauteren, Mathias Unberath, Nicolas Padoy, and Nassir Navab. Cai4cai: the rise of contex-\\ntual artificial intelligence in computer-assisted interventions. Proceedings of the IEEE , 108(1):\\n198–214, 2019.\\nLimin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool.\\nTemporal segment networks: Towards good practices for deep action recognition. In European\\nconference on computer vision , pp. 20–36. Springer, 2016.\\nJilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, and Weidi Xie. Retrieval-\\naugmented egocentric video captioning. arXiv preprint arXiv:2401.00789 , 2024.\\nZihui Xue, Yale Song, Kristen Grauman, and Lorenzo Torresani. Egocentric video task translation.\\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.\\n2310–2320, 2023.\\n14'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='Technical Report\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\\nmodels from natural language supervision. In International conference on machine learning , pp.\\n8748–8763. PMLR, 2021.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\\ntransformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='Ivan Rodin, Antonino Furnari, Dimitrios Mavroeidis, and Giovanni Maria Farinella. Predicting the\\nfuture from first person (egocentric) vision: A survey. Computer Vision and Image Understanding ,\\n211:103252, 2021.\\nDaniel Roetenberg, Henk Luinge, and Per Slycke. Xsens mvn: Full 6dof human motion tracking\\nusing miniature inertial sensors. Xsens Motion Technol. BV Tech. Rep. , 3, 01 2009.\\nKiran Somasundaram, Jing Dong, Huixuan Tang, Julian Straub, Mingfei Yan, Michael Goesele,\\nJakob Julian Engel, Renzo De Nardi, and Richard Newcombe. Project aria: A new tool for'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='egocentric multi-modal ai research. arXiv preprint arXiv:2308.13561 , 2023.\\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 1'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='classes from videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.\\nShuhan Tan, Tushar Nagarajan, and Kristen Grauman. Egodistill: Egocentric head motion distilla-\\ntion for efficient video understanding. Advances in Neural Information Processing Systems , 36,\\n2024.\\nGuy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano.\\nHuman motion diffusion model. arXiv preprint arXiv:2209.14916 , 2022.\\nAlexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep neural net-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='works. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\\n1653–1660, 2014.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee\\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\\nefficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023a.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023b.\\nDu Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spa-\\ntiotemporal features with 3d convolutional networks. In Proceedings of the IEEE International\\nConference on Computer Vision (ICCV) , December 2015.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\\ntion processing systems , 30, 2017.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='tion processing systems , 30, 2017.\\nTom Vercauteren, Mathias Unberath, Nicolas Padoy, and Nassir Navab. Cai4cai: the rise of contex-\\ntual artificial intelligence in computer-assisted interventions. Proceedings of the IEEE , 108(1):\\n198–214, 2019.\\nLimin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool.\\nTemporal segment networks: Towards good practices for deep action recognition. In European\\nconference on computer vision , pp. 20–36. Springer, 2016.\\nJilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, and Weidi Xie. Retrieval-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='augmented egocentric video captioning. arXiv preprint arXiv:2401.00789 , 2024.\\nZihui Xue, Yale Song, Kristen Grauman, and Lorenzo Torresani. Egocentric video task translation.\\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.\\n2310–2320, 2023.\\n14'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 14}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 14}, page_content='Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for\\nskeleton-based action recognition. In Proceedings of the AAAI conference on artificial intelli-\\ngence , volume 32, 2018.\\nXinyu Yi, Yuxiao Zhou, Marc Habermann, Vladislav Golyanik, Shaohua Pan, Christian Theobalt,\\nand Feng Xu. Egolocate: Real-time motion capture, localization, and mapping with sparse body-\\nmounted sensors. arXiv preprint arXiv:2305.01599 , 2023.\\nRyo Yonetani, Kris M Kitani, and Yoichi Sato. Recognizing micro-actions and reactions from\\npaired egocentric videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pp. 2629–2638, 2016.\\nHang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language\\nmodel for video understanding. arXiv preprint arXiv:2306.02858 , 2023a.\\nJianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao,\\nHongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual descriptions with\\ndiscrete representations. arXiv preprint arXiv:2301.06052 , 2023b.\\nMingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei\\nLiu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE Transac-\\ntions on Pattern Analysis and Machine Intelligence , 2024.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluat-\\ning text generation with bert. arXiv preprint arXiv:1904.09675 , 2019.\\nYaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu,\\nand Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. arXiv\\npreprint arXiv:2306.10900 , 2023c.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\\nZixiang Zhou, Yu Wan, and Baoyuan Wang. Avatargpt: All-in-one framework for motion under-\\nstanding, planning, generation and beyond. arXiv preprint arXiv:2311.16468 , 2023.\\n15'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 14}, page_content='Technical Report\\nSijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for\\nskeleton-based action recognition. In Proceedings of the AAAI conference on artificial intelli-\\ngence , volume 32, 2018.\\nXinyu Yi, Yuxiao Zhou, Marc Habermann, Vladislav Golyanik, Shaohua Pan, Christian Theobalt,\\nand Feng Xu. Egolocate: Real-time motion capture, localization, and mapping with sparse body-\\nmounted sensors. arXiv preprint arXiv:2305.01599 , 2023.\\nRyo Yonetani, Kris M Kitani, and Yoichi Sato. Recognizing micro-actions and reactions from'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 14}, page_content='paired egocentric videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pp. 2629–2638, 2016.\\nHang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language\\nmodel for video understanding. arXiv preprint arXiv:2306.02858 , 2023a.\\nJianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao,\\nHongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual descriptions with\\ndiscrete representations. arXiv preprint arXiv:2301.06052 , 2023b.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 14}, page_content='Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei\\nLiu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE Transac-\\ntions on Pattern Analysis and Machine Intelligence , 2024.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluat-\\ning text generation with bert. arXiv preprint arXiv:1904.09675 , 2019.\\nYaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu,\\nand Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. arXiv'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 14}, page_content='preprint arXiv:2306.10900 , 2023c.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\\nZixiang Zhou, Yu Wan, and Baoyuan Wang. Avatargpt: All-in-one framework for motion under-\\nstanding, planning, generation and beyond. arXiv preprint arXiv:2311.16468 , 2023.\\n15'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='SUPPLEMENTARY\\nWe provide more implementation details (Sec. A) and qualitative results (Sec. B) in this supple-\\nmentary material. To better showcase our results, We also provide videos in our project page\\nhttps://hongfz16.github.io/projects/EgoLM .\\nEgoLM\\n𝑡−1𝑡1𝑡−1𝑡𝑡−2\\n𝑡−1𝑡1………EgoLM\\n𝑡𝑡+1𝑡−1𝑡𝑡+1𝑡−1𝑡−1𝑡……b) Auto-regressive motionInference for next step of 𝑡+1𝑡+1…a) InferenceInitializationMotion Tokens\\nFigure 8: Online Motion Tracking Inference. For the new time step of t+1with new data coming\\nin, last motion tokens are combined with the new input tokens to decode the next motion token t+1.\\nA I MPLEMENTATION DETAILS\\nA.1 A UTO-REGRESSIVE INFERENCE FOR MOTION TRACKING\\nAt inference time, motion understanding is the same as the language model inference. For motion\\ntracking, it usually requires online inference over a long period. With a language model, which is an\\nauto-regressive model, it is straight-forward to perform online motion tracking. As shown in Fig. 8,\\nfirstly, an initialization over the first tframes of data is required. When the new data frame t+ 1\\ncomes in, the input conditions are updated accordingly. Then, it is not necessary to predict all the\\nmotion tokens from frame 2to frame t+ 1. We take the previously generated motion tokens from\\nframe 2to frame tas inputs and prompt the network to generate one more token for frame t+ 1.\\nA.2 E VALUATION METRICS\\nFor motion tracking, we use joint position errors and joint angle errors to evaluate the performance.\\nSpecifically, for the joint position errors, we first align ground truth skeletons and generated skele-\\ntons by the head positions only by translation. Then full body, upper body and lower body joint po-\\nsition errors are calculated separately. Joint angle errors are calculated on full body and root joints.\\nFor the evaluation of motion VQ-V AE in main paper Tab. 4, we apply widely adopted metrics for\\nmotion regression, i.e., Mean Per-Joint Position Error (MPJPE) (Ionescu et al., 2013), Procrustes-\\nAligned (PA-)MPJPE (Kanazawa et al., 2018), and joint position acceleration (ACCL) error. For the\\nmotion understanding, we use standard NLP metrics, please kindly refer to corresponding papers\\nfor more details.\\nB M ORE QUALITATIVE RESULTS\\nB.1 T HREE -POINTS MOTION TRACKING\\nWe show four more visual examples of three-points motion tracking in Fig. 9, Fig. 10 and Fig. 11.\\nAvatarPoser (Jiang et al., 2022) and BoDiffusion (Castillo et al., 2023) are solid baselines that per-\\nform well on easy walking cases, e.g., upper example in Fig. 10. For the workout sequence, i.e.,\\nlower example in Fig. 11, even only given three points of upper body, the distribution of lower body\\nmotion can be collapsed and generate reasonable motions that matches the ground truth. In Fig. 11,\\nwe demonstrate the effectiveness of including egocentric videos as inputs. Without any environment\\ncontext, AvatarPoser and BoDiffusion often fail to distinguish standing and sitting down. We do not\\nassume the knowledge of the head height over the floor, meaning that the three-points positions are\\nnormalized to the local coordinates of the first frame. Therefore, it is hard for baseline methods to\\ndisambiguate certain scenarios. We propose to introduce contexts using egocentric videos, which\\ncontains rich information about the environment and how the person is interacting with it. There-\\n16'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='Technical Report\\nSUPPLEMENTARY\\nWe provide more implementation details (Sec. A) and qualitative results (Sec. B) in this supple-\\nmentary material. To better showcase our results, We also provide videos in our project page\\nhttps://hongfz16.github.io/projects/EgoLM .\\nEgoLM\\n𝑡−1𝑡1𝑡−1𝑡𝑡−2\\n𝑡−1𝑡1………EgoLM\\n𝑡𝑡+1𝑡−1𝑡𝑡+1𝑡−1𝑡−1𝑡……b) Auto-regressive motionInference for next step of 𝑡+1𝑡+1…a) InferenceInitializationMotion Tokens\\nFigure 8: Online Motion Tracking Inference. For the new time step of t+1with new data coming'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='in, last motion tokens are combined with the new input tokens to decode the next motion token t+1.\\nA I MPLEMENTATION DETAILS\\nA.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='-REGRESSIVE INFERENCE FOR MOTION TRACKING\\nAt inference time, motion understanding is the same as the language model inference. For motion\\ntracking, it usually requires online inference over a long period. With a language model, which is an\\nauto-regressive model, it is straight-forward to perform online motion tracking. As shown in Fig. 8,\\nfirstly, an initialization over the first tframes of data is required. When the new data frame t+'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content=', the input conditions are updated accordingly. Then, it is not necessary to predict all the\\nmotion tokens from frame 2to frame t+ 1. We take the previously generated motion tokens from\\nframe 2to frame tas inputs and prompt the network to generate one more token for frame t+ 1.\\nA.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='For motion tracking, we use joint position errors and joint angle errors to evaluate the performance.\\nSpecifically, for the joint position errors, we first align ground truth skeletons and generated skele-\\ntons by the head positions only by translation. Then full body, upper body and lower body joint po-\\nsition errors are calculated separately. Joint angle errors are calculated on full body and root joints.\\nFor the evaluation of motion VQ-V AE in main paper Tab. 4, we apply widely adopted metrics for'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='motion regression, i.e., Mean Per-Joint Position Error (MPJPE) (Ionescu et al., 2013), Procrustes-\\nAligned (PA-)MPJPE (Kanazawa et al., 2018), and joint position acceleration (ACCL) error. For the\\nmotion understanding, we use standard NLP metrics, please kindly refer to corresponding papers\\nfor more details.\\nB M ORE QUALITATIVE RESULTS\\nB.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='-POINTS MOTION TRACKING\\nWe show four more visual examples of three-points motion tracking in Fig. 9, Fig.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='. 11.\\nAvatarPoser (Jiang et al., 2022) and BoDiffusion (Castillo et al., 2023) are solid baselines that per-\\nform well on easy walking cases, e.g., upper example in Fig. 10. For the workout sequence, i.e.,\\nlower example in Fig. 11, even only given three points of upper body, the distribution of lower body\\nmotion can be collapsed and generate reasonable motions that matches the ground truth. In Fig. 11,\\nwe demonstrate the effectiveness of including egocentric videos as inputs. Without any environment'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='context, AvatarPoser and BoDiffusion often fail to distinguish standing and sitting down. We do not\\nassume the knowledge of the head height over the floor, meaning that the three-points positions are\\nnormalized to the local coordinates of the first frame. Therefore, it is hard for baseline methods to\\ndisambiguate certain scenarios. We propose to introduce contexts using egocentric videos, which\\ncontains rich information about the environment and how the person is interacting with it. There-\\n16'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 16}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 16}, page_content='AvatarPoserBoDiffusionOurs\\nGTEgocentricVideo0mm200mm\\nFigure 9: Qualitative Results of Three-Points Motion Tracking. Skeletons are color-coded by\\njoint position errors.\\nfore, our model can generate the most accurate motions by utilizing these information. For more\\nvisualization of three-points motion tracking, please kindly refer to our supplementary videos.\\n17'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 16}, page_content='Technical Report\\nAvatarPoserBoDiffusionOurs\\nGTEgocentricVideo0mm200mm\\nFigure 9: Qualitative Results of Three-Points Motion Tracking. Skeletons are color-coded by\\njoint position errors.\\nfore, our model can generate the most accurate motions by utilizing these information. For more\\nvisualization of three-points motion tracking, please kindly refer to our supplementary videos.\\n17'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 17}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 17}, page_content='AvatarPoser\\nBoDiffusion\\nOurs\\nGT\\nEgocentricVideo\\nAvatarPoser\\nBoDiffusion\\nOurs\\nGT\\nEgocentricVideo0mm200mm\\nFigure 10: Qualitative Results of Three-Points Motion Tracking. Skeletons are color-coded by\\njoint position errors.\\n18'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 17}, page_content='Technical Report\\nAvatarPoser\\nBoDiffusion\\nOurs\\nGT\\nEgocentricVideo\\nAvatarPoser\\nBoDiffusion\\nOurs\\nGT\\nEgocentricVideo0mm200mm\\nFigure 10: Qualitative Results of Three-Points Motion Tracking. Skeletons are color-coded by\\njoint position errors.\\n18'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 18}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 18}, page_content='AvatarPoser\\nBoDiffusion\\nOurs\\nGT\\nEgocentricVideo\\nAvatarPoser\\nBoDiffusion\\nOurs\\nGT\\nEgocentricVideo0mm200mm\\nFigure 11: Qualitative Results of Three-Points Motion Tracking. Skeletons are color-coded by\\njoint position errors.\\n19'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 18}, page_content='Technical Report\\nAvatarPoser\\nBoDiffusion\\nOurs\\nGT\\nEgocentricVideo\\nAvatarPoser\\nBoDiffusion\\nOurs\\nGT\\nEgocentricVideo0mm200mm\\nFigure 11: Qualitative Results of Three-Points Motion Tracking. Skeletons are color-coded by\\njoint position errors.\\n19'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 19}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 19}, page_content='EgoEgo\\nOurs\\nGT\\nEgocentricVideo\\nEgoEgo\\nOurs\\nGT\\nEgocentricVideo0mm200mm\\nFigure 12: Qualitative Results of One-Point Motion Tracking. Skeletons are color-coded by joint\\nposition errors.\\nB.2 O NE-POINT MOTION TRACKING\\nWe show four more examples of one-point motion tracking in Fig. 12 and Fig. 13. The introduction\\nof egocentric videos has two advantages. Firstly, similar to the case in three-points body tracking,\\nthe environment contexts in egocentric videos can disambiguate cases like standing and sitting. Sec-\\nondly, specifically for one-point motion tracking, egocentric videos provide clues of hand positions.\\nAs shown in all four examples, when the person raises the arms in front of the body, hands would be\\nvisible in the egocentric videos, which helps the hand position tracking. Admittedly, high-level se-\\nmantic information provided by CLIP (Radford et al., 2021) encoders cannot accurately track hand\\npositions. Therefore, as shown in the lower example in Fig. 12, our method correctly generates arms\\nmoving in the air, but lacks accuracy. For more visual examples of one-point motion tracking, please\\nkindly refer to our supplementary video.\\n20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 19}, page_content='Technical Report\\nEgoEgo\\nOurs\\nGT\\nEgocentricVideo\\nEgoEgo\\nOurs\\nGT\\nEgocentricVideo0mm200mm\\nFigure 12: Qualitative Results of One-Point Motion Tracking. Skeletons are color-coded by joint\\nposition errors.\\nB.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 19}, page_content='. 13. The introduction\\nof egocentric videos has two advantages. Firstly, similar to the case in three-points body tracking,\\nthe environment contexts in egocentric videos can disambiguate cases like standing and sitting. Sec-\\nondly, specifically for one-point motion tracking, egocentric videos provide clues of hand positions.\\nAs shown in all four examples, when the person raises the arms in front of the body, hands would be\\nvisible in the egocentric videos, which helps the hand position tracking. Admittedly, high-level se-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 19}, page_content='mantic information provided by CLIP (Radford et al., 2021) encoders cannot accurately track hand\\npositions. Therefore, as shown in the lower example in Fig. 12, our method correctly generates arms\\nmoving in the air, but lacks accuracy. For more visual examples of one-point motion tracking, please\\nkindly refer to our supplementary video.\\n20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 20}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 20}, page_content='EgoEgo\\nOurs\\nGT\\nEgocentricVideo\\nEgoEgo\\nOurs\\nGT\\nEgocentricVideo0mm200mm\\nFigure 13: Qualitative Results of One-Point Motion Tracking. Skeletons are color-coded by joint\\nposition errors.\\n21'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 20}, page_content='Technical Report\\nEgoEgo\\nOurs\\nGT\\nEgocentricVideo\\nEgoEgo\\nOurs\\nGT\\nEgocentricVideo0mm200mm\\nFigure 13: Qualitative Results of One-Point Motion Tracking. Skeletons are color-coded by joint\\nposition errors.\\n21'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 21}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 21}, page_content='Sample 1Sample 2Sample 3GTEgocentricVideo\\nFigure 14: Three Random Samples of One-Point Motion Tracking with Egocentric Videos as\\nInputs. Since we use language models as our backbone, EgoLM has the ability to randomly sample\\noutputs given the same inputs. Egocentric videos provide strong clues for hand positions, leading to\\nless diversity as shown in the highlighted areas.\\nB.2.1 M ULTIPLE SAMPLES .\\nNote that EgoLM is essentially a generative model. Therefore, our model is capable of generating\\ndifferent samples with the same inputs. In Fig. 14, we show three random samplings on the same\\ninput one-point and egocentric video. When hands are not visible in the frame, i.e., the left high-\\nlighted frame, hand positions are not constrained, and therefore shows high diversity across different\\nsamples. For the other highlighted frames, hands are visible in the egocentric videos, which helps to\\ncollapse the distribution of possible positions of hands. But as discussed above, our way of encoding\\negocentric videos cannot accurately track the hand positions. Therefore, our model also shows some\\ndiversity of hand positions in these cases.\\nTo further demonstrate the diversity of our model, we also show three random samples from our\\none-point motion tracking model that does not take egocentric videos as inputs in Fig. 15. Lack of\\nany indication of the hand positions, the upper body generation is even less constrained than that of\\nthe lower body and shows high diversity across three samples.\\n22'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 21}, page_content='Technical Report\\nSample 1Sample 2Sample 3GTEgocentricVideo\\nFigure 14: Three Random Samples of One-Point Motion Tracking with Egocentric Videos as\\nInputs. Since we use language models as our backbone, EgoLM has the ability to randomly sample\\noutputs given the same inputs. Egocentric videos provide strong clues for hand positions, leading to\\nless diversity as shown in the highlighted areas.\\nB.2.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 21}, page_content='.\\nNote that EgoLM is essentially a generative model. Therefore, our model is capable of generating\\ndifferent samples with the same inputs. In Fig. 14, we show three random samplings on the same\\ninput one-point and egocentric video. When hands are not visible in the frame, i.e., the left high-\\nlighted frame, hand positions are not constrained, and therefore shows high diversity across different\\nsamples. For the other highlighted frames, hands are visible in the egocentric videos, which helps to\\ncollapse the distribution of possible positions of hands. But as discussed above, our way of encoding'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 21}, page_content='egocentric videos cannot accurately track the hand positions. Therefore, our model also shows some\\ndiversity of hand positions in these cases.\\nTo further demonstrate the diversity of our model, we also show three random samples from our\\none-point motion tracking model that does not take egocentric videos as inputs in Fig. 15. Lack of\\nany indication of the hand positions, the upper body generation is even less constrained than that of\\nthe lower body and shows high diversity across three samples.\\n22'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 22}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 22}, page_content='Sample 1Sample 2Sample 3GT\\nFigure 15: Three Random Samples of One-Point Motion Tracking without Egocentric Videos\\nas Inputs. With only head poses as inputs, the generation of full body motion, especially upper\\nbody motions, is less constrained.\\n23'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 22}, page_content='Technical Report\\nSample 1Sample 2Sample 3GT\\nFigure 15: Three Random Samples of One-Point Motion Tracking without Egocentric Videos\\nas Inputs. With only head poses as inputs, the generation of full body motion, especially upper\\nbody motions, is less constrained.\\n23'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content=\"TM2T: The person is sitting on a chairand leaning backward on the table while talking to her peers. The person is resting both of her arms on the table, lifts and bends her left arm as she touches the table with her left hand. The person is sitting with both legs bent and with both feet flat on the floor widely apart.MotionGPT: The person is still sitting on the chair with a hunched backwhile playing arcade and eating some chips. The person's both arms are bent forward while holding and sliding the joystick with his left hand to the left then his right hand is on top of the buttons and clicks them with his right fingers. The person's both legs are still bent while sitting on the chair with both feet flat on the floor and slightly apart.V2T: The human is sittingon the sofa and leaning forward while arranging the chess pieces on the chessboard. The person has both of her arms extended forward while picking up the chess pieces with her left hand and puts down the chess piece with her right hand on the chess board. The human is sitting with both feet fixed on the floor and shoulder-width apart.Ours: The person is sitting in front of the checkerboard. The person is extending his right arm toward the checkerboard while keeping his left arm on top of his leg. The human is bending both of his knees while keeping both of his feet flat on the floor.GT:Thehumanis sitting in front of the table as he plays chess. The person is moving the knight with his right hand while his left hand remains resting on his leg. The human is bending both of his knees while keeping both of his feet flat on the floor.\\nTM2T: The person still marches in place while facing his peers. The person still swings both of his hands up and down. The person still marches in place with his left foot and right foot alternately. The person still repeatedly bends both of his legs alternately. The person still marches in place with his left foot and right foot alternately.MotionGPT: The human swings his body to the right and swings back to the left while standing, hunching his back and doing some exercise in the living area with his colleagues. The human slightly swings both of his arms back and forth on his side. The human raises his right leg to his waist level then stretches and lowers it while his left foot is fixed on the floor.V2T: The human is standing in the living room while watching the television. The person is resting both arms on his sides. The human has both feet fixed on the floor.Ours: The person is swaying her body side to side while exercising in the living area. The person repeatedly swings and bends both of her arms in front of her then lowers it down on her side. The person repeatedly raises both of her feet in front of her then lowers them down on the floor alternately.GT: The person is walking in place in front of the laptop. The human repeatedly bends both of her arm in front of her them lowers them down on her side. The human repeatedly steps both of her feet alternately.\\nTM2T: The person is still standing straightin front of the table while playing the board game with his peer. The person's both arms are still bentforward while both hands are still holding the edge of the knife.MotionGPT: The human still standsnear the closet. the human still holds the hanger with his left hand and his right hand holding the hanger. The person still stands with his feet slightly apart.V2T: The person is standing straight in the living areawith his colleagues while doing some exercise. The person raises both of his arms straight above his head from the back then lowers them in front and rests them on his side. The person is standing with both feet apart and fixed on the floor.Ours: The person is standing in the living area. The human repeatedly swings both of his arms in front of himand in front of his stomach. The person is standing with both feet fixed on the floor.GT:Thehumanisstandingstillinfrontofhiscolleagues in the living room while playing charades. The person is slightly raising and lowering both of his arms to gesticulate. The human is resting both of his legs fixedtothefloor.\\nTM2T: The person walks towards the cabinet then bends forward to pick up and reach for the clothes. The person extends his right arm to pick up the clothes from the cabinetthen bends his left arm to hold the clothes.MotionGPT: The person bends forwardwhile standing in the living room. The person extends her right arm to open the cabinet and extends her left arm to grab the keys on the right. The person slightly bends both of her legs then steps her right foot forward while her left foot is fixed on the floor.V2T: The human walks towards the couch and bends down while putting down the piece of clothing. The person extends both of her arms to pick up and put downthe piece of clothing with her right hand while holding the clothes with her left hand. The human steps both of her feet forward alternately.Ours:Thehumanwalkstowardsthesofathenslightlyleans forward to put down the folded piece of clothing. The person extends her right armto put down the folded piece of clothing on the sofa, then extends her left arm to pick upanother piece of clothing on the sofa. The human is stepping both of her feet forward alternately then bends both of her legs to support her body.GT:Thepersonbendshisbodytogetanotherclothesonthesofa.Thepersonextendshisrightarmtogettheclotheswithhis right hand then raises his left arm to hold the clothes with his left hand. The person steps both feet forward towards the sofa.\\nFigure 16: Qualitative Results of Motion Understanding. We use green to highlight correct parts\\nin the answers while red for wrong ones.\\nB.3 M OTION UNDERSTANDING\\nWe show eight more examples of motion understanding in Fig. 16 and Fig. 17. Similar to the\\nmain paper, we use green to highlight correct parts in the answers and red for wrong answers.\\nSimilar to the observation made in the main paper, even though TM2T (Guo et al., 2022b) and\\nMotionGPT (Jiang et al., 2024) have access to the full body motion, the generated narrations are\\nreasonable but completely wrong if consider the environment context. For example, in the upper\\nright example in Fig. 17, given the simple walking sequence, both TM2T and MotionGPT can\\ncorrectly understanding that the person is walking forward. But they all give the wrong answers\\nabout the places the person is walking in. Thanks to the egocentric videos, our model successfully\\nproduces the correct description as “walking towards the beds”.\\n24\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content=\"TM2T: The person is sitting on a chairand leaning backward on the table while talking to her peers. The person is resting both of her arms on the table, lifts and bends her left arm as she touches the table with her left hand. The person is sitting with both legs bent and with both feet flat on the floor widely apart.MotionGPT: The person is still sitting on the chair with a hunched backwhile playing arcade and eating some chips. The person's both arms are bent forward while holding and sliding the joystick with his left hand to the left then his right hand is on top of the buttons and clicks\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content=\"then his right hand is on top of the buttons and clicks them with his right fingers. The person's both legs are still bent while sitting on the chair with both feet flat on the floor and slightly apart.V2T: The human is sittingon the sofa and leaning forward while arranging the chess pieces on the chessboard. The person has both of her arms extended forward while picking up the chess pieces with her left hand and puts down the chess piece with her right hand on the chess board. The human is sitting with both feet fixed on the floor and shoulder-width apart.Ours: The person is sitting in front\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='shoulder-width apart.Ours: The person is sitting in front of the checkerboard. The person is extending his right arm toward the checkerboard while keeping his left arm on top of his leg. The human is bending both of his knees while keeping both of his feet flat on the floor.GT:Thehumanis sitting in front of the table as he plays chess. The person is moving the knight with his right hand while his left hand remains resting on his leg. The human is bending both of his knees while keeping both of his feet flat on the floor.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='TM2T: The person still marches in place while facing his peers. The person still swings both of his hands up and down. The person still marches in place with his left foot and right foot alternately. The person still repeatedly bends both of his legs alternately. The person still marches in place with his left foot and right foot alternately.MotionGPT: The human swings his body to the right and swings back to the left while standing, hunching his back and doing some exercise in the living area with his colleagues. The human slightly swings both of his arms back and forth on his side. The'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='swings both of his arms back and forth on his side. The human raises his right leg to his waist level then stretches and lowers it while his left foot is fixed on the floor.V2T: The human is standing in the living room while watching the television. The person is resting both arms on his sides. The human has both feet fixed on the floor.Ours: The person is swaying her body side to side while exercising in the living area. The person repeatedly swings and bends both of her arms in front of her then lowers it down on her side. The person repeatedly raises both of her feet in front of her then'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='repeatedly raises both of her feet in front of her then lowers them down on the floor alternately.GT: The person is walking in place in front of the laptop. The human repeatedly bends both of her arm in front of her them lowers them down on her side. The human repeatedly steps both of her feet alternately.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content=\"TM2T: The person is still standing straightin front of the table while playing the board game with his peer. The person's both arms are still bentforward while both hands are still holding the edge of the knife.MotionGPT: The human still standsnear the closet. the human still holds the hanger with his left hand and his right hand holding the hanger. The person still stands with his feet slightly apart.V2T: The person is standing straight in the living areawith his colleagues while doing some exercise. The person raises both of his arms straight above his head from the back then lowers them in\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='straight above his head from the back then lowers them in front and rests them on his side. The person is standing with both feet apart and fixed on the floor.Ours: The person is standing in the living area. The human repeatedly swings both of his arms in front of himand in front of his stomach. The person is standing with both feet fixed on the floor.GT:Thehumanisstandingstillinfrontofhiscolleagues in the living room while playing charades. The person is slightly raising and lowering both of his arms to gesticulate. The human is resting both of his legs fixedtothefloor.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='TM2T: The person walks towards the cabinet then bends forward to pick up and reach for the clothes. The person extends his right arm to pick up the clothes from the cabinetthen bends his left arm to hold the clothes.MotionGPT: The person bends forwardwhile standing in the living room. The person extends her right arm to open the cabinet and extends her left arm to grab the keys on the right. The person slightly bends both of her legs then steps her right foot forward while her left foot is fixed on the floor.V2T: The human walks towards the couch and bends down while putting down the piece of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='the couch and bends down while putting down the piece of clothing. The person extends both of her arms to pick up and put downthe piece of clothing with her right hand while holding the clothes with her left hand. The human steps both of her feet forward alternately.Ours:Thehumanwalkstowardsthesofathenslightlyleans forward to put down the folded piece of clothing. The person extends her right armto put down the folded piece of clothing on the sofa, then extends her left arm to pick upanother piece of clothing on the sofa. The human is stepping both of her feet forward alternately then bends'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='is stepping both of her feet forward alternately then bends both of her legs to support her body.GT:Thepersonbendshisbodytogetanotherclothesonthesofa.Thepersonextendshisrightarmtogettheclotheswithhis right hand then raises his left arm to hold the clothes with his left hand. The person steps both feet forward towards the sofa.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='Figure 16: Qualitative Results of Motion Understanding. We use green to highlight correct parts\\nin the answers while red for wrong ones.\\nB.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='. 17. Similar to the\\nmain paper, we use green to highlight correct parts in the answers and red for wrong answers.\\nSimilar to the observation made in the main paper, even though TM2T (Guo et al., 2022b) and\\nMotionGPT (Jiang et al., 2024) have access to the full body motion, the generated narrations are\\nreasonable but completely wrong if consider the environment context. For example, in the upper\\nright example in Fig. 17, given the simple walking sequence, both TM2T and MotionGPT can\\ncorrectly understanding that the person is walking forward. But they all give the wrong answers'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='about the places the person is walking in. Thanks to the egocentric videos, our model successfully\\nproduces the correct description as “walking towards the beds”.\\n24'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content=\"TM2T: The person walksout of the bedroom then turns to the leftto enter another bedroom. The person rests both her arms on her sides. the person is stepping forward with her right and left legs alternately. The person is stepping forward with her right and left legs alternately. The person then steps forward with her right feet.MotionGPT: The person walksin the garage. the person sways his hands on the sides. The human extends both legs forward alternately.V2T:  The human walks towards the bedroom. The human slightly sways her hands on her sides. The human takes four steps towards the bedroom.Ours: The human walks towards the door. The human puts down her right arm and sways both hands on the side. The person extends both legs forward alternately.GT:Thepersonwalkstowardsthedoor.The person walks towards the door. The person rests his left arm on the sideand he raises his right arm while holding the hanger with his right hand. The human extends both legs forward alternately.\\nTM2T: The person walkstowards the doorthen leans forward as he tucks in the chairand stands in front of the door to open it. The person's right arm is swinging back and forth on his side while his left arm is bent and his left hand holding the top railings then pushes the door open with his left hand.MotionGPT: The person is walkingforward towards the shower room, pauses on the shower room and then leans forward to put down the towelon the shower curtain holder. The person is bending both of his arms and then extends his left arm forward to put down the towelon the shower curtain holder. The person is alternately stepping both of his feet forward.V2T: The person straightens up as she slightly turns to the left while walking towards the closet. The person keeps holding the clothes with her bent left arm as she lowers down and slightly raises her right arm and then she bends it back. The person steps both of her feet forward alternately.Ours: The human turns clockwise as she walks towards the closet to put the clothes on the top shelf in the bedroom. The human is holding the clothes hanger with both of her bent arms in front of her then she extends her left arm froward and grabs the clothes hangerwith her left hand. The human turns her right foot to the right, steps her left foot forward then slightly moves her right foot forward.GT:Thehumanwalkstowards the closet. The human raises his left arm to grab the clothes while he holds the hanger with his right hand. The person extends both legs forward alternately.\\nTM2T: The person is standingin front of the door. the person is raising his left arm and is resting his right arm on his side. The person bends both of his legs while resting on the floor.MotionGPT: The person standsin the bedroomwhile talking to her colleague. The human is resting and bending her left arm in front while she lowers down her right hand before touching the wall with her right hand. The person stands with both feet fixed on the floor.V2T: The human is standingstraight while picking a condiment jar in the hanging cabinet. The human grabs a condiment jar with her right hand and flips up the other condiment jar in front of her with right hand and then she bends and slightly lowers down her right arm. The person is standing with both feet fixed on the ground.Ours: The person is standing in front of the hanging cabinet and slightly leaning forward while picking up a condiment jar. The person is extending her right hand forward, picks up the condiment jar cover then puts it down again on the top of the hanging cabinet while resting her left arm on her side. The human is standing with both of her legs parallel to each other and both of her feet spread slightly apart.GT:Thepersonisstanding on tiptoes while checking inside the cupboard. The human grabs and places the bottle down on the countertop with her right hand while her left hand is resting on the countertop. The human is standing on tiptoes with both feet as she reaches inside the cupboard.\\nTM2T: The person is walkingforward in the pathwaythen she slightly leans forward as she sitson the pathway. The person alternately swings both hands on her sides while both arms hang naturally at her sides.MotionGPT: The human is walkingforward while looking at the office surrounding. The human has her both arms swaying them back and forth. The human extends both legs forward alternately.V2T: The person is walking forward towards the bed. the person rests both arms on her sides. The person is extending both her legs forward alternately.Ours:The human is walking towards the bed. The person is resting both of her arms beside her. The person is extending both of her legs forward alternately.GT:Thepersonwalkstowardsthebed.Thepersonslightlyswingsbothofherarmsbackandforth.Thepersonstepsbothofher legs forward alternately.\\nFigure 17: Qualitative Results of Motion Understanding. We use green to highlight correct parts\\nin the answers while red for wrong ones.\\n25\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='TM2T: The person walksout of the bedroom then turns to the leftto enter another bedroom. The person rests both her arms on her sides. the person is stepping forward with her right and left legs alternately. The person is stepping forward with her right and left legs alternately. The person then steps forward with her right feet.MotionGPT: The person walksin the garage. the person sways his hands on the sides. The human extends both legs forward alternately.V2T:  The human walks towards the bedroom. The human slightly sways her hands on her sides. The human takes four steps towards the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='hands on her sides. The human takes four steps towards the bedroom.Ours: The human walks towards the door. The human puts down her right arm and sways both hands on the side. The person extends both legs forward alternately.GT:Thepersonwalkstowardsthedoor.The person walks towards the door. The person rests his left arm on the sideand he raises his right arm while holding the hanger with his right hand. The human extends both legs forward alternately.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content=\"TM2T: The person walkstowards the doorthen leans forward as he tucks in the chairand stands in front of the door to open it. The person's right arm is swinging back and forth on his side while his left arm is bent and his left hand holding the top railings then pushes the door open with his left hand.MotionGPT: The person is walkingforward towards the shower room, pauses on the shower room and then leans forward to put down the towelon the shower curtain holder. The person is bending both of his arms and then extends his left arm forward to put down the towelon the shower curtain holder. The\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='to put down the towelon the shower curtain holder. The person is alternately stepping both of his feet forward.V2T: The person straightens up as she slightly turns to the left while walking towards the closet. The person keeps holding the clothes with her bent left arm as she lowers down and slightly raises her right arm and then she bends it back. The person steps both of her feet forward alternately.Ours: The human turns clockwise as she walks towards the closet to put the clothes on the top shelf in the bedroom. The human is holding the clothes hanger with both of her bent arms in front of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='the clothes hanger with both of her bent arms in front of her then she extends her left arm froward and grabs the clothes hangerwith her left hand. The human turns her right foot to the right, steps her left foot forward then slightly moves her right foot forward.GT:Thehumanwalkstowards the closet. The human raises his left arm to grab the clothes while he holds the hanger with his right hand. The person extends both legs forward alternately.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='TM2T: The person is standingin front of the door. the person is raising his left arm and is resting his right arm on his side. The person bends both of his legs while resting on the floor.MotionGPT: The person standsin the bedroomwhile talking to her colleague. The human is resting and bending her left arm in front while she lowers down her right hand before touching the wall with her right hand. The person stands with both feet fixed on the floor.V2T: The human is standingstraight while picking a condiment jar in the hanging cabinet. The human grabs a condiment jar with her right hand and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='The human grabs a condiment jar with her right hand and flips up the other condiment jar in front of her with right hand and then she bends and slightly lowers down her right arm. The person is standing with both feet fixed on the ground.Ours: The person is standing in front of the hanging cabinet and slightly leaning forward while picking up a condiment jar. The person is extending her right hand forward, picks up the condiment jar cover then puts it down again on the top of the hanging cabinet while resting her left arm on her side. The human is standing with both of her legs parallel to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='The human is standing with both of her legs parallel to each other and both of her feet spread slightly apart.GT:Thepersonisstanding on tiptoes while checking inside the cupboard. The human grabs and places the bottle down on the countertop with her right hand while her left hand is resting on the countertop. The human is standing on tiptoes with both feet as she reaches inside the cupboard.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='TM2T: The person is walkingforward in the pathwaythen she slightly leans forward as she sitson the pathway. The person alternately swings both hands on her sides while both arms hang naturally at her sides.MotionGPT: The human is walkingforward while looking at the office surrounding. The human has her both arms swaying them back and forth. The human extends both legs forward alternately.V2T: The person is walking forward towards the bed. the person rests both arms on her sides. The person is extending both her legs forward alternately.Ours:The human is walking towards the bed. The person is'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='human is walking towards the bed. The person is resting both of her arms beside her. The person is extending both of her legs forward alternately.GT:Thepersonwalkstowardsthebed.Thepersonslightlyswingsbothofherarmsbackandforth.Thepersonstepsbothofher legs forward alternately.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='Figure 17: Qualitative Results of Motion Understanding. We use green to highlight correct parts\\nin the answers while red for wrong ones.\\n25'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 25}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 25}, page_content='Motion Prompt #1Temperature 0.6Motion Prompt #1Temperature 1.0Motion Prompt #1Temperature 1.4Motion Prompt #2Temperature 1.0Motion Prompt #3Temperature 1.0Motion Prompt #4Temperature 1.0\\nFigure 18: Qualitative Results of Motion Prediction. The first skeletons in red are input motion\\nprompts. The following motions are randomly sampled auto-regressively from our motion pre-\\ntraining network.\\nB.4 M OTION PREDICTION\\nAs a by-product of the second stage of our training pipeline, motion pre-training, we build a motion\\nprediction network. Given leading motions as the prompts, our model is capable of auto-regressively\\nsample motions that complete the motion prompts. As shown in Fig. 18, the first three samples\\nshow three different samples given the same motion prompt. We can increase the intensity of the\\ngenerated motions by increasing the temperature. The last three samples show three random samples\\ngiven various motion prompts, e.g., bending forward, sitting down and standing.\\n26'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 25}, page_content='Technical Report\\nMotion Prompt #1Temperature 0.6Motion Prompt #1Temperature 1.0Motion Prompt #1Temperature 1.4Motion Prompt #2Temperature 1.0Motion Prompt #3Temperature 1.0Motion Prompt #4Temperature 1.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 25}, page_content='18: Qualitative Results of Motion Prediction. The first skeletons in red are input motion\\nprompts. The following motions are randomly sampled auto-regressively from our motion pre-\\ntraining network.\\nB.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 25}, page_content='As a by-product of the second stage of our training pipeline, motion pre-training, we build a motion\\nprediction network. Given leading motions as the prompts, our model is capable of auto-regressively\\nsample motions that complete the motion prompts. As shown in Fig. 18, the first three samples\\nshow three different samples given the same motion prompt. We can increase the intensity of the\\ngenerated motions by increasing the temperature. The last three samples show three random samples\\ngiven various motion prompts, e.g., bending forward, sitting down and standing.\\n26'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='FlowTurbo: Towards Real-time Flow-Based Image'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='Generation with Velocity Refiner\\nWenliang Zhao∗Minglei Shi∗Xumin Yu Jie Zhou Jiwen Lu†\\nTsinghua University\\nAbstract\\nBuilding on the success of diffusion models in visual generation, flow-based models\\nreemerge as another prominent family of generative models that have achieved\\ncompetitive or better performance in terms of both visual quality and inference\\nspeed. By learning the velocity field through flow-matching, flow-based models\\ntend to produce a straighter sampling trajectory, which is advantageous during\\nthe sampling process. However, unlike diffusion models for which fast samplers\\nare well-developed, efficient sampling of flow-based generative models has been\\nrarely explored. In this paper, we propose a framework called FlowTurbo to\\naccelerate the sampling of flow-based models while still enhancing the sampling\\nquality. Our primary observation is that the velocity predictor’s outputs in the\\nflow-based models will become stable during the sampling, enabling the estimation\\nof velocity via a lightweight velocity refiner. Additionally, we introduce several\\ntechniques including a pseudo corrector and sample-aware compilation to further\\nreduce inference time. Since FlowTurbo does not change the multi-step sampling\\nparadigm, it can be effectively applied for various tasks such as image editing,\\ninpainting, etc. By integrating FlowTurbo into different flow-based models, we\\nobtain an acceleration ratio of 53.1% ∼58.3% on class-conditional generation and\\n29.8%∼38.5% on text-to-image generation. Notably, FlowTurbo reaches an FID\\nof 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img),\\nachieving the real-time image generation and establishing the new state-of-the-art.\\nCode is available at https://github.com/shiml20/FlowTurbo .\\n1 Introduction\\nIn recent years, diffusion models have emerged as powerful generative models, drawing considerable\\ninterest and demonstrating remarkable performance across various domains[ 10,38,30,12]. Diffusion\\nmodels utilize a denoising network, ϵθ, to learn the reverse of a diffusion process that gradually\\nadds noise to transform the data distribution into a Gaussian distribution. While the formulation of\\ndiffusion models enables stable training and flexible condition injection[ 30], sampling from these\\nmodels requires iterative denoising. This process necessitates multiple evaluations of the denoising\\nnetwork, thereby increasing computational costs. To address this, several techniques such as fast\\ndiffusion samplers[ 22,18,42] and efficient distillation[ 31,37] have been proposed to reduce the\\nsampling steps of diffusion models.\\nAlongside the research on diffusion models, flow-based models[ 5,19,17] have garnered increasing\\nattention due to their versatility in modeling data distributions. Flow is defined as a probability path\\nthat connects two distributions and can be efficiently modeled by learning a neural network to estimate\\nthe conditional velocity field through a neural network vθvia flow matching [ 17]. Encompassing\\n∗Equal contribution.†Corresponding author.\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2409.18128v1  [cs.CV]  26 Sep 2024'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='FlowTurbo: Towards Real-time Flow-Based Image\\nGeneration with Velocity Refiner\\nWenliang Zhao∗Minglei Shi∗Xumin Yu Jie Zhou Jiwen Lu†\\nTsinghua University\\nAbstract\\nBuilding on the success of diffusion models in visual generation, flow-based models\\nreemerge as another prominent family of generative models that have achieved\\ncompetitive or better performance in terms of both visual quality and inference\\nspeed. By learning the velocity field through flow-matching, flow-based models\\ntend to produce a straighter sampling trajectory, which is advantageous during'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='the sampling process. However, unlike diffusion models for which fast samplers\\nare well-developed, efficient sampling of flow-based generative models has been\\nrarely explored. In this paper, we propose a framework called FlowTurbo to\\naccelerate the sampling of flow-based models while still enhancing the sampling\\nquality. Our primary observation is that the velocity predictor’s outputs in the\\nflow-based models will become stable during the sampling, enabling the estimation\\nof velocity via a lightweight velocity refiner. Additionally, we introduce several'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='techniques including a pseudo corrector and sample-aware compilation to further\\nreduce inference time. Since FlowTurbo does not change the multi-step sampling\\nparadigm, it can be effectively applied for various tasks such as image editing,\\ninpainting, etc. By integrating FlowTurbo into different flow-based models, we\\nobtain an acceleration ratio of 53.1% ∼58.3% on class-conditional generation and\\n29.8%∼38.5% on text-to-image generation. Notably, FlowTurbo reaches an FID\\nof 2.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='38 (ms / img),\\nachieving the real-time image generation and establishing the new state-of-the-art.\\nCode is available at https://github.com/shiml20/FlowTurbo .'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='In recent years, diffusion models have emerged as powerful generative models, drawing considerable\\ninterest and demonstrating remarkable performance across various domains[ 10,38,30,12]. Diffusion\\nmodels utilize a denoising network, ϵθ, to learn the reverse of a diffusion process that gradually\\nadds noise to transform the data distribution into a Gaussian distribution. While the formulation of\\ndiffusion models enables stable training and flexible condition injection[ 30], sampling from these\\nmodels requires iterative denoising. This process necessitates multiple evaluations of the denoising'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='network, thereby increasing computational costs. To address this, several techniques such as fast\\ndiffusion samplers[ 22,18,42] and efficient distillation[ 31,37] have been proposed to reduce the\\nsampling steps of diffusion models.\\nAlongside the research on diffusion models, flow-based models[ 5,19,17] have garnered increasing\\nattention due to their versatility in modeling data distributions. Flow is defined as a probability path\\nthat connects two distributions and can be efficiently modeled by learning a neural network to estimate'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='the conditional velocity field through a neural network vθvia flow matching [ 17]. Encompassing\\n∗Equal contribution.†Corresponding author.\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2409.18128v'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='4 8 12 16 20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='Sampling Steps0.000.180.350.530.70Normalized Value of CurvatureSiT\\nDiT\\nSD3-Medium\\nFLUX.1-dev\\nOpen-Sora(a) Comparison of curvatures of different models.\\n4 8 12 16 200.000.010.020.03\\nSiT(b) SiT\\n4 8 12 16 200.000.010.020.03\\nSD3-Medium (c) SD3-Medium\\n4 8 12 16 200.000.010.020.03\\nFLUX.1-dev\\n(d) FLUX.1-dev\\n4 8 12 16 200.000.010.020.03\\nOpen-Sora (e) Open-Sora\\nFigure 1: Visualization of the curvatures of the sampling trajectories of different models. We\\ncompare the curvatures of the model predictions of a standard diffusion model (DiT [ 28]) and several\\nflow-based models (SiT [ 24], SD3-Medium [ 8], FLUX.1-dev [ 14], and Open-Sora [ 43]) during the\\nsampling. We observe that the vθin flow-based models is much more stable than ϵof diffusion\\nmodels during the sampling, which motivates us to seek a more lightweight estimation model to\\nreduce the sampling costs of flow-based generative models.\\nthe standard diffusion process as a special case, flow-based generative models support more flexible\\nchoices of probability paths. Recent work has favored a simple linear interpolant path[ 20,24,8],\\nwhich corresponds to the optimal transport from the Gaussian distribution to the data distribution.\\nThis linear connection between data and noise results in a more efficient sampling process for flow-\\nbased models. However, unlike diffusion models, which benefit from numerous efficient sampling\\nmethods, current samplers for flow-based models primarily rely on traditional numerical methods\\nsuch as Euler’s method and Heun’s method [ 24]. These traditional methods, while functional, fail to\\nfully exploit the unique properties of flow-based generative models, thereby limiting the potential for\\nfaster and more efficient sampling.\\nIn this paper, we propose FlowTurbo, a framework designed to accelerate the generation process\\nof flow-based generative models. FlowTurbo is motivated by comparing the training objectives of\\ndiffusion and flow-based generative models, as well as analyzing how the prediction results ϵθand\\nvθvary over time. Our observation, illustrated in Figure 1, indicates that the velocity predictions\\nof a flow-based model remain relatively stable during sampling, in contrast to the more variable\\npredictions of ϵθin diffusion models. This stability allows us to regress the offset of the velocity at\\neach sampling step using a lightweight velocity refiner, which contains only 5%of the parameters\\nof the original velocity prediction model. During the sampling process, we can replace the original\\nvelocity prediction model with our lightweight refiner at specific steps to reduce computational costs.\\nAs a step towards real-time image generation, we propose two useful techniques called pseudo\\ncorrector and sample-aware compilation to further improve the sampling speed. Specifically, the\\npseudo corrector method modifies the updating rule in Heun’s method by reusing the velocity\\nprediction of the previous sampling step, which will reduce the number of model evaluations at each\\nstep by half while keeping the original convergence order. The sample-aware compilation integrates\\nthe model evaluations, the sampling steps as well as the classifier-free guidance [ 11] together and\\ncompile them into a static graph, which can bring extra speedup compared with standard model-level\\ncompilation. Since each sample block is independent, we can still adjust the number of inference\\nsteps and sampling configurations flexibly.\\nOur FlowTurbo framework is fundamentally different from previous one-step distillation methods\\nfor diffusion models [ 20,40,32], which require generating millions of noise-image pairs offline and\\nconducting distillation over hundreds of GPU days. In contrast, FlowTurbo’s velocity refiner can be\\nefficiently trained on pure images in less than 6 hours. Moreover, one-step distillation-based methods\\nare limited to image generation and disable most of the functionalities of the original base model.\\nConversely, FlowTurbo preserves the multi-step sampling paradigm, allowing it to be effectively\\napplied to various tasks such as image editing, inpainting, and more.\\nWe perform extensive experiments to evaluate our method. By applying FlowTurbo to different\\nflow-based models, we obtain an acceleration ratio of 53.1% ∼58.3% on class-conditional generation\\n2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='0.000.180.350.530.70Normalized Value of CurvatureSiT\\nDiT\\nSD3-Medium\\nFLUX.1-dev\\nOpen-Sora(a) Comparison of curvatures of different models.\\n4 8 12 16 200.000.010.020.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='-Sora (e) Open-Sora\\nFigure 1: Visualization of the curvatures of the sampling trajectories of different models. We\\ncompare the curvatures of the model predictions of a standard diffusion model (DiT [ 28]) and several\\nflow-based models (SiT [ 24], SD3-Medium [ 8], FLUX.1-dev [ 14], and Open-Sora [ 43]) during the\\nsampling. We observe that the vθin flow-based models is much more stable than ϵof diffusion\\nmodels during the sampling, which motivates us to seek a more lightweight estimation model to\\nreduce the sampling costs of flow-based generative models.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='reduce the sampling costs of flow-based generative models.\\nthe standard diffusion process as a special case, flow-based generative models support more flexible\\nchoices of probability paths. Recent work has favored a simple linear interpolant path[ 20,24,8],\\nwhich corresponds to the optimal transport from the Gaussian distribution to the data distribution.\\nThis linear connection between data and noise results in a more efficient sampling process for flow-\\nbased models. However, unlike diffusion models, which benefit from numerous efficient sampling'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='methods, current samplers for flow-based models primarily rely on traditional numerical methods\\nsuch as Euler’s method and Heun’s method [ 24]. These traditional methods, while functional, fail to\\nfully exploit the unique properties of flow-based generative models, thereby limiting the potential for\\nfaster and more efficient sampling.\\nIn this paper, we propose FlowTurbo, a framework designed to accelerate the generation process\\nof flow-based generative models. FlowTurbo is motivated by comparing the training objectives of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='diffusion and flow-based generative models, as well as analyzing how the prediction results ϵθand\\nvθvary over time. Our observation, illustrated in Figure 1, indicates that the velocity predictions\\nof a flow-based model remain relatively stable during sampling, in contrast to the more variable\\npredictions of ϵθin diffusion models. This stability allows us to regress the offset of the velocity at\\neach sampling step using a lightweight velocity refiner, which contains only 5%of the parameters\\nof the original velocity prediction model. During the sampling process, we can replace the original'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='velocity prediction model with our lightweight refiner at specific steps to reduce computational costs.\\nAs a step towards real-time image generation, we propose two useful techniques called pseudo\\ncorrector and sample-aware compilation to further improve the sampling speed. Specifically, the\\npseudo corrector method modifies the updating rule in Heun’s method by reusing the velocity\\nprediction of the previous sampling step, which will reduce the number of model evaluations at each\\nstep by half while keeping the original convergence order. The sample-aware compilation integrates'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='the model evaluations, the sampling steps as well as the classifier-free guidance [ 11] together and\\ncompile them into a static graph, which can bring extra speedup compared with standard model-level\\ncompilation. Since each sample block is independent, we can still adjust the number of inference\\nsteps and sampling configurations flexibly.\\nOur FlowTurbo framework is fundamentally different from previous one-step distillation methods\\nfor diffusion models [ 20,40,32], which require generating millions of noise-image pairs offline and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='conducting distillation over hundreds of GPU days. In contrast, FlowTurbo’s velocity refiner can be\\nefficiently trained on pure images in less than'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='. Moreover, one-step distillation-based methods\\nare limited to image generation and disable most of the functionalities of the original base model.\\nConversely, FlowTurbo preserves the multi-step sampling paradigm, allowing it to be effectively\\napplied to various tasks such as image editing, inpainting, and more.\\nWe perform extensive experiments to evaluate our method. By applying FlowTurbo to different\\nflow-based models, we obtain an acceleration ratio of 53.1% ∼58.3% on class-conditional generation\\n2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='and 29.8% ∼38.5% on text-to-image generation. Notably, FlowTurbo attains an FID score of 2.12'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='on ImageNet with 100 (ms / img) and FID of score 3.93 with 38 (ms / img), thereby enabling real-\\ntime image generation and establishes the new state-of-the-art. Additionally, we present qualitative\\ncomparisons demonstrating how FlowTurbo generates superior images with higher throughput and\\nhow it can be seamlessly integrated into various applications such as image editing, inpainting, etc.\\nWe believe our FlowTurbo can serve as a general framework to accelerate flow-based generative\\nmodels and will see wider use as these models continue to grow [24, 20, 8, 9].\\n2 Related Work\\nDiffusion and flow-based models. Diffusion models [ 10,38] are a family of generative models that\\nhave become the de-facto method for high-quality generation. The diffusion process gradually adds\\nnoise to transform the data distribution to a normal distribution, and the goal of diffusion models is to\\nuse a network ϵθto learn the reverse of the diffusion process via score-matching [ 10,38]. Rombach et\\nal. [30] first scales up diffusion models to large-scale text-to-image generation by performing the\\ndiffusion on latent space and adopting cross-attention to inject conditions. The pre-trained diffusion\\nmodels can also be easily fine-tuned to achieve generation with more diverse conditions [ 41,27] and\\nhave attracted increasing attention in the community. Flow-based generative models are different from\\ndiffusion models in both data modeling and training objectives. Flow-based models [ 20,17,8,24]\\nconsider the probability path from one distribution to another, and learn the velocity field via flow\\nmatching [ 17]. By choosing the linear interpolant as the probability path which corresponds to the\\noptimal transport from the normal distribution to the data distribution, the trajectory from noise\\nto data becomes more straighter which is beneficial to the sampling. Recent work [ 24,8] have\\ndemonstrates the effectiveness and scalability of flow-based generation models. However, both\\ndiffusion and flow-based models requires multiple evaluations of the prediction model, leading to\\nlower inference speed than traditional architectures like GAN. In this work, we focus on this issue\\nand aim to accelerate flow-based generative models.\\nEfficient visual generation. Accelerating the generation of diffusion models has become an increas-\\ningly important topic. Existing methods can be roughly categorized as training-free and training-based\\nmethods. Training-free methods aim to design faster samplers that can reduce the approximation\\nerror when sampling from the diffusion SDE or ODE [ 36,22,18,42], while keeping the weights of\\ndiffusion models unchanged. Training-based methods often aim to reshape the sampling trajectory by\\ndistillation from the diffusion model [ 31,40] to achieve the few-step or even one-step generation.\\nThese training-based methods usually requires multiple-round of distillation [31, 20] and expensive\\ntraining resources ( e.g.,>100 GPU days in [ 20]). Besides, the distilled one-step model no longer\\nsupports image editing due to the lack of multi-step sampling. Although there are a variety of meth-\\nods for accelerating diffusion models, there are few fast sampling methods designed for flow-based\\ngenerative models. Existing flow-based models adopt traditional numerical methods like Euler’s\\nmethod or Heun’s method during the inference [ 24]. In this work, we provide a framework called\\nFlowTurbo to accelerate the generation of flow-based models by learning a lightweight velocity\\nrefiner (which only requires <6 GPU hours) to regress the offset of the velocity. Together with other\\nproposed techniques, FlowTurbo addresses the previously unmet need for an efficient flow-based\\ngeneration framework, paving the way for real-time generative applications.\\n3 Method\\n3.1 Preliminaries: Diffusion and Flow-based Models\\nDiffusion models. Recently, diffusion models [ 10,38,35,30] have emerged as a powerful family of\\ngenerative models. The diffusion models are trained to learn the inverse of a diffusion process such\\nthat it can recover the data distribution p0(x0)from the Gaussian noise. The diffusion process can be\\nrepresented as:\\nxt=αtx0+σtϵ, t∈[0,1],ϵ∼ N(0,I), (1)\\nwhere αt, σtare the chosen noise schedule such that the marginal distribution p1(x1)∼ N(0,I).\\nThe optimization of diffusion models can be derived by either minimizing the ELBO of the reverse\\nprocess [ 10] or solving the reverse diffusion SDE [ 38], which would both lead to the same training\\nobjective of score-matching, i.e., to learn a noise prediction model ϵθ(xt, t)to estimate the scaled\\n3'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='38 (ms / img), thereby enabling real-\\ntime image generation and establishes the new state-of-the-art. Additionally, we present qualitative\\ncomparisons demonstrating how FlowTurbo generates superior images with higher throughput and\\nhow it can be seamlessly integrated into various applications such as image editing, inpainting, etc.\\nWe believe our FlowTurbo can serve as a general framework to accelerate flow-based generative\\nmodels and will see wider use as these models continue to grow [24, 20, 8, 9].'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='Diffusion and flow-based models. Diffusion models [ 10,38] are a family of generative models that\\nhave become the de-facto method for high-quality generation. The diffusion process gradually adds\\nnoise to transform the data distribution to a normal distribution, and the goal of diffusion models is to\\nuse a network ϵθto learn the reverse of the diffusion process via score-matching [ 10,38]. Rombach et\\nal. [30] first scales up diffusion models to large-scale text-to-image generation by performing the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='diffusion on latent space and adopting cross-attention to inject conditions. The pre-trained diffusion\\nmodels can also be easily fine-tuned to achieve generation with more diverse conditions [ 41,27] and\\nhave attracted increasing attention in the community. Flow-based generative models are different from\\ndiffusion models in both data modeling and training objectives. Flow-based models [ 20,17,8,24]\\nconsider the probability path from one distribution to another, and learn the velocity field via flow'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='matching [ 17]. By choosing the linear interpolant as the probability path which corresponds to the\\noptimal transport from the normal distribution to the data distribution, the trajectory from noise\\nto data becomes more straighter which is beneficial to the sampling. Recent work [ 24,8] have\\ndemonstrates the effectiveness and scalability of flow-based generation models. However, both\\ndiffusion and flow-based models requires multiple evaluations of the prediction model, leading to\\nlower inference speed than traditional architectures like GAN. In this work, we focus on this issue'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='and aim to accelerate flow-based generative models.\\nEfficient visual generation. Accelerating the generation of diffusion models has become an increas-\\ningly important topic. Existing methods can be roughly categorized as training-free and training-based\\nmethods. Training-free methods aim to design faster samplers that can reduce the approximation\\nerror when sampling from the diffusion SDE or ODE [ 36,22,18,42], while keeping the weights of\\ndiffusion models unchanged. Training-based methods often aim to reshape the sampling trajectory by'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='distillation from the diffusion model [ 31,40] to achieve the few-step or even one-step generation.\\nThese training-based methods usually requires multiple-round of distillation [31, 20] and expensive\\ntraining resources ( e.g.,>1'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='[ 20]). Besides, the distilled one-step model no longer\\nsupports image editing due to the lack of multi-step sampling. Although there are a variety of meth-\\nods for accelerating diffusion models, there are few fast sampling methods designed for flow-based\\ngenerative models. Existing flow-based models adopt traditional numerical methods like Euler’s\\nmethod or Heun’s method during the inference [ 24]. In this work, we provide a framework called\\nFlowTurbo to accelerate the generation of flow-based models by learning a lightweight velocity\\nrefiner (which only requires <'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content=') to regress the offset of the velocity. Together with other\\nproposed techniques, FlowTurbo addresses the previously unmet need for an efficient flow-based\\ngeneration framework, paving the way for real-time generative applications.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content=': Diffusion and Flow-based Models\\nDiffusion models. Recently, diffusion models [ 10,38,35,30] have emerged as a powerful family of\\ngenerative models. The diffusion models are trained to learn the inverse of a diffusion process such\\nthat it can recover the data distribution p0(x0)from the Gaussian noise. The diffusion process can be\\nrepresented as:\\nxt=αtx0+σtϵ, t∈[0,1],ϵ∼ N(0,I), (1)\\nwhere αt, σtare the chosen noise schedule such that the marginal distribution p1(x1)∼ N(0,I).\\nThe optimization of diffusion models can be derived by either minimizing the ELBO of the reverse'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='process [ 10] or solving the reverse diffusion SDE [ 38], which would both lead to the same training\\nobjective of score-matching, i.e., to learn a noise prediction model ϵθ(xt, t)to estimate the scaled\\n3'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='score function −σt∇xlogpt(xt):'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='LDM(θ) =Et,p0(x0),p(xt|x0)h\\nλ(t)∥ϵθ(xt, t) +σt∇xlogpt(xt)∥2\\n2i\\n, (2)\\nwhere λ(t)is a time-dependent coefficient. Sampling from a diffusion model can be achieved by\\nsolving the reverse-time SDE or the corresponding diffusion ODEs [ 38], which can be efficiently\\nachieved by modern fast diffusion samplers [36, 22, 42].\\nFlow-based models. Flow-based models can be traced back to Continuous Normalizing Flows [ 5]\\n(CNF), which is a more generic modeling technique and can capture the probability paths of the\\ndiffusion process as well [ 17]. Training a CNF becomes more practical since the purpose of the flow\\nmatching technique [ 17], which learns the conditional velocity field of the flow. Similar to (1), we\\ncan add some constraints to the noise schedule such that α0= 1, σ0= 0andα1= 0, σ1= 1, and\\nthen define the flow as:\\nψt(·|ϵ) :x07→αtx0+σtϵ, (3)\\nIn this case, the velocity field that generates the flow ψtcan be represented as:\\nut(ψt(x0|ϵ)|ϵ) =d\\ndtψt(x0|ϵ) = ˙αtx0+ ˙σtϵ. (4)\\nThe training objective of conditional flow matching is to train a velocity prediction model vθto\\nestimate the conditional velocity field:\\nLFM(θ) =Et,p1(ϵ),p0(x0)\\r\\r\\r\\rvθ(ψt(x0|ϵ), t)−d\\ndtψt(x0|ϵ)\\r\\r\\r\\r2\\n2(5)\\nThe sampling of a flow-based model can be achieved by solving the probability flow ODE with the\\nlearned velocity\\ndxt\\ndt=vθ(xt, t),x1∼p1(x1). (6)\\nSince the formulation of the flow ψtcan be viewed as the interpolation between x0andv, it is also\\nreferred to as interpolant in some literature [ 1,24]. Among various types of interpolants, a very\\nsimple choice is linear interpolant [ 24,8], where αt= (1−t)andσt=t. In this case, the velocity\\nfield becomes a straight line connecting the initial noise and the data point, which also corresponds to\\nthe optimal transport between the two distributions [ 19,17]. The effectiveness and scalability of the\\nlinear interpolant have also been proven in recent work [17, 24, 8].\\n3.2 Efficient Estimation of Velocity\\nWe consider the velocity estimation in flow-based generative models with the linear interpolant [ 24,\\n8,20]. As shown in (5), the training target of the velocity prediction model vθis exactly ϵ−x0, a\\nconstant value independent of t. Our main motivation is to efficiently estimate the velocity during the\\nsampling, instead of evaluating the whole velocity prediction model vθevery time.\\nAnalyzing the stability of velocity. We start by analyzing the stability of the output value of vθ\\nalong the sampling trajectory. By comparing the training objectives of diffusion and flow-based\\nmodels (2)(5) , we know that the target of vθis independent of t. A more in-depth discussion is\\nprovided in Appendix A.3, where we show the two training objectives have different time-dependent\\nweight functions. To verify whether there are similar patterns during the sampling, we compare how\\nthe prediction results change across the sampling steps in Figure 1. Specifically, we compare the\\ncurvatures of ϵθof a diffusion model (DiT [ 28]) and the vθof flow-based models (SiT [ 8], SD3 [ 8],\\netc) during the sampling steps. For each model, we sample from 8 random noises and set the total\\nsampling steps as 20. It can be clearly observed that vθof a flow-based model is much more stable\\nthan the ϵθof a diffusion model. Therefore, We define the vθas a“stable value” . The stability of vθ\\nmakes it possible to obtain the velocity more efficiently rather than performing the forward pass of\\nthe whole velocity prediction network vθat every sampling step.\\nLearning a lightweight velocity refiner. Since the velocity in a flow-based model is a “stable value”,\\nwe propose to learn a lightweight refiner that can adjust the velocity with minimal computational\\ncosts. The velocity refiner takes as inputs both the current intermediate result and the velocity of the\\nprevious step, and returns the offset of velocity:\\nvti=rϕ(xti,vti−1, ti) +vti−1. (7)\\n4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='score function −σt∇xlogpt(xt):\\nLDM(θ) =Et,p0(x0),p(xt|x0)h\\nλ(t)∥ϵθ(xt, t) +σt∇xlogpt(xt)∥2\\n2i\\n, (2)\\nwhere λ(t)is a time-dependent coefficient. Sampling from a diffusion model can be achieved by\\nsolving the reverse-time SDE or the corresponding diffusion ODEs [ 38], which can be efficiently\\nachieved by modern fast diffusion samplers [36, 22, 42].\\nFlow-based models. Flow-based models can be traced back to Continuous Normalizing Flows [ 5]\\n(CNF), which is a more generic modeling technique and can capture the probability paths of the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='diffusion process as well [ 17]. Training a CNF becomes more practical since the purpose of the flow\\nmatching technique [ 17], which learns the conditional velocity field of the flow. Similar to (1), we\\ncan add some constraints to the noise schedule such that α0= 1, σ0= 0andα1= 0, σ1= 1, and\\nthen define the flow as:\\nψt(·|ϵ) :x07→αtx0+σtϵ, (3)\\nIn this case, the velocity field that generates the flow ψtcan be represented as:\\nut(ψt(x0|ϵ)|ϵ) =d\\ndtψt(x0|ϵ) = ˙αtx0+ ˙σtϵ. (4)\\nThe training objective of conditional flow matching is to train a velocity prediction model vθto'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='estimate the conditional velocity field:\\nLFM(θ) =Et,p1(ϵ),p0(x0)\\r\\r\\r\\rvθ(ψt(x0|ϵ), t)−d\\ndtψt(x0|ϵ)\\r\\r\\r\\r2\\n2(5)\\nThe sampling of a flow-based model can be achieved by solving the probability flow ODE with the\\nlearned velocity\\ndxt\\ndt=vθ(xt, t),x1∼p1(x1). (6)\\nSince the formulation of the flow ψtcan be viewed as the interpolation between x0andv, it is also\\nreferred to as interpolant in some literature [ 1,24]. Among various types of interpolants, a very\\nsimple choice is linear interpolant [ 24,8], where αt= (1−t)andσt=t. In this case, the velocity'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='field becomes a straight line connecting the initial noise and the data point, which also corresponds to\\nthe optimal transport between the two distributions [ 19,17]. The effectiveness and scalability of the\\nlinear interpolant have also been proven in recent work [17, 24, 8].\\n3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='We consider the velocity estimation in flow-based generative models with the linear interpolant [ 24,\\n8,20]. As shown in (5), the training target of the velocity prediction model vθis exactly ϵ−x0, a\\nconstant value independent of t. Our main motivation is to efficiently estimate the velocity during the\\nsampling, instead of evaluating the whole velocity prediction model vθevery time.\\nAnalyzing the stability of velocity. We start by analyzing the stability of the output value of vθ\\nalong the sampling trajectory. By comparing the training objectives of diffusion and flow-based'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='models (2)(5) , we know that the target of vθis independent of t. A more in-depth discussion is\\nprovided in Appendix A.3, where we show the two training objectives have different time-dependent\\nweight functions. To verify whether there are similar patterns during the sampling, we compare how\\nthe prediction results change across the sampling steps in Figure 1. Specifically, we compare the\\ncurvatures of ϵθof a diffusion model (DiT [ 28]) and the vθof flow-based models (SiT [ 8], SD3 [ 8],\\netc) during the sampling steps. For each model, we sample from'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='sampling steps as 20. It can be clearly observed that vθof a flow-based model is much more stable\\nthan the ϵθof a diffusion model. Therefore, We define the vθas a“stable value” . The stability of vθ\\nmakes it possible to obtain the velocity more efficiently rather than performing the forward pass of\\nthe whole velocity prediction network vθat every sampling step.\\nLearning a lightweight velocity refiner. Since the velocity in a flow-based model is a “stable value”,\\nwe propose to learn a lightweight refiner that can adjust the velocity with minimal computational'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='costs. The velocity refiner takes as inputs both the current intermediate result and the velocity of the\\nprevious step, and returns the offset of velocity:\\nvti=rϕ(xti,vti−1, ti) +vti−1. (7)\\n4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='+𝐱𝑡𝑖−1=𝜓𝑡𝑖−1𝐱0𝝐=1−𝑡𝑖−1𝐱0+𝑡𝑖−1𝝐 𝐱0∼𝑝0𝐱,𝝐∼𝒩(0,𝐈)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='𝐱𝑡𝑖−1 Velocity \\nPredictor 𝐯𝜃𝐯𝑡𝑖−1\\n×Δ𝑡𝐱𝑡𝑖Velocity \\nRefiner 𝐫𝜙𝐯𝑡𝑖−1\\u0de4𝐯𝑡𝑖+\\nℒ𝜙=𝔼\\u0de4𝐯𝑡𝑖−𝐯𝜃(𝐱𝑡𝑖,𝑡𝑖)22×Δ𝑡\\n+𝐱𝑡𝑖−1 Velocity \\nPredictor 𝐯𝜃𝐯𝑡𝑖−1\\n×Δ𝑡𝐱𝑡𝑖\\nVelocity \\nPredictor 𝐯𝜃 ത𝐱𝑡𝑖ത𝐯𝑡𝑖+×0.5+(b)Heun’s Method Sample Block (a)Learning a Lightweight V elocity Refiner\\n×Δ𝑡\\n+𝐱𝑡𝑖−1 Velocity \\nCacheത𝐯𝑡𝑖−1\\n×Δ𝑡𝐱𝑡𝑖\\nVelocity \\nPredictor 𝐯𝜃 ത𝐱𝑡𝑖ത𝐯𝑡𝑖+×0.5+\\n𝐱𝑡0∼𝒩(0,𝐈)(c)Pseudo Corrector Sample Block (d)FlowTurbo Sampling\\n𝐱𝑡0\\nSACHeun’s Method \\nSample Block\\nSACPseudo Corrector\\nSample Block\\nSACVelocity Refiner\\nSample block𝐱𝑡𝑁\\nSAC: Sample -A ware Compilation×𝑁𝐻 ×𝑁𝑃 ×𝑁𝑅𝑁=𝑁𝐻+𝑁𝑃+𝑁𝑅Figure 2: Overview of FlowTurbo. (a) Motivated by the stability of the velocity predictor’s outputs during the\\nsampling, we propose to learn a lightweight velocity refiner to regress the offset of the velocity field. (b)(c) We\\npropose the pseudo corrector which leverages a velocity cache to reduce the number of model evaluations while\\nmaintaining the same convergence order as Heun’s method. (d)During sampling, we employ a combination of\\nHeun’s method, the pseudo corrector, and the velocity refiner, where each sample block is processed with the\\nproposed sample-aware compilation.\\nThe velocity refiner rϕcan be designed to be very lightweight ( <5%parameters of vθ). The detailed\\narchitecture can be found in Appendix C.\\nTo learn the velocity refiner, we need to minimize the difference between the output of rϕand the\\nactual offset vti−vti−1. However, it requires multiple-step sampling to obtain an intermediate result\\nxtito make the training objective perfectly align with our target. To reduce the training cost, we\\nsimulate the xtwith one-step sampling starting from xti−1, which is directly obtained by the flow\\nψti−1. The detailed procedure to compute the loss is listed as follows:\\nxti−1←ψti−1(x0|ϵ),x0∼p0(x),ϵ∼p1(x) (8)\\nvti−1←vθ(xti−1, ti−1),xti←Solver( xti−1,vti−1,∆t) (9)\\nLϕ←E∥vθ(xti, ti)−(rϕ(xti,vti−1, ti) +vti−1)∥2\\n2 (10)\\nWhere ∆t=ti−ti−1and we use a simple Euler step for the Solver to obtain xti. Once the velocity\\nrefiner is learned, we can use it to replace the original vθat some specific sampling steps. We will\\ndemonstrate through experiments that adding the velocity refiner can improve the sampling quality\\nwithout introducing noticeable computational overhead.\\nCompatibility with classifier-free guidance. Classifier-free guidance [ 11] is a useful technique\\nto improve the sampling quality in conditional sampling. Let ybe the condition, the classifier-free\\nguidance for a velocity prediction model [8] can be defined as:\\nvζ(x, t|y) = (1 −ζ)vθ(x, t|∅) +ζvθ(x, t|y), (11)\\nwhere ζis the guidance scale and ∅denotes the null condition. To make our velocity refiner support\\nclassifier-free guidance, we only need to make sure both the conditional prediction vθ(x, t|y)and the\\nunconditional prediction vθ(x, t|∅)appear during the training. Note that we always feed the velocity\\nprediction model vθand the velocity refiner rϕwith the same condition.\\nyγ=Iγ≤γ1·∅+Iγ>γ 1·y, γ∈ U[0,1], (12)\\nLCFG\\nϕ←E∥vθ(xti, ti|yγ)−(rϕ(xti,vti−1, ti|yγ) +vti−1)∥2\\n2, (13)\\nwhere we set the γ1= 0.1as the probability of using an unconditional velocity.\\n3.3 Towards Real-Time Image Generation\\nThe sampling costs of a flow-based model can be significantly minimized by integrating our\\nlightweight velocity refiner rϕin place of the velocity prediction network vθat selected sampling\\n5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='Predictor 𝐯𝜃𝐯𝑡𝑖−1\\n×Δ𝑡𝐱𝑡𝑖\\nVelocity \\nPredictor 𝐯𝜃 ത𝐱𝑡𝑖ത𝐯𝑡𝑖+×0.5+(b)Heun’s Method Sample Block (a)Learning a Lightweight V elocity Refiner\\n×Δ𝑡\\n+𝐱𝑡𝑖−'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='Cacheത𝐯𝑡𝑖−1\\n×Δ𝑡𝐱𝑡𝑖\\nVelocity \\nPredictor 𝐯𝜃 ത𝐱𝑡𝑖ത𝐯𝑡𝑖+×0.5+\\n𝐱𝑡0∼𝒩(0,𝐈)(c)Pseudo Corrector Sample Block (d)FlowTurbo Sampling\\n𝐱𝑡'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='’s Method \\nSample Block\\nSACPseudo Corrector\\nSample Block\\nSACVelocity Refiner\\nSample block𝐱𝑡𝑁\\nSAC: Sample -A ware Compilation×𝑁𝐻 ×𝑁𝑃 ×𝑁𝑅𝑁=𝑁𝐻+𝑁𝑃+𝑁𝑅Figure 2: Overview of FlowTurbo. (a) Motivated by the stability of the velocity predictor’s outputs during the\\nsampling, we propose to learn a lightweight velocity refiner to regress the offset of the velocity field. (b)(c) We\\npropose the pseudo corrector which leverages a velocity cache to reduce the number of model evaluations while\\nmaintaining the same convergence order as Heun’s method. (d)During sampling, we employ a combination of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='Heun’s method, the pseudo corrector, and the velocity refiner, where each sample block is processed with the\\nproposed sample-aware compilation.\\nThe velocity refiner rϕcan be designed to be very lightweight ( <5%parameters of vθ). The detailed\\narchitecture can be found in Appendix C.\\nTo learn the velocity refiner, we need to minimize the difference between the output of rϕand the\\nactual offset vti−vti−1. However, it requires multiple-step sampling to obtain an intermediate result\\nxtito make the training objective perfectly align with our target. To reduce the training cost, we'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='simulate the xtwith one-step sampling starting from xti−1, which is directly obtained by the flow\\nψti−1. The detailed procedure to compute the loss is listed as follows:\\nxti−1←ψti−1(x0|ϵ),x0∼p0(x),ϵ∼p1(x) (8)\\nvti−1←vθ(xti−1, ti−1),xti←Solver( xti−1,vti−1,∆t) (9)\\nLϕ←E∥vθ(xti, ti)−(rϕ(xti,vti−1, ti) +vti−1)∥2\\n2 (10)\\nWhere ∆t=ti−ti−1and we use a simple Euler step for the Solver to obtain xti. Once the velocity\\nrefiner is learned, we can use it to replace the original vθat some specific sampling steps. We will'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='demonstrate through experiments that adding the velocity refiner can improve the sampling quality\\nwithout introducing noticeable computational overhead.\\nCompatibility with classifier-free guidance. Classifier-free guidance [ 11] is a useful technique\\nto improve the sampling quality in conditional sampling. Let ybe the condition, the classifier-free\\nguidance for a velocity prediction model [8] can be defined as:\\nvζ(x, t|y) = (1 −ζ)vθ(x, t|∅) +ζvθ(x, t|y), (11)\\nwhere ζis the guidance scale and ∅denotes the null condition. To make our velocity refiner support'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='classifier-free guidance, we only need to make sure both the conditional prediction vθ(x, t|y)and the\\nunconditional prediction vθ(x, t|∅)appear during the training. Note that we always feed the velocity\\nprediction model vθand the velocity refiner rϕwith the same condition.\\nyγ=Iγ≤γ1·∅+Iγ>γ 1·y, γ∈ U[0,1], (12)\\nLCFG\\nϕ←E∥vθ(xti, ti|yγ)−(rϕ(xti,vti−1, ti|yγ) +vti−1)∥2\\n2, (13)\\nwhere we set the γ1= 0.1as the probability of using an unconditional velocity.\\n3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='-Time Image Generation\\nThe sampling costs of a flow-based model can be significantly minimized by integrating our\\nlightweight velocity refiner rϕin place of the velocity prediction network vθat selected sampling\\n5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='steps. In this section, we propose two techniques to further improve the sampling speed towards'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='real-time image generation.\\nPseudo corrector. Traditional numerical ODE solvers are usually used to sample from a probability\\nflow ODE. For example, SiT [ 8] adopt a Heun method (or improved Euler’s method) [ 15] as the ODE\\nsolver. The update rule from ti−1totican be written as:\\ndi−1←vθ(xti−1, ti−1|y), ˜xti←xti−1+ ∆tdi−1 (14)\\ndi←vθ(˜xti, ti|y), xi←xi−1+∆t\\n2[di−1+di] (15)\\nEach Heun step contains a predictor step (14) and a corrector step (15), thus includes two evaluations\\nof the velocity predictor vθ, bringing extra inference costs. Motivated by [ 42], we propose to reuse\\nthediin the next sampling step, instead of re-computing it via di←vθ(xti, ti|y)(see Figure 2\\n(b)(c) for illustration). We call this a pseudo corrector since it is different from the predictor-corrector\\nsolvers in numerical analysis. It can be proved (see Appendix B) that the pseudo corrector also enjoys\\n2-order convergence while only having one model evaluation at each step.\\nSample-aware compilation. Compiling the network into a static graph is a widely used technique\\nfor model acceleration. However, all the previous work only considers network-level compilation,\\ni.e., only compiling the ϵθorvθ. We propose the sample-aware compilation which wraps both the\\nforward pass of vθorrϕand the sampling operation together (including the classifier-free guidance)\\nand performs the compilation. For example, the sample blocks illustrated in Figure 2 (b, c) are\\ncompiled into static graphs. Since each sample block is independent, we can still adjust the number\\nof inference steps and sampling configurations flexibly.\\n3.4 Discussion\\nRecently, there have been more and more training-based methods [ 20,40,32] aiming to accelerate\\ndiffusion models or flow-based models through one-step distillation. Although these methods can\\nachieve faster inference, they usually require generating paired data using the pre-trained model and\\nsuffer from large training costs ( e.g., >100 GPU days in [ 40,20]). Besides, one-step methods only\\nkeep the generation ability of the original model while disabling more diverse applications such as\\nimage inpainting and image editing. In contrast, our FlowTurbo aims to accelerate flow-based models\\nthrough velocity refinement, which still works in a multi-step manner and performs sampling on the\\noriginal trajectory. For example, FlowTurbo can be easily combined with existing diffusion-based\\nimage editing methods like SDEdit [25] (see Section 4.4).\\n4 Experiments\\nWe conduct extensive experiments to verify the effectiveness of FlowTurbo. Specifically, we ap-\\nply FlowTurbo to both class-conditional image generation and text-to-image generation tasks and\\ndemonstrate that FlowTurbo can significantly reduce the sampling costs of the flow-based generative\\nmodels. We also provide a detailed analysis of each component of FlowTurbo, as well as qualitative\\ncomparisons of different tasks.\\n4.1 Setups\\nIn our experiments, we consider two widely used benchmarks including class-conditional image gen-\\neration and text-to-image generation. For class-conditional image generation, we adopt a transformer-\\nstyle flow-based model SiT-XL [ 24] pre-trained on ImageNet 256 ×256. For text-to-image generation,\\nwe utilize InstaFlow [ 20] as the flow-based model, whose backbone is a U-Net similar to Stable-\\nDiffusion [ 30]. Note that we use the 2-RF model from [ 19] instead of the distilled version since\\nour FlowTurbo is designed to achieve acceleration within the multi-step sampling framework. The\\nvelocity refiner only contains 4.3% and 5% parameters of the corresponding predictor, and the detailed\\narchitecture can be found in Appendix C. During training, we randomly sample ∆t∈(0,0.12]and\\ncompute the training objectives in (13). In both tasks, we use a single NVIDIA A800 GPU to train\\nthe velocity refiner and find it converges within 6 hours. We use a batch size of 8 on a single A800\\nGPU to measure the latency of each method. Please refer to Appendix C for more details.\\n6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='steps. In this section, we propose two techniques to further improve the sampling speed towards\\nreal-time image generation.\\nPseudo corrector. Traditional numerical ODE solvers are usually used to sample from a probability\\nflow ODE. For example, SiT [ 8] adopt a Heun method (or improved Euler’s method) [ 15] as the ODE\\nsolver. The update rule from ti−1totican be written as:\\ndi−1←vθ(xti−1, ti−1|y), ˜xti←xti−1+ ∆tdi−1 (14)\\ndi←vθ(˜xti, ti|y), xi←xi−1+∆t\\n2[di−1+di] (15)\\nEach Heun step contains a predictor step (14) and a corrector step (15), thus includes two evaluations'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='of the velocity predictor vθ, bringing extra inference costs. Motivated by [ 42], we propose to reuse\\nthediin the next sampling step, instead of re-computing it via di←vθ(xti, ti|y)(see Figure 2\\n(b)(c) for illustration). We call this a pseudo corrector since it is different from the predictor-corrector\\nsolvers in numerical analysis. It can be proved (see Appendix B) that the pseudo corrector also enjoys\\n2-order convergence while only having one model evaluation at each step.\\nSample-aware compilation. Compiling the network into a static graph is a widely used technique'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='for model acceleration. However, all the previous work only considers network-level compilation,\\ni.e., only compiling the ϵθorvθ. We propose the sample-aware compilation which wraps both the\\nforward pass of vθorrϕand the sampling operation together (including the classifier-free guidance)\\nand performs the compilation. For example, the sample blocks illustrated in Figure 2 (b, c) are\\ncompiled into static graphs. Since each sample block is independent, we can still adjust the number\\nof inference steps and sampling configurations flexibly.\\n3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='Recently, there have been more and more training-based methods [ 20,40,32] aiming to accelerate\\ndiffusion models or flow-based models through one-step distillation. Although these methods can\\nachieve faster inference, they usually require generating paired data using the pre-trained model and\\nsuffer from large training costs ( e.g., >1'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='[ 40,20]). Besides, one-step methods only\\nkeep the generation ability of the original model while disabling more diverse applications such as\\nimage inpainting and image editing. In contrast, our FlowTurbo aims to accelerate flow-based models\\nthrough velocity refinement, which still works in a multi-step manner and performs sampling on the\\noriginal trajectory. For example, FlowTurbo can be easily combined with existing diffusion-based\\nimage editing methods like SDEdit [25] (see Section 4.4).'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='We conduct extensive experiments to verify the effectiveness of FlowTurbo. Specifically, we ap-\\nply FlowTurbo to both class-conditional image generation and text-to-image generation tasks and\\ndemonstrate that FlowTurbo can significantly reduce the sampling costs of the flow-based generative\\nmodels. We also provide a detailed analysis of each component of FlowTurbo, as well as qualitative\\ncomparisons of different tasks.\\n4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='In our experiments, we consider two widely used benchmarks including class-conditional image gen-\\neration and text-to-image generation. For class-conditional image generation, we adopt a transformer-\\nstyle flow-based model SiT-XL [ 24] pre-trained on ImageNet 256 ×256. For text-to-image generation,\\nwe utilize InstaFlow [ 20] as the flow-based model, whose backbone is a U-Net similar to Stable-\\nDiffusion [ 30]. Note that we use the 2-RF model from [ 19] instead of the distilled version since\\nour FlowTurbo is designed to achieve acceleration within the multi-step sampling framework. The'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='velocity refiner only contains 4.3% and 5% parameters of the corresponding predictor, and the detailed\\narchitecture can be found in Appendix C. During training, we randomly sample ∆t∈(0,0.12]and\\ncompute the training objectives in (13). In both tasks, we use a single NVIDIA A8'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='Table 1: Main results. We apply our FlowTurbo on SiT-XL [ 24] and the 2-RF of InstaFlow [ 20] to perform'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='class-conditional image generation and text-to-image generation, respectively. The image quality is measured by\\nthe FID 50K ↓on ImageNet (256 ×256) and the FID 5K ↓on MS COCO 2017 (512 ×512). We use the suffix to\\nrepresent the number of Heun’s method block ( H), pseudo corrector block ( P), and the velocity refiner block\\n(R). Our results demonstrate that FlowTurbo can significantly accelerate the inference of flow-based models\\nwhile achieving better sampling quality.\\n(a) Class-conditional Image Generation\\nMethodSample FLOPsFID↓Latency\\nConfig (G) (ms / img)\\nSiT-XL [8], ImageNet (256×256)\\nHeun’s H8 1898 3.68 89.4\\nFlowTurbo H2P4R2 957 3.63 41.6 (-53.4%)\\nHeun’s H11 2610 2.79 117.8\\nFlowTurbo H2P8R2 1431 2.69 55.2 (-53.1%)\\nHeun’s H15 3559 2.42 154.8\\nFlowTurbo H5P7R3 2274 2.22 72.5 (-53.2%)\\nHeun’s H24 5694 2.20 240.6\\nFlowTurbo H8P9R5 3457 2.12 100.3 (-58.3%)(b) Text-to-image Generation\\nMethodSample FLOPsFID↓Latency\\nConfig (G) (ms / img)\\nInstaFlow [20], MS COCO 2017 (512×512)\\nHeun’s H4 3955 32.77 104.5\\nFlowTurbo H1P2R2 2649 32.48 68.4 (-34.5%)\\nHeun’s H5 4633 30.73 120.3\\nFlowTurbo H1P4R2 3327 30.19 84.5 (-29.8%)\\nHeun’s H8 6667 28.61 170.5\\nFlowTurbo H1P6R3 4030 28.60 104.8 (-38.5%)\\nHeun’s H10 8023 28.06 203.7\\nFlowTurbo H3P6R3 5386 27.60 137.0 (-32.7%)\\nTable 2: Comparisons with the state-of-the-arts . We compare the sampling quality and speed of different\\nmethods on ImageNet 256×256class-conditional sampling. We demonstrate that FlowTurbo can significantly\\nimprove over the baseline SiT-XL [ 24] and achieves the fastest sampling (38 ms / img) and the best quality (2.12\\nFID) with different configurations.\\nModel Sample Config ParamsLatencyFID↓ IS↑ Precision ↑Recall↑(ms / img)\\nStyleGAN-XL [33] - 166M 190 2.30 265.1 0.78 0.53\\nMask-GIT [2] 8 steps 227M 120 6.18 182.1 0.80 0.51\\nADM [7] 250 steps DDIM [36] 554M 2553 10.94 101.0 0.69 0.63\\nADM-G [7] 250 steps DDIM [36] 608M 4764 4.59 186.7 0.83 0.53\\nLDM-4-G [30] 250 steps DDIM [36] 400M 448 3.60 247.7 0.87 0.48\\nDiT-XL [28] 250 steps DDPM [10] 675M 6914 2.27 278.2 0.83 0.57\\nSiT-XL [24] 25 steps dopri5 [15] 675M 3225 2.15 258.1 0.81 0.60\\nSiT-XL [24] 25 steps Heun’s [15] 675M 250 2.20 254.9 0.81 0.60\\nFlowTurbo (ours) H1P5R3 704M 38 3.93 223.6 0.79 0.56\\nFlowTurbo (ours) H5P7R3 704M 73 2.22 248.0 0.81 0.60\\nFlowTurbo (ours) H8P9R5 704M 100 2.12 255.6 0.81 0.60\\n4.2 Main Results\\nClass-conditional image generation. We adopt the SiT-XL [ 24] trained on ImageNet [ 6] of resolution\\nof256×256. Following common practice [ 24,30], we adopt a classifier-free guidance scale (CFG)\\nof 1.5. According to [ 24], a widely used sampling method of the flow-based model is Heun’s\\nmethod [ 15]. In Table 1a, we demonstrate how our FlowTurbo can achieve faster inference than\\nHeun’s method in various computational budgets. Specifically, we conduct experiments with different\\nsampling configurations (the second column of Table 1a), where we use the suffix to represent the\\nnumber of Heun’s method block ( H), pseudo corrector block ( P), and the velocity refiner block ( R).\\nNote that each Heun’s block contains two evaluations of the velocity predictor while each pseudo\\ncorrector block only contains one. We also provide the total FLOPs during the sampling and the\\ninference speed of each sample configuration. In each group of comparison, we choose the sampling\\nstrategy of FlowTurbo to make the sampling quality (measured by the FID 50K ↓) similar to the\\nbaseline. Our results demonstrate that FlowTurbo can accelerate the inference by 37.2%∼43.1%,\\nwhile still achieving better sampling quality. Notably, FlowTurbo obtains 3.63 FID with a sampling\\nspeed of 41.6 ms/img, achieving real-time image generation.\\nText-to-image generation. We adopt the 2-RF model in [ 20] as our base model for text-to-image\\ngeneration. Note that we do not adopt the distilled version in [ 20] since we focus on accelerating\\nflow-based models within the multi-step sampling paradigm. Following [ 20,26], we compute\\nthe FID 5K ↓between the generated 512×512samples and the images on MS COCO 2017 [ 16]\\n7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='Table 1: Main results. We apply our FlowTurbo on SiT-XL [ 24] and the 2-RF of InstaFlow [ 20] to perform\\nclass-conditional image generation and text-to-image generation, respectively. The image quality is measured by\\nthe FID 50K ↓on ImageNet (256 ×256) and the FID 5K ↓on MS COCO 2017 (512 ×512). We use the suffix to\\nrepresent the number of Heun’s method block ( H), pseudo corrector block ( P), and the velocity refiner block\\n(R). Our results demonstrate that FlowTurbo can significantly accelerate the inference of flow-based models\\nwhile achieving better sampling quality.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='while achieving better sampling quality.\\n(a) Class-conditional Image Generation\\nMethodSample FLOPsFID↓Latency\\nConfig (G) (ms / img)\\nSiT-XL [8], ImageNet (256×256)\\nHeun’s H8 1898 3.68 89.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='8P9R5 3457 2.12 100.3 (-58.3%)(b) Text-to-image Generation\\nMethodSample FLOPsFID↓Latency\\nConfig (G) (ms / img)\\nInstaFlow [20], MS COCO 2017 (512×512)\\nHeun’s H4 3955 32.77 104.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='3P6R3 5386 27.60 137.0 (-32.7%)\\nTable 2: Comparisons with the state-of-the-arts . We compare the sampling quality and speed of different\\nmethods on ImageNet 256×256class-conditional sampling. We demonstrate that FlowTurbo can significantly\\nimprove over the baseline SiT-XL [ 24] and achieves the fastest sampling ('),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content=') with different configurations.\\nModel Sample Config ParamsLatencyFID↓ IS↑ Precision ↑Recall↑(ms / img)\\nStyleGAN-XL [33] - 166M 190 2.30 265.1 0.78 0.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='Class-conditional image generation. We adopt the SiT-XL [ 24] trained on ImageNet [ 6] of resolution\\nof256×256. Following common practice [ 24,30], we adopt a classifier-free guidance scale (CFG)\\nof 1.5. According to [ 24], a widely used sampling method of the flow-based model is Heun’s\\nmethod [ 15]. In Table 1a, we demonstrate how our FlowTurbo can achieve faster inference than\\nHeun’s method in various computational budgets. Specifically, we conduct experiments with different\\nsampling configurations (the second column of Table 1a), where we use the suffix to represent the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='number of Heun’s method block ( H), pseudo corrector block ( P), and the velocity refiner block ( R).\\nNote that each Heun’s block contains two evaluations of the velocity predictor while each pseudo\\ncorrector block only contains one. We also provide the total FLOPs during the sampling and the\\ninference speed of each sample configuration. In each group of comparison, we choose the sampling\\nstrategy of FlowTurbo to make the sampling quality (measured by the FID 50K ↓) similar to the\\nbaseline. Our results demonstrate that FlowTurbo can accelerate the inference by 37.2%∼43.1%,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='while still achieving better sampling quality. Notably, FlowTurbo obtains 3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='/img, achieving real-time image generation.\\nText-to-image generation. We adopt the 2-RF model in [ 20] as our base model for text-to-image\\ngeneration. Note that we do not adopt the distilled version in [ 20] since we focus on accelerating\\nflow-based models within the multi-step sampling paradigm. Following [ 20,26], we compute\\nthe FID 5K ↓between the generated 512×512samples and the images on MS COCO 2017 [ 16]\\n7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='Table 3: Ablation studies. We evaluate the effectiveness of each component in FlowTurbo as well as the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='selection of some hyper-parameters. (a)We gradually add the components of FlowTurbo to the baseline and\\nshow that FlowTurbo can achieve over 50% acceleration with better sampling quality. (b)we experiment with\\ndifferent ranges of ∆tand find ∆t∈(0.0,0.12]yields relatively good results in all the situations. (c)(d) we\\nshow how the sampling quality/speed changes with the number of velocity refiner and pseudo corrector blocks.\\n(a) Ablation of components of FlowTurbo.\\nSampleNH NP HR FID↓Latency\\nConfig (ms / img)\\nbaselines7 0 0 4.42 80.0\\n8 0 0 3.68 89.4 (+11.8%)\\nA 7 0 1 2.80 80.5 (+0.7%)\\nB 2 8 2 2.69 71.6 (-10.4%)\\nC 3 3 2 3.25 57.4 (-28.2%)\\nD 2 4 2 3.63 52.7 (-34.1%)\\nE 1 5 3 3.93 48.0 (-40.0%)\\nE + Model-Level Comp. 3.93 41.0 (-48.7%)\\nE + Sample-Aware Comp. 3.93 38.3 (-52.2%)\\n(b) Ablation of ∆t.\\nSample\\nConfig∆t\\\\ FID↓\\n(0.0,0.1] (0 .0,0.12] (0 .0,0.2] [0 .06,0.12]\\nH6R2 3.58 3.55 4.48 3.36\\nH9R3 2.73 2.65 2.93 2.62\\nH12R5 2.53 2.54 2.64 2.89(c) Effects of the velocity refiner.\\nSampleFID↓Latency\\nConfig (ms / img)\\nH8 3.68 68.0\\nH7R1 2.80 61.6 (-9.4%)\\nH6R2 3.55 55.2 (-18.8%)\\nH5R3 7.62 49.9 (-26.5%)\\n(d) Effects of the pseudo corrector.\\nSampleFID↓Latency\\nConfig (ms / img)\\nH8 3.68 68.0\\nH7R2 2.93 62.0 (-8.8%)\\nH6P1R2 2.60 58.6 (-13.8%)\\nH5P2R2 2.66 55.2 (-18.8%)\\nH4P3R2 2.78 51.8 (-23.8%)\\nH3P4R2 2.96 48.4 (-28.8%)\\nH2P5R2 3.21 45.0 (-33.7%)\\nH1P6R2 3.59 41.6 (-38.7%)\\nvalidation set. The results are summarized in Table 1b, where we compare the sampling speed/quality\\nwith the baseline Heun’s method. Note that the notation of the sampling configuration is the same\\nas Table 1a. The results clearly demonstrate that Our FlowTurbo can also achieve significant\\nacceleration (29.8% ∼38.5%) on text-to-image generation.\\n4.3 Comparisons to State-of-the-Arts\\n40100 1000 10000\\nLatency (ms / img)123456789FID 50K\\nMask-GIT\\nADM-G\\nLDM-4-G\\nDiT-XL StyleGAN-XLSiT\\nFlowTurbo\\nFigure 3: FlowTurbo exhibits favorable\\ntrade-offs compared with SOTA methods.In Table 2, we compare our FlowTurbo with state-of-the-\\nart methods on ImageNet 256×256class-conditional\\ngeneration. We use SiT-XL [ 24] as our base model and\\napply FlowTurbo with different sampling configurations\\non it. We show that FlowTurbo with H1P5R3achieves\\nthe sampling speed of 38 (ms / img) with 3.93 FID (still\\nbetter than most methods like Mask-GIT [ 2], ADM [ 7]).\\nOn the other hand, FlowTurbo with H8P9R5archives the\\nlowest FID 2.12, outperforming all the other methods.\\nBesides, we also provide a comparison of the sampling\\nspeed/quality trade-offs of SiT (by changing the number\\nof sampling steps of Heun’s method) and FlowTurbo (by\\nchanging the sampling configurations) in 3, where the\\nresults of some other state-of-the-arts methods are also\\nincluded. The comparison shows our FlowTurbo exhibits\\nfavorable sampling quality/speed trade-offs.\\n4.4 Analysis\\nAblation of components of FlowTurbo. We evaluate the effectiveness of each component of\\nFlowTurbo in Table 3a. Specifically, we start from the baseline, a 7-step Heun’s method and gradually\\nadd components of FlowTurbo. In the sample config A, we show that adding a velocity refiner\\ncan significantly improve the FID ↓(4.42→2.80), while introducing minimal computational costs\\n(only +0.7%in the latency). From B to E, we adjust the ratios of Heun’s method block, the pseudo\\ncorrector block, and the velocity refiner block to achieve different trade-offs between sampling speed\\n8'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='Table 3: Ablation studies. We evaluate the effectiveness of each component in FlowTurbo as well as the\\nselection of some hyper-parameters. (a)We gradually add the components of FlowTurbo to the baseline and\\nshow that FlowTurbo can achieve over 50% acceleration with better sampling quality. (b)we experiment with\\ndifferent ranges of ∆tand find ∆t∈(0.0,0.12]yields relatively good results in all the situations. (c)(d) we\\nshow how the sampling quality/speed changes with the number of velocity refiner and pseudo corrector blocks.\\n(a) Ablation of components of FlowTurbo.\\nSampleNH NP HR FID↓Latency'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='SampleNH NP HR FID↓Latency\\nConfig (ms / img)\\nbaselines7 0 0 4.42 80.0\\n8 0 0 3.68 89.4 (+11.8%)\\nA 7 0 1 2.80 80.5 (+0.7%)\\nB 2 8 2 2.69 71.6 (-10.4%)\\nC 3 3 2 3.25 57.4 (-28.2%)\\nD 2 4 2 3.63 52.7 (-34.1%)\\nE 1 5 3 3.93 48.0 (-40.0%)\\nE + Model-Level Comp. 3.93 41.0 (-48.7%)\\nE + Sample-Aware Comp. 3.93 38.3 (-52.2%)\\n(b) Ablation of ∆t.\\nSample\\nConfig∆t\\\\ FID↓\\n(0.0,0.1] (0 .0,0.12] (0 .0,0.2] [0 .06,0.12]\\nH6R2 3.58 3.55 4.48 3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='12R5 2.53 2.54 2.64 2.89(c) Effects of the velocity refiner.\\nSampleFID↓Latency\\nConfig (ms / img)\\nH8 3.68 68.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='7R1 2.80 61.6 (-9.4%)\\nH6R2 3.55 55.2 (-18.8%)\\nH5R3 7.62 49.9 (-26.5%)\\n(d) Effects of the pseudo corrector.\\nSampleFID↓Latency\\nConfig (ms / img)\\nH8 3.68 68.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='7R2 2.93 62.0 (-8.8%)\\nH6P1R2 2.60 58.6 (-13.8%)\\nH5P2R2 2.66 55.2 (-18.8%)\\nH4P3R2 2.78 51.8 (-23.8%)\\nH3P4R2 2.96 48.4 (-28.8%)\\nH2P5R2 3.21 45.0 (-33.7%)\\nH1P6R2 3.59 41.6 (-38.7%)\\nvalidation set. The results are summarized in Table 1b, where we compare the sampling speed/quality\\nwith the baseline Heun’s method. Note that the notation of the sampling configuration is the same\\nas Table 1a. The results clearly demonstrate that Our FlowTurbo can also achieve significant\\nacceleration (29.8% ∼38.5%) on text-to-image generation.\\n4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='(ms / img)123456789FID 50K\\nMask-GIT\\nADM-G\\nLDM-4-G\\nDiT-XL StyleGAN-XLSiT\\nFlowTurbo\\nFigure 3: FlowTurbo exhibits favorable\\ntrade-offs compared with SOTA methods.In Table 2, we compare our FlowTurbo with state-of-the-\\nart methods on ImageNet 256×256class-conditional\\ngeneration. We use SiT-XL [ 24] as our base model and\\napply FlowTurbo with different sampling configurations\\non it. We show that FlowTurbo with H1P5R3achieves\\nthe sampling speed of 38 (ms / img) with 3.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='(still\\nbetter than most methods like Mask-GIT [ 2], ADM [ 7]).\\nOn the other hand, FlowTurbo with H8P9R5archives the\\nlowest FID 2.12, outperforming all the other methods.\\nBesides, we also provide a comparison of the sampling\\nspeed/quality trade-offs of SiT (by changing the number\\nof sampling steps of Heun’s method) and FlowTurbo (by\\nchanging the sampling configurations) in 3, where the\\nresults of some other state-of-the-arts methods are also\\nincluded. The comparison shows our FlowTurbo exhibits\\nfavorable sampling quality/speed trade-offs.\\n4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='Ablation of components of FlowTurbo. We evaluate the effectiveness of each component of\\nFlowTurbo in Table 3a. Specifically, we start from the baseline, a 7-step Heun’s method and gradually\\nadd components of FlowTurbo. In the sample config A, we show that adding a velocity refiner\\ncan significantly improve the FID ↓(4.42→2.80), while introducing minimal computational costs\\n(only +0.7%in the latency). From B to E, we adjust the ratios of Heun’s method block, the pseudo\\ncorrector block, and the velocity refiner block to achieve different trade-offs between sampling speed\\n8'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='A moving train against the background of a blue sky and epic clouds'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='A pink rectangular potion with intricate gold adornment\\nAn alien ship crashed with a sad alien sitting on the desert ground\\n A multicolored iridescent horse with unicorn horn and dragon wings(a) Results of Heun’s (2.6 s / img, left) and FlowTurbo (1.8 s / img ,right )\\nObject Removal\\nImage Editing\\nImage Inpainting (b) Extensions\\nFigure 4: Qualitative results. (a) We compared our FlowTurbo with Heun’s method on Lumina-Next-T2I [ 9].\\nWith better image quality, our method requires much less sampling time ( −30.8%).(b)Since FlowTurbo\\nremains the multi-step sampling paradigm, it can be seamlessly applied to more applications such as image\\ninpainting, image editing, and object removal.\\nand quality. In the last two rows, we show that our sample-aware compilation is better than standard\\nmodel-level compilation, further increasing the sampling speed.\\nChoice of ∆t.We find the choice of ∆tduring training is crucial and affects the sampling results a\\nlot in our experiments, as shown in Table 3b. We find ∆t∈(0.0,0.1]works well for more sampling\\nsteps like H12R5, while ∆t∈[0.06,0.12]is better fore fewer sampling steps like H6R2andH9R3.\\nBesides, we find ∆t∈(0.0,0.12]yields relatively good results in all the situations.\\nEffects of velocity refiner. We evaluate the effects of the different number of velocity refiners\\nin Table 3c, and find that appropriately increasing the number of velocity refiners can improve the\\ntrade-off between sampling quality and speed. Specifically, we find H6R2can achieve better image\\nquality and generation speed than the baseline H8.\\nEffects of pseudo corrector. In Table 3d, we fix the total number of both Heun’s sample block and\\npseudo corrector block and adjust the ratio of the pseudo corrector. Our results demonstrate that\\nincreasing the number of pseudo corrector blocks can significantly improve the sampling speed while\\nintroducing neglectable performance drop ( e.g., FlowTurbo with H1P6R2performs better than H8).\\nQualitative results and extensions. We provide qualitative results of high-resolution text-to-image\\ngeneration by applying FlowTurbo to the newly released flow-based model Lumina-Next-T2I [ 9].\\nSince Lumina-Next-T2I adopts a heavy language model Gemma-2B [ 39] to extract text features and\\ngenerates high-resolution images ( 1024×1024 ), the inference speed of it is slower than SiT [ 24].\\nIn Figure 4a, we show that our FlowTurbo can generate images with better quality and higher\\ninference speed compared with the baseline Heun’s method. Besides, since FlowTurbo remains the\\nmulti-step sampling paradigm, it can be seamlessly applied to more applications like image inpainting,\\nimage editing, and object removal (Section 4.4). Please also refer to the Appendix C for the detailed\\nimplementation of various tasks.\\nLimitations and broader impact. Despite the effectiveness of FlowTurbo, our velocity refiner\\nhighly relies on the observation that the velocity is a “stable value” during the sampling. However, we\\nhave not found such a stable value in diffusion-based models yet, which might limit the application.\\nBesides, the abuse of FlowTurbo may also accelerate the generation of malicious content.\\n5 Conclusion\\nIn this paper, we introduce FlowTurbo, a novel framework designed to accelerate flow-based genera-\\ntive models. By leveraging the stability of the velocity predictor’s outputs, we propose a lightweight\\nvelocity refiner to adjust the velocity field offsets. This refiner comprises only about 5% of the original\\nvelocity predictor’s parameters and can be efficiently trained in under 6 GPU hours. Additionally,\\nwe have proposed a pseudo corrector that reduces the number of model evaluations while maintain-\\ning the same convergence order as the second-order Heun’s method. Furthermore, we propose a\\n9'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='A moving train against the background of a blue sky and epic clouds\\n A pink rectangular potion with intricate gold adornment\\nAn alien ship crashed with a sad alien sitting on the desert ground\\n A multicolored iridescent horse with unicorn horn and dragon wings(a) Results of Heun’s (2.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='/ img ,right )\\nObject Removal\\nImage Editing\\nImage Inpainting (b) Extensions\\nFigure 4: Qualitative results. (a) We compared our FlowTurbo with Heun’s method on Lumina-Next-T2I [ 9].\\nWith better image quality, our method requires much less sampling time ( −30.8%).(b)Since FlowTurbo\\nremains the multi-step sampling paradigm, it can be seamlessly applied to more applications such as image\\ninpainting, image editing, and object removal.\\nand quality. In the last two rows, we show that our sample-aware compilation is better than standard\\nmodel-level compilation, further increasing the sampling speed.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='Choice of ∆t.We find the choice of ∆tduring training is crucial and affects the sampling results a\\nlot in our experiments, as shown in Table 3b. We find ∆t∈(0.0,0.1]works well for more sampling\\nsteps like H12R5, while ∆t∈[0.06,0.12]is better fore fewer sampling steps like H6R2andH9R3.\\nBesides, we find ∆t∈(0.0,0.12]yields relatively good results in all the situations.\\nEffects of velocity refiner. We evaluate the effects of the different number of velocity refiners\\nin Table 3c, and find that appropriately increasing the number of velocity refiners can improve the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='trade-off between sampling quality and speed. Specifically, we find H6R2can achieve better image\\nquality and generation speed than the baseline H8.\\nEffects of pseudo corrector. In Table 3d, we fix the total number of both Heun’s sample block and\\npseudo corrector block and adjust the ratio of the pseudo corrector. Our results demonstrate that\\nincreasing the number of pseudo corrector blocks can significantly improve the sampling speed while\\nintroducing neglectable performance drop ( e.g., FlowTurbo with H1P6R2performs better than H8).'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='Qualitative results and extensions. We provide qualitative results of high-resolution text-to-image\\ngeneration by applying FlowTurbo to the newly released flow-based model Lumina-Next-T2I [ 9].\\nSince Lumina-Next-T2I adopts a heavy language model Gemma-2B [ 39] to extract text features and\\ngenerates high-resolution images ( 1024×1024 ), the inference speed of it is slower than SiT [ 24].\\nIn Figure 4a, we show that our FlowTurbo can generate images with better quality and higher\\ninference speed compared with the baseline Heun’s method. Besides, since FlowTurbo remains the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='multi-step sampling paradigm, it can be seamlessly applied to more applications like image inpainting,\\nimage editing, and object removal (Section 4.4). Please also refer to the Appendix C for the detailed\\nimplementation of various tasks.\\nLimitations and broader impact. Despite the effectiveness of FlowTurbo, our velocity refiner\\nhighly relies on the observation that the velocity is a “stable value” during the sampling. However, we\\nhave not found such a stable value in diffusion-based models yet, which might limit the application.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='Besides, the abuse of FlowTurbo may also accelerate the generation of malicious content.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='In this paper, we introduce FlowTurbo, a novel framework designed to accelerate flow-based genera-\\ntive models. By leveraging the stability of the velocity predictor’s outputs, we propose a lightweight\\nvelocity refiner to adjust the velocity field offsets. This refiner comprises only about 5% of the original\\nvelocity predictor’s parameters and can be efficiently trained in under'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='. Additionally,\\nwe have proposed a pseudo corrector that reduces the number of model evaluations while maintain-\\ning the same convergence order as the second-order Heun’s method. Furthermore, we propose a\\n9'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='sample-aware compilation technique to enhance sampling speed. Extensive experiments on various'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='flow-based generative models demonstrate FlowTurbo’s effectiveness on both class-conditional image\\ngeneration and text-to-image generation. We hope our work will inspire future efforts to accelerate\\nflow-based generative models across various application scenarios.\\nAcknowledgments\\nThis work was supported in part by the National Natural Science Foundation of China under Grant\\n62321005, Grant 624B1026, Grant 62336004, and Grant 62125603.\\nReferences\\n[1]Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying\\nframework for flows and diffusions. arXiv preprint arXiv:2303.08797 , 2023.\\n[2]Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image\\ntransformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\\npages 11315–11325, 2022.\\n[3]Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo,\\nHuchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k\\ntext-to-image generation. arXiv preprint arXiv:2403.04692 , 2024.\\n[4]Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James\\nKwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic\\ntext-to-image synthesis. arXiv preprint arXiv:2310.00426 , 2023.\\n[5]Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential\\nequations. Advances in neural information processing systems , 31, 2018.\\n[6]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In CVPR , pages 248–255. IEEE, 2009.\\n[7]Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS ,\\n34:8780–8794, 2021.\\n[8]Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi,\\nDominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution\\nimage synthesis. arXiv preprint arXiv:2403.03206 , 2024.\\n[9]Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu,\\nYuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via\\nflow-based large diffusion transformers. arXiv preprint arXiv:2405.05945 , 2024.\\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS , 33:6840–\\n6851, 2020.\\n[11] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. NeurIPS , 2021.\\n[12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.\\nVideo diffusion models. arXiv preprint arXiv:2204.03458 , 2022.\\n[13] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. NeurIPS ,\\n34:21696–21707, 2021.\\n[14] Black Forest Labs. Flux: A powerful tool for text generation. https://huggingface.co/\\nblack-forest-labs/FLUX.1-dev , 2024. Accessed: 2024-09-26.\\n[15] John Denholm Lambert et al. Numerical methods for ordinary differential systems , volume 146. Wiley\\nNew York, 1991.\\n[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV , pages 740–755. Springer,\\n2014.\\n[17] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for\\ngenerative modeling. arXiv preprint arXiv:2210.02747 , 2022.\\n[18] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on\\nmanifolds. ICLR , 2022.\\n[19] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer\\ndata with rectified flow. arXiv preprint arXiv:2209.03003 , 2022.\\n10'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='sample-aware compilation technique to enhance sampling speed. Extensive experiments on various\\nflow-based generative models demonstrate FlowTurbo’s effectiveness on both class-conditional image\\ngeneration and text-to-image generation. We hope our work will inspire future efforts to accelerate\\nflow-based generative models across various application scenarios.\\nAcknowledgments\\nThis work was supported in part by the National Natural Science Foundation of China under Grant\\n62321005, Grant 624B1026, Grant 62336004, and Grant 62125603.\\nReferences'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='References\\n[1]Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying\\nframework for flows and diffusions. arXiv preprint arXiv:2303.08797 , 2023.\\n[2]Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image\\ntransformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\\npages 11315–11325, 2022.\\n[3]Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k\\ntext-to-image generation. arXiv preprint arXiv:2403.04692 , 2024.\\n[4]Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James\\nKwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic\\ntext-to-image synthesis. arXiv preprint arXiv:2310.00426 , 2023.\\n[5]Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='equations. Advances in neural information processing systems , 31, 2018.\\n[6]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In CVPR , pages 248–255. IEEE, 2009.\\n[7]Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS ,\\n34:8780–8794, 2021.\\n[8]Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi,\\nDominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='image synthesis. arXiv preprint arXiv:2403.03206 , 2024.\\n[9]Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu,\\nYuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via\\nflow-based large diffusion transformers. arXiv preprint arXiv:2405.05945 , 2024.\\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS , 33:6840–\\n6851, 2020.\\n[11] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. NeurIPS , 2021.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='[12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.\\nVideo diffusion models. arXiv preprint arXiv:2204.03458 , 2022.\\n[13] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. NeurIPS ,\\n34:21696–21707, 2021.\\n[14] Black Forest Labs. Flux: A powerful tool for text generation. https://huggingface.co/\\nblack-forest-labs/FLUX.1-dev , 2024. Accessed: 2024-09-26.\\n[15] John Denholm Lambert et al. Numerical methods for ordinary differential systems , volume 146. Wiley\\nNew York, 1991.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='New York, 1991.\\n[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV , pages 740–755. Springer,\\n2014.\\n[17] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for\\ngenerative modeling. arXiv preprint arXiv:2210.02747 , 2022.\\n[18] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on\\nmanifolds. ICLR , 2022.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='manifolds. ICLR , 2022.\\n[19] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer\\ndata with rectified flow. arXiv preprint arXiv:2209.03003 , 2022.\\n10'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='[20] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='quality diffusion-based text-to-image generation. In The Twelfth International Conference on Learning\\nRepresentations , 2023.\\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101 , 2017.\\n[22] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode\\nsolver for diffusion probabilistic model sampling in around 10 steps. NeurIPS , 2022.\\n[23] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver\\nfor guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095 , 2022.\\n[24] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie.\\nSit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv\\npreprint arXiv:2401.08740 , 2024.\\n[25] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit:\\nGuided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073 ,\\n2021.\\n[26] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim\\nSalimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition , pages 14297–14306, 2023.\\n[27] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-\\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In\\nProceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 4296–4304, 2024.\\n[28] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision , pages 4195–4205, 2023.\\n[29] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,\\nand Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv\\npreprint arXiv:2307.01952 , 2023.\\n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\\nimage synthesis with latent diffusion models. In CVPR , pages 10684–10695, 2022.\\n[31] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. ICLR , 2022.\\n[32] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation.\\narXiv preprint arXiv:2311.17042 , 2023.\\n[33] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In\\nACM SIGGRAPH 2022 conference proceedings , pages 1–10, 2022.\\n[34] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush\\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400\\nmillion image-text pairs. arXiv preprint arXiv:2111.02114 , 2021.\\n[35] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning\\nusing nonequilibrium thermodynamics. In ICML , pages 2256–2265. PMLR, 2015.\\n[36] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. ICLR , 2021.\\n[37] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023.\\n[38] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\\nScore-based generative modeling through stochastic differential equations. In ICLR , 2021.\\n[39] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,\\nLaurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on\\ngemini research and technology. arXiv preprint arXiv:2403.08295 , 2024.\\n[40] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman, and\\nTaesung Park. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828 ,\\n2023.\\n[41] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\\nmodels. In ICCV , pages 3836–3847, 2023.\\n[42] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictor-corrector\\nframework for fast sampling of diffusion models. NeurIPS , 2023.\\n[43] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou,\\nTianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024.\\n11'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='[20] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-\\nquality diffusion-based text-to-image generation. In The Twelfth International Conference on Learning\\nRepresentations , 2023.\\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101 , 2017.\\n[22] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode\\nsolver for diffusion probabilistic model sampling in around'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='. NeurIPS , 2022.\\n[23] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver\\nfor guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095 , 2022.\\n[24] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie.\\nSit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv\\npreprint arXiv:2401.08740 , 2024.\\n[25] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit:'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073 ,\\n2021.\\n[26] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim\\nSalimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition , pages 14297–14306, 2023.\\n[27] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-\\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 4296–4304, 2024.\\n[28] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision , pages 4195–4205, 2023.\\n[29] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,\\nand Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv\\npreprint arXiv:2307.01952 , 2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='preprint arXiv:2307.01952 , 2023.\\n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\\nimage synthesis with latent diffusion models. In CVPR , pages 10684–10695, 2022.\\n[31] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. ICLR , 2022.\\n[32] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation.\\narXiv preprint arXiv:2311.17042 , 2023.\\n[33] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='ACM SIGGRAPH 20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content=', pages 1–10, 2022.\\n[34] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush\\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='-text pairs. arXiv preprint arXiv:2111.02114 , 2021.\\n[35] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning\\nusing nonequilibrium thermodynamics. In ICML , pages 2256–2265. PMLR, 2015.\\n[36] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. ICLR , 2021.\\n[37] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023.\\n[38] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='Score-based generative modeling through stochastic differential equations. In ICLR , 2021.\\n[39] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,\\nLaurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on\\ngemini research and technology. arXiv preprint arXiv:2403.08295 , 2024.\\n[40] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman, and\\nTaesung Park. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828 ,\\n2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='2023.\\n[41] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\\nmodels. In ICCV , pages 3836–3847, 2023.\\n[42] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictor-corrector\\nframework for fast sampling of diffusion models. NeurIPS , 2023.\\n[43] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou,\\nTianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024.\\n11'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='A Detailed Background of Diffusion and Flow-based Models'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='In this section, we will provide a detailed background of diffusion and flow-based models, which is\\nhelpful to understand the difference and relationship between them.\\nA.1 Diffusion Models\\nTheforward pass i.e.diffusioin pass of DPMs can be defined as a sequence of variables {xt}t∈[0,1]\\nstarting with x0, such that for any t∈[0,1],x0∈RDis a D-dimensional random variable with an\\nunknown data distribution p0(x0). the distribution of xtconditioned on x0satisfies\\np0t(xt|x0) =N(xt|αtx0, σtI) (16)\\nwhere αt, σt∈R+are differentiable functions of twith bounded derevatives. The choice for αt\\nandσtis referred to as the noise schedule of a DPM. Let pt(xt)denote the marginal distribution\\nofxt, DPMs choose noise schedules to ensure the marginal distribution p1(x1) =N(0,I)and the\\nsignal-to-noise-ratio (SNR) α2\\nt/σ2\\ntis strictly decreasing w.r.t. t[13]. And we have\\nxt=αtx0+σtϵ, t∈[0,1],ϵ∼ N(0,I) (17)\\nMoreover, Kingma et al. [13] prove that the following stochastic differential equation (SDE) has the\\nsame transition distribution q0t(xt|x0)as in (16) for any t∈[0,1]:\\ndxt=f(t)xtdt+g(t)dwt, t∈[0,1],x0∼p0(x0) (18)\\nwhere wt∈RDis the standard Wiener process , and\\nf(t) =d logαt\\ndt, g2(t) =dσ2\\nt\\ndt−2d logαt\\ndtσ2\\nt (19)\\nSong et al. [ 38] have shown that the forward process in (18) has an equivalent reverse process from\\ntime1to0under some regularity conditions, starting with the marginal distribution pT(xT):\\ndxt= [f(t)xt−g2(t)∇xlogpt(xt)]dt+g(t)d¯wt,xT∼pT(xT) (20)\\nwhere ¯wt∈RDis a standard Wiener process in the reverse time. To solve the reverse process in (20),\\nthe only thing we should do is to estimate the score term ∇xlogpt(xt)at each time t. In practice,\\nDPMs train a neural network ϵθ(x, t)parameterized by θto estimate the scaled score function:\\n−σt∇xlogpt(xt). The parameter θis optimized by minimizing the following objective [ 10,38,24]\\nLDM(θ) =Et,p0(x0),p(xt|x0)\\x02\\nλ(t)∥ϵθ(xt, t) +σt∇xlogpt(xt)∥2\\n2\\x03\\n(21)\\nwhere λ(t)is a time-dependent coefficient. As ϵθ(xt, t)can alse be regarded as predicting the\\nGaussian noise added to xt, it is usually called the noise prediction model . Since the ground truth of\\nϵθ(xt, t)is−σt∇xlogpt(xt), DPMs replace the score function in (20) by−ϵθ(xt, t)/σtand we refer\\nto it as diffusion-based generative model. DPMs define a parameterized reverse process (diffusion\\nSDE) from time 1to0, starting with x1∼p1(x1):\\ndxt=\\x14\\nf(t)xt+g2(t)\\nσtϵθ(xt, t)\\x15\\ndt+g(t)d¯wt, x 1∼ N(0,I) (22)\\nSamples can be generated from DPMs by solving the diffusion SDE in (22) with numerical solvers.\\nWhen discretizing SDEs, the step size is limited by the randomness of the Wiener process. A large\\nstep size (small number of steps) often causes non-convergence, especially in high dimensional\\nspaces. For faster sampling, we can consider the associated probability flow ODE [ 38] which has the\\nsame marginla distribution at each time tas that of the SDE. Specifically, for DPMs, Song et al. [38]\\nproved that the probability flow ODE of (22) is\\ndxt\\ndt=v(xt, t) :=f(t)xt+g2(t)\\n2σtϵθ(xt, t),x1∼ N(0,I) (23)\\nSamples can be generated by solving the ODE from 1to0. Comparing with SDEs, ODEs can be\\nsolved with larger step sizes as they have no randomness. Furthermore, we can take advantage of\\nefficient numerical ODE solvers to accelerate the sampling.\\n12'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='A Detailed Background of Diffusion and Flow-based Models\\nIn this section, we will provide a detailed background of diffusion and flow-based models, which is\\nhelpful to understand the difference and relationship between them.\\nA.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='Theforward pass i.e.diffusioin pass of DPMs can be defined as a sequence of variables {xt}t∈[0,1]\\nstarting with x0, such that for any t∈[0,1],x0∈RDis a D-dimensional random variable with an\\nunknown data distribution p0(x0). the distribution of xtconditioned on x0satisfies\\np0t(xt|x0) =N(xt|αtx0, σtI) (16)\\nwhere αt, σt∈R+are differentiable functions of twith bounded derevatives. The choice for αt\\nandσtis referred to as the noise schedule of a DPM. Let pt(xt)denote the marginal distribution\\nofxt, DPMs choose noise schedules to ensure the marginal distribution p1(x1) =N(0,I)and the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='signal-to-noise-ratio (SNR) α'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='.r.t. t[13]. And we have\\nxt=αtx0+σtϵ, t∈[0,1],ϵ∼ N(0,I) (17)\\nMoreover, Kingma et al. [13] prove that the following stochastic differential equation (SDE) has the\\nsame transition distribution q0t(xt|x0)as in (16) for any t∈[0,1]:\\ndxt=f(t)xtdt+g(t)dwt, t∈[0,1],x0∼p0(x0) (18)\\nwhere wt∈RDis the standard Wiener process , and\\nf(t) =d logαt\\ndt, g2(t) =dσ'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='(19)\\nSong et al. [ 38] have shown that the forward process in (18) has an equivalent reverse process from\\ntime1to0under some regularity conditions, starting with the marginal distribution pT(xT):\\ndxt= [f(t)xt−g2(t)∇xlogpt(xt)]dt+g(t)d¯wt,xT∼pT(xT) (20)\\nwhere ¯wt∈RDis a standard Wiener process in the reverse time. To solve the reverse process in (20),\\nthe only thing we should do is to estimate the score term ∇xlogpt(xt)at each time t. In practice,\\nDPMs train a neural network ϵθ(x, t)parameterized by θto estimate the scaled score function:'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='−σt∇xlogpt(xt). The parameter θis optimized by minimizing the following objective [ 10,38,24]\\nLDM(θ) =Et,p0(x0),p(xt|x0)\\x02\\nλ(t)∥ϵθ(xt, t) +σt∇xlogpt(xt)∥2\\n2\\x03\\n(21)\\nwhere λ(t)is a time-dependent coefficient. As ϵθ(xt, t)can alse be regarded as predicting the\\nGaussian noise added to xt, it is usually called the noise prediction model . Since the ground truth of\\nϵθ(xt, t)is−σt∇xlogpt(xt), DPMs replace the score function in (20) by−ϵθ(xt, t)/σtand we refer\\nto it as diffusion-based generative model. DPMs define a parameterized reverse process (diffusion\\nSDE) from time 1to0, starting with x1∼p1(x1):'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='SDE) from time 1to0, starting with x1∼p1(x1):\\ndxt=\\x14\\nf(t)xt+g2(t)\\nσtϵθ(xt, t)\\x15\\ndt+g(t)d¯wt, x 1∼ N(0,I) (22)\\nSamples can be generated from DPMs by solving the diffusion SDE in (22) with numerical solvers.\\nWhen discretizing SDEs, the step size is limited by the randomness of the Wiener process. A large\\nstep size (small number of steps) often causes non-convergence, especially in high dimensional\\nspaces. For faster sampling, we can consider the associated probability flow ODE [ 38] which has the\\nsame marginla distribution at each time tas that of the SDE. Specifically, for DPMs, Song et al. [38]'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='proved that the probability flow ODE of (22) is\\ndxt\\ndt=v(xt, t) :=f(t)xt+g2(t)\\n2σtϵθ(xt, t),x1∼ N(0,I) (23)\\nSamples can be generated by solving the ODE from 1to0. Comparing with SDEs, ODEs can be\\nsolved with larger step sizes as they have no randomness. Furthermore, we can take advantage of\\nefficient numerical ODE solvers to accelerate the sampling.\\n12'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 12}, page_content='A.2 Flow-based Models'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 12}, page_content='To introduce flow in detail, first we construct a time-dependent vector field, u: [0,1]×RD→RD.\\nA vector field utcan be used to construct a time-dependent diffeomorphic map, called a flow,\\nϕ: [0,1]×RD→RD,defined via the ordinary differential equation (ODE):\\nd\\ndtϕt(x0) =ut(ϕt(x0)) (24)\\nϕ0(x0) =x0 (25)\\nChen et al. [5] suggested modeling the vector field utwith a neural network vθ, which in turn leads\\nto a deep parametric model of the flow ϕt, called a Continuous Normalizing Flow (CNF). It is a\\nmore generic modeling technique and can capture the probability paths of diffusion process as well.\\nTraining a CNF becomes more practical since the propose of the conditional flow matching (CFM)\\ntechnique [17], which learns the conditional velocity field of the flow.\\nFor generative models, similar to (17) we can add some constraints to the noise schedule such that\\nα0= 1, σ0= 0andα1= 0, σ1= 1, and then define the flow as:\\nψt(·|ϵ) :x07→αtx0+σtϵ (26)\\nThe corresponding velocity vector field which is used to construct the flow ψtcan be represented as:\\nut(ψt(x0|ϵ)|ϵ) =d\\ndtψt(x0|ϵ) = ˙αtx0+ ˙σtϵ (27)\\nConsider the time-dependent probability density function (PDF) pt(x)ofxt=ψt(x0|ϵ) =αtx0+\\nσtϵ. Lipman et al. [17] proved that the marginal vector field utthat generates the probability path pt\\nsatisfies a Partial Differential Equation (PDE) called continuity equation (also transport equation )\\nd\\ndtpt(x) +∇x·(ut(x)pt(x)) = 0 (28)\\nUsing conditional flow matching technique v(xt, t)in(23) can be estimated parametrically as\\nvθ(xt, t)by minimizing the following objective\\nLFM(θ) =Et,p1(ϵ),p0(x0)\\r\\r\\r\\rvθ(xt, t)−d\\ndtψt(x0|ϵ)\\r\\r\\r\\r2\\n2(29)\\n=Et,p1(ϵ),p0(x0)∥vθ(xt, t)−˙αtx0−˙σtϵ∥2\\n2(30)\\nWe refer to (23) as aflow-based generative model. Since we have xt=ψt(x0|ϵ), the sampling of a\\nflow-based model can be achieved by solving the probability flow ODE with learned velocity\\ndxt\\ndt=vθ(xt, t), x 1∼p1(x1) (31)\\nA.3 Relationship Between Diffusion and Flow-based Models\\nThere exists a straightforward connection between v(xt, t)and the score term σt∇xlogpt(xt)\\naccording to [24].\\nv(xt, t) =˙αt\\nαtxt+\\x12\\n˙σt−˙αtσt\\nαt\\x13\\n(−σt∇xlogpt(xt)) (32)\\n13'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 12}, page_content='-based Models\\nTo introduce flow in detail, first we construct a time-dependent vector field, u: [0,1]×RD→RD.\\nA vector field utcan be used to construct a time-dependent diffeomorphic map, called a flow,\\nϕ: [0,1]×RD→RD,defined via the ordinary differential equation (ODE):\\nd\\ndtϕt(x0) =ut(ϕt(x0)) (24)\\nϕ0(x0) =x0 (25)\\nChen et al. [5] suggested modeling the vector field utwith a neural network vθ, which in turn leads\\nto a deep parametric model of the flow ϕt, called a Continuous Normalizing Flow (CNF). It is a'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 12}, page_content='more generic modeling technique and can capture the probability paths of diffusion process as well.\\nTraining a CNF becomes more practical since the propose of the conditional flow matching (CFM)\\ntechnique [17], which learns the conditional velocity field of the flow.\\nFor generative models, similar to (17) we can add some constraints to the noise schedule such that\\nα0= 1, σ0= 0andα1= 0, σ1= 1, and then define the flow as:\\nψt(·|ϵ) :x07→αtx0+σtϵ (26)\\nThe corresponding velocity vector field which is used to construct the flow ψtcan be represented as:\\nut(ψt(x0|ϵ)|ϵ) =d'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 12}, page_content='ut(ψt(x0|ϵ)|ϵ) =d\\ndtψt(x0|ϵ) = ˙αtx0+ ˙σtϵ (27)\\nConsider the time-dependent probability density function (PDF) pt(x)ofxt=ψt(x0|ϵ) =αtx0+\\nσtϵ. Lipman et al. [17] proved that the marginal vector field utthat generates the probability path pt\\nsatisfies a Partial Differential Equation (PDE) called continuity equation (also transport equation )\\nd\\ndtpt(x) +∇x·(ut(x)pt(x)) = 0 (28)\\nUsing conditional flow matching technique v(xt, t)in(23) can be estimated parametrically as\\nvθ(xt, t)by minimizing the following objective\\nLFM(θ) =Et,p1(ϵ),p0(x0)\\r\\r\\r\\rvθ(xt, t)−d\\ndtψt(x0|ϵ)\\r\\r\\r\\r2\\n2(29)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 12}, page_content='dtψt(x0|ϵ)\\r\\r\\r\\r2\\n2(29)\\n=Et,p1(ϵ),p0(x0)∥vθ(xt, t)−˙αtx0−˙σtϵ∥2\\n2(30)\\nWe refer to (23) as aflow-based generative model. Since we have xt=ψt(x0|ϵ), the sampling of a\\nflow-based model can be achieved by solving the probability flow ODE with learned velocity\\ndxt\\ndt=vθ(xt, t), x 1∼p1(x1) (31)\\nA.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 12}, page_content='-based Models\\nThere exists a straightforward connection between v(xt, t)and the score term σt∇xlogpt(xt)\\naccording to [24].\\nv(xt, t) =˙αt\\nαtxt+\\x12\\n˙σt−˙αtσt\\nαt\\x13\\n(−σt∇xlogpt(xt)) (32)\\n13'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 13}, page_content='Algorithm 1 Heun’s Method Sampler'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 13}, page_content='Require: timesteps {ti}N−1\\ni=0,αt, σt,x0∼ N(0,I), velocity prediction model vθ(x, t|y)\\nfori= 0toN−1do\\n∆ti←ti+1−ti\\ndi←vθ(xi, ti|y)\\n˜xti+1←xi+ ∆tidi\\ndi+1←vθ(˜xti+1, ti+1|y)\\nxti+1←xi+∆ti\\n2[di+di+1]\\nend for\\nreturn: xN\\nAlgorithm 2 Pseudo Corrector Sampler\\nRequire: timesteps {ti}N−1\\ni=0,αt, σt,x0∼ N(0,I), velocity prediction model vθ(x, t|y)\\n∆t←t1−t0\\nfori= 0toN−1do\\n∆ti←ti+1−ti\\nifi= 0then\\ndi←vθ(xi, ti|y)\\nend if\\n˜xti+1←xi+ ∆tidi\\ndi+1←vθ(˜xti+1, ti+1|y)\\nxti+1←xi+∆ti\\n2[di+di+1]\\nend for\\nreturn: xN\\nWe can define ζt= ˙σt−˙αtσt\\nαt, and we have ϵθ(xt, t)to estimate −σt∇xlogpt(xt), then derive the\\nrelationship between LDM(θ)andLFM(θ)We can plug (32) into the loss LFM(θ)in Equation (30)\\nLFM(θ) =Et,p1(ϵ),p0(x0)∥vθ(xt, t)−˙αtx0−˙σtϵ∥2\\n2(33)\\n=Et,p1(ϵ),p0(x0)\\r\\r\\r\\r˙αt\\nαtxt+ζtϵθ(xt, t)−˙αtx0−˙σtϵ\\r\\r\\r\\r2\\n2(34)\\n=Et,p1(ϵ),p0(x0)\\r\\r\\r\\r˙αtσt\\nαtϵ+ζtϵθ(xt, t)−˙αtx0−˙σtϵ\\r\\r\\r\\r2\\n2(35)\\n=Et,p1(ϵ),p0(x0)∥ζtϵθ(xt, t)−ζtϵ∥2\\n2(36)\\n=Et,p1(ϵ),p0(x0)h\\nζ2\\nt∥ϵθ(xt, t)−ϵ∥2\\n2i\\n(37)\\nxt=αtx0+σtϵ= Et,p0(x0),p(xt|x0)h\\nζ2\\nt∥ϵθ(xt, t) +σt∇xlogpt(xt)∥2\\n2i\\n(38)\\nRecall that\\nLDM(θ) =Et,p0(x0),p(xt|x0)h\\nλ(t)∥ϵθ(xt, t) +σt∇xlogpt(xt)∥2\\n2i\\n, (39)\\nwe can see that the difference of LDM(θ)andLFM(θ)during training is caused by the weighted\\nfunction, which leaving to different trajectories and properties.\\nB Proof of Convergence of Pseudo Corrector\\nIn this section, we will prove that the proposed pseudo corrector has the same local truncation error\\nand global convergence order as Heun’s method. The detailed sampling procedure of Heun’s method\\n14'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 13}, page_content='=0,αt, σt,x0∼ N(0,I), velocity prediction model vθ(x, t|y)\\nfori= 0toN−1do\\n∆ti←ti+1−ti\\ndi←vθ(xi, ti|y)\\n˜xti+1←xi+ ∆tidi\\ndi+1←vθ(˜xti+1, ti+1|y)\\nxti+1←xi+∆ti\\n2[di+di+1]\\nend for\\nreturn: xN\\nAlgorithm'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 13}, page_content='= 0toN−1do\\n∆ti←ti+1−ti\\nifi= 0then\\ndi←vθ(xi, ti|y)\\nend if\\n˜xti+1←xi+ ∆tidi\\ndi+1←vθ(˜xti+1, ti+1|y)\\nxti+1←xi+∆ti\\n2[di+di+1]\\nend for\\nreturn: xN\\nWe can define ζt= ˙σt−˙αtσt\\nαt, and we have ϵθ(xt, t)to estimate −σt∇xlogpt(xt), then derive the\\nrelationship between LDM(θ)andLFM(θ)We can plug (32) into the loss LFM(θ)in Equation (30)\\nLFM(θ) =Et,p1(ϵ),p0(x0)∥vθ(xt, t)−˙αtx0−˙σtϵ∥2\\n2(33)\\n=Et,p1(ϵ),p0(x0)\\r\\r\\r\\r˙αt\\nαtxt+ζtϵθ(xt, t)−˙αtx0−˙σtϵ\\r\\r\\r\\r2\\n2(34)\\n=Et,p1(ϵ),p0(x0)\\r\\r\\r\\r˙αtσt\\nαtϵ+ζtϵθ(xt, t)−˙αtx0−˙σtϵ\\r\\r\\r\\r2\\n2(35)\\n=Et,p1(ϵ),p0(x0)∥ζtϵθ(xt, t)−ζtϵ∥2\\n2(36)\\n=Et,p1(ϵ),p0(x0)h\\nζ'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 13}, page_content='∥ϵθ(xt, t) +σt∇xlogpt(xt)∥2\\n2i\\n(38)\\nRecall that\\nLDM(θ) =Et,p0(x0),p(xt|x0)h\\nλ(t)∥ϵθ(xt, t) +σt∇xlogpt(xt)∥2\\n2i\\n, (39)\\nwe can see that the difference of LDM(θ)andLFM(θ)during training is caused by the weighted\\nfunction, which leaving to different trajectories and properties.\\nB Proof of Convergence of Pseudo Corrector\\nIn this section, we will prove that the proposed pseudo corrector has the same local truncation error\\nand global convergence order as Heun’s method. The detailed sampling procedure of Heun’s method\\n14'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='and pseudo corrector are provided in Algorithm 1 and Algorithm 2. In this section, we use xti'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='to represent the intermediate sampling result at the titimestep, and use x∗\\nti=x(ti)to denote the\\ncorresponding ground-truth value on the trajectory. In all the proofs in this section, we omit the\\ncondition yfor simplicity.\\nB.1 Assumptions\\nAssumption B.1. The velocity predictor vθ(x, t)is Lipschitz continous of constant Lw.r.tx.\\nAssumption B.2. The velocity predictor vθ(x, t)has at least 2 derivativesd\\ndtvθ(x, t)andd2\\ndt2vθ(x, t)\\nand the derivatives are continuous.\\nAssumption B.3. h= max 0≤i≤N−1hi=O(1/N), where Nis the total number of sampling steps.\\nAll the above are common in the analysis of the convergence order of fast samplers [ 22,23,42] of\\ndiffusion models.\\nB.2 Local Convergence\\nWe start by studying the local convergence and Heun’s method. Considering the updating from tito\\nti+1and assume all previous results are correct (see the definition of local convergence [ 15]). The\\nTaylor’s expansion of x∗\\nti+1attigives:\\nx∗\\nti+1=xti+hix(1)(ti) +h2\\ni\\n2x(2)(ti) +h3\\ni\\n6x(3)(ti) +O(h4). (40)\\nOn the other hand, let ¯xti+1be the prediction assuming xiis correct, the updating rule of Heun’s\\nmethod shows:\\n¯xti+1=xti+hi\\n2[di+di+1] (41)\\n=xti+hi\\n2[x(1)(ti) +x(1)(ti) +hix(2)(ti) +h2\\ni\\n2x(3)(ti) +O(h3)] (42)\\n=xi+hix(1)(ti) +h2\\ni\\n2x(2)(ti) +h3\\ni\\n4x(3)(ti) +O(h4) (43)\\nTherefore, the local truncation error can be computed by:\\nTi+1=∥x∗\\nti+1−¯xti+1∥=∥ −h3\\ni\\n12x(3)(ti) +O(h4)∥ ≤C1h3, (44)\\nwhich indicates that Heun’s method has 2 order of accuracy.\\nIt is also noted that the local truncation error of the predictor step (which is the same as Euler’s\\nmethod) can be similarly derived by:\\n˜Ti+1=∥x∗\\nti+1−˜xti+1∥=∥h2\\n2x(2)(ti) +O(h2)∥ ≤C2h2. (45)\\nFor pseudo corrector, the analysis of local convergence is the same since we need to assume all\\nprevious results (including the xtianddi), which means the local truncation error of pseudo corrector\\nis the same as the Heun’s method.\\nB.3 Global Convergence\\nGlobal convergence for Heun’s method. When analyzing global convergence, we need to take\\ninto account both the local truncation error and the effects of the error of previous results. According\\nto the Lipschitz condition, we have:\\n∥x∗\\nti+1−˜xti+1∥ ≤(1 +hL)∥x∗\\nti−xti∥+C2h2(46)\\n15'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='2. In this section, we use xti\\nto represent the intermediate sampling result at the titimestep, and use x∗\\nti=x(ti)to denote the\\ncorresponding ground-truth value on the trajectory. In all the proofs in this section, we omit the\\ncondition yfor simplicity.\\nB.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='Assumption B.1. The velocity predictor vθ(x, t)is Lipschitz continous of constant Lw.r.tx.\\nAssumption B.2. The velocity predictor vθ(x, t)has at least'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='2vθ(x, t)\\nand the derivatives are continuous.\\nAssumption B.3. h= max 0≤i≤N−1hi=O(1/N), where Nis the total number of sampling steps.\\nAll the above are common in the analysis of the convergence order of fast samplers [ 22,23,42] of\\ndiffusion models.\\nB.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='We start by studying the local convergence and Heun’s method. Considering the updating from tito\\nti+1and assume all previous results are correct (see the definition of local convergence [ 15]). The\\nTaylor’s expansion of x∗\\nti+1attigives:\\nx∗\\nti+1=xti+hix(1)(ti) +h'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='6x(3)(ti) +O(h4). (40)\\nOn the other hand, let ¯xti+1be the prediction assuming xiis correct, the updating rule of Heun’s\\nmethod shows:\\n¯xti+1=xti+hi\\n2[di+di+1] (41)\\n=xti+hi\\n2[x(1)(ti) +x(1)(ti) +hix(2)(ti) +h'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='4x(3)(ti) +O(h4) (43)\\nTherefore, the local truncation error can be computed by:\\nTi+1=∥x∗\\nti+1−¯xti+1∥=∥ −h'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='.\\nIt is also noted that the local truncation error of the predictor step (which is the same as Euler’s\\nmethod) can be similarly derived by:\\n˜Ti+1=∥x∗\\nti+1−˜xti+1∥=∥h2\\n2x(2)(ti) +O(h2)∥ ≤C2h2. (45)\\nFor pseudo corrector, the analysis of local convergence is the same since we need to assume all\\nprevious results (including the xtianddi), which means the local truncation error of pseudo corrector\\nis the same as the Heun’s method.\\nB.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='Global convergence for Heun’s method. When analyzing global convergence, we need to take\\ninto account both the local truncation error and the effects of the error of previous results. According\\nto the Lipschitz condition, we have:\\n∥x∗\\nti+1−˜xti+1∥ ≤(1 +hL)∥x∗\\nti−xti∥+C2h2(46)\\n15'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 15}, page_content='and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 15}, page_content='∥x∗\\nti+1−xti+1∥ ≤(1 +hL\\n2)∥x∗\\nti−xti∥+hL\\n2∥x∗\\nti+1−˜xti+1∥+C1h3. (47)\\nCombining the above two inequalities together, we have\\n∥x∗\\nti+1−xti+1∥ ≤(1 +hL+h2L2\\n2)∥x∗\\nti−xti∥+C3h3, (48)\\nwhere C3=hLC 2\\n2+C1. Note that ∥x∗\\nt0−xt0∥= 0 (their is no error at the beginning of the\\nsampling), it can be easily derived that\\n∥x∗\\ntN−xtN∥ ≤C3h2\\nL+hL2\\n2((1 + hL+h2L2\\n2)N−1)≤C4h2(eC5−1) = C6h2. (49)\\nTherefore, we have proven that Heun’s method have 2 order of global convergence.\\nGlobal convergence for pseudo corrector. The only difference between pseudo corrector and\\nHeun’s method is how diis obtained. Pseudo corrector reuse the difrom the last sampling step\\nrather than re-compute it as in Heun’s method. As a result, diused in pseudo corrector is computed\\non˜xtirather than xti, which will lead to another error term when analyzing the global convergence.\\nConcretely, the global error of pseudo corrector can be computed by:\\n∥x∗\\nti+1−˜xti+1∥ ≤(1 +hL)∥x∗\\nti−xti∥+hL∥˜xti−xti∥+C2h2(50)\\n∥x∗\\nti+1−xti+1∥ ≤(1 +hL\\n2)∥x∗\\nti−˜xti∥+hL\\n2∥x∗\\nti+1−˜xti+1∥+hL\\n2∥xti−˜xti∥+C1h3.(51)\\nFor the sake of simplicity, let ˜∆i=∥x∗\\nti−˜xti∥and∆i=∥x∗\\nti−xti∥. Therefore, the above formulas\\nbecomes:\\n˜∆i+1≤∆i+hL˜∆i+C2h2(52)\\n∆i+1≤(1 +hL\\n2)∆i+hL\\n2(1 +hL\\n2)˜∆i+C4h3. (53)\\nBy calculating (52) ×hL+(53) we have:\\n∆i+1+hL˜∆i+1≤(1 +hL\\n2)∆i+hL\\n2(1 +hL\\n2)˜∆i+C4h3+hL∆i+h2L2˜∆i+C2Lh3\\n= (1 +3\\n2hL)\"\\n∆i+hL\\n2+5\\n4h2L2\\n1 +3\\n2hL˜∆i#\\n+C7h3\\n≤(1 +3\\n2hL)(∆i+hL˜∆i) +C7h3.(54)\\nNote that ∆0+hL˜∆0= 0. Let∆′\\ni= ∆ i+hL˜∆i, we have\\n∆′\\ni≤(1 +3\\n2hL)∆′\\ni+C7h3. (55)\\nSimilar to the derivation of (49), we can derive that\\n∆′\\ni≤C8h2((1 +3\\n2hL)N−1)≤C9h2, (56)\\nwhich indicates that\\n∆N≤C9h2, hL ˜∆i+1≤C9h2. (57)\\nTherefore we have ∆N≤C9h2, and thus the global convergence of pseudo corrector is 2-order.\\n16'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 15}, page_content='and\\n∥x∗\\nti+1−xti+1∥ ≤(1 +hL\\n2)∥x∗\\nti−xti∥+hL\\n2∥x∗\\nti+1−˜xti+1∥+C1h3. (47)\\nCombining the above two inequalities together, we have\\n∥x∗\\nti+1−xti+1∥ ≤(1 +hL+h2L2\\n2)∥x∗\\nti−xti∥+C3h3, (48)\\nwhere C3=hLC 2\\n2+C1. Note that ∥x∗\\nt0−xt0∥= 0 (their is no error at the beginning of the\\nsampling), it can be easily derived that\\n∥x∗\\ntN−xtN∥ ≤C3h'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 15}, page_content='+hL2\\n2((1 + hL+h2L2\\n2)N−1)≤C4h2(eC5−1) = C6h2. (49)\\nTherefore, we have proven that Heun’s method have'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 15}, page_content='.\\nGlobal convergence for pseudo corrector. The only difference between pseudo corrector and\\nHeun’s method is how diis obtained. Pseudo corrector reuse the difrom the last sampling step\\nrather than re-compute it as in Heun’s method. As a result, diused in pseudo corrector is computed\\non˜xtirather than xti, which will lead to another error term when analyzing the global convergence.\\nConcretely, the global error of pseudo corrector can be computed by:\\n∥x∗\\nti+1−˜xti+1∥ ≤(1 +hL)∥x∗\\nti−xti∥+hL∥˜xti−xti∥+C2h2(50)\\n∥x∗\\nti+1−xti+1∥ ≤(1 +hL\\n2)∥x∗\\nti−˜xti∥+hL\\n2∥x∗\\nti+1−˜xti+1∥+hL\\n2∥xti−˜xti∥+C1h3.(51)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 15}, page_content='ti−˜xti∥+hL\\n2∥x∗\\nti+1−˜xti+1∥+hL\\n2∥xti−˜xti∥+C1h3.(51)\\nFor the sake of simplicity, let ˜∆i=∥x∗\\nti−˜xti∥and∆i=∥x∗\\nti−xti∥. Therefore, the above formulas\\nbecomes:\\n˜∆i+1≤∆i+hL˜∆i+C2h2(52)\\n∆i+1≤(1 +hL\\n2)∆i+hL\\n2(1 +hL\\n2)˜∆i+C4h3. (53)\\nBy calculating (52) ×hL+(53) we have:\\n∆i+1+hL˜∆i+1≤(1 +hL\\n2)∆i+hL\\n2(1 +hL\\n2)˜∆i+C4h3+hL∆i+h2L2˜∆i+C2Lh3\\n= (1 +3\\n2hL)\"\\n∆i+hL\\n2+5\\n4h2L2\\n1 +3\\n2hL˜∆i#\\n+C7h3\\n≤(1 +3\\n2hL)(∆i+hL˜∆i) +C7h3.(54)\\nNote that ∆0+hL˜∆0= 0. Let∆′\\ni= ∆ i+hL˜∆i, we have\\n∆′\\ni≤(1 +3\\n2hL)∆′\\ni+C7h3. (55)\\nSimilar to the derivation of (49), we can derive that\\n∆′\\ni≤C8h2((1 +3\\n2hL)N−1)≤C9h2, (56)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 15}, page_content='∆′\\ni≤C8h2((1 +3\\n2hL)N−1)≤C9h2, (56)\\nwhich indicates that\\n∆N≤C9h2, hL ˜∆i+1≤C9h2. (57)\\nTherefore we have ∆N≤C9h2, and thus the global convergence of pseudo corrector is 2-order.\\n16'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='Table 4: Ablation of the number of the velocity refiners. We change the number of velocity refiners and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='compare the sampling quality of each configuration. We find there exists a optimal number of velocity refiners\\nto achieve the lowest FID.\\nMethod Sample Config Ratio of Refiner FID ↓ Latency (ms / img)\\nSiT-XL [8], ImageNet (256×256)\\nFlowTurbo H7P10R6 0.26 2.19 100.7\\nFlowTurbo H7P10R5 0.23 2.12 100.3\\nFlowTurbo H7P10R4 0.19 2.18 99.9\\nFlowTurbo H7P10R3 0.15 2.15 99.6\\nFlowTurbo H5P10R6 0.29 2.25 87.2\\nFlowTurbo H5P10R5 0.25 2.20 86.8\\nFlowTurbo H5P10R4 0.21 2.21 86.4\\nFlowTurbo H5P10R3 0.17 2.22 86.0\\nC Implementation Details\\nClass-conditional image generation. We use the SiT-XL-2[ 24] as our base model to perform the\\nexperiments on class-conditional image generation. We use a single block of SiT-XL-2 as the Velocity\\nRefiner. We double the input channel from 4 to 8 to take the previous velocity as input. The resulting\\nvelocity refiner only contains 29M parameters, about 4.3% of the original SiT-XL-2(675M). We use\\nImageNet-1K [ 6]2to train our velocity model. We used AdamW [ 21] optimizer for all models. We\\nuse a constant learning rate of 5×10−5and a batch size of 18 on a single A800 GPU. We used a\\nrandom horizontal flip with a probability of 0.5 in data augmentation. We did not tune the learning\\nrates, decay/warm-up schedules, AdamW parameters, or use any extra data augmentation during\\ntraining. Our velocity refiner (for SiT-XL-2) trains at approximately 4.44 steps/sec on an A800 GPU,\\nand converges in 30,000 steps, which takes about 2 hours.\\nText-to-image generation. We use the 2-RF in InstaFlow [ 20] as our base model to perform the\\nexperiments on text-to-image generation. Since the architecture of the original velocity predictor\\nin [20] is a U-Net [ 30], we cannot directly use a single block of it as the velocity refiner as we do\\nfor SiT [ 24]. Instead, we simply reduce the number of channels in each block from [320, 640, 1280,\\n1280] to [160, 160, 320, 320] and reduce the number of layers in each block from 2 to 1. We also\\ndouble the input channel from 4 to 8 to take the previous velocity as input. The resulting velocity\\nrefiner only contains 43.5M parameters, about 5% of the original U-Net (860M). We use a subset\\nof LAION [ 34]3containing only 50K images to train our velocity model. We use AdamW [ 21]\\noptimizer with a learning rate of 2e-5 and weight decay of 0.0. We adopt a batch size of 16 and set\\nthe warming-up steps as 100. We also use a gradient clipping of 0.01 to stabilize training. We train\\nour model on a single A800 GPU for 10K iterations, which takes about 5.5 hours.\\nImplementation of extension tasks. We have demonstrated our FlowTurbo is also suitable for\\nextension tasks due to the multi-step nature of our framework in Section 4.4. For image inpainting, we\\nadopt the inpainting pipeline in diffusion models4, where we merge the noise latent and the generated\\nlatent at a specific timestep by the input mask. For object removal, we first use a Grounded-SAM5\\nto generate the mask and perform similar image inpainting pipeline. For image editing, we adopt\\nthe SDEdit [ 25] which first adds noise to the original image and use it as an intermediate result to\\ncontinue the sampling.\\nD More Analysis\\nIn this section, we provide more analysis through both quantitative results and qualitative results.\\n2License: Custom (research, non-commercial)\\n3License: Creative Common CC-BY 4.0\\n4https://huggingface.co/docs/diffusers/en/using-diffusers/inpaint\\n5https://github.com/IDEA-Research/Grounded-Segment-Anything\\n17'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='Table 4: Ablation of the number of the velocity refiners. We change the number of velocity refiners and\\ncompare the sampling quality of each configuration. We find there exists a optimal number of velocity refiners\\nto achieve the lowest FID.\\nMethod Sample Config Ratio of Refiner FID ↓ Latency (ms / img)\\nSiT-XL [8], ImageNet (256×256)\\nFlowTurbo H7P10R6 0.26 2.19 100.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='Class-conditional image generation. We use the SiT-XL-2[ 24] as our base model to perform the\\nexperiments on class-conditional image generation. We use a single block of SiT-XL-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='. The resulting\\nvelocity refiner only contains 29M parameters, about 4.3% of the original SiT-XL-2(675M). We use\\nImageNet-1K [ 6]2to train our velocity model. We used AdamW [ 21] optimizer for all models. We\\nuse a constant learning rate of 5×10−5and a batch size of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='. We did not tune the learning\\nrates, decay/warm-up schedules, AdamW parameters, or use any extra data augmentation during\\ntraining. Our velocity refiner (for SiT-XL-2) trains at approximately 4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='.\\nText-to-image generation. We use the 2-RF in InstaFlow [ 20] as our base model to perform the\\nexperiments on text-to-image generation. Since the architecture of the original velocity predictor\\nin [20] is a U-Net [ 30], we cannot directly use a single block of it as the velocity refiner as we do\\nfor SiT [ 24]. Instead, we simply reduce the number of channels in each block from [320, 640, 1280,\\n1280] to [160, 160, 320, 320] and reduce the number of layers in each block from'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='. The resulting velocity\\nrefiner only contains 43.5M parameters, about 5% of the original U-Net (860M). We use a subset\\nof LAION [ 34]3containing only 50K images to train our velocity model. We use AdamW [ 21]\\noptimizer with a learning rate of 2e-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='.\\nImplementation of extension tasks. We have demonstrated our FlowTurbo is also suitable for\\nextension tasks due to the multi-step nature of our framework in Section 4.4. For image inpainting, we\\nadopt the inpainting pipeline in diffusion models4, where we merge the noise latent and the generated\\nlatent at a specific timestep by the input mask. For object removal, we first use a Grounded-SAM'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='. For image editing, we adopt\\nthe SDEdit [ 25] which first adds noise to the original image and use it as an intermediate result to\\ncontinue the sampling.\\nD More Analysis\\nIn this section, we provide more analysis through both quantitative results and qualitative results.\\n2License: Custom (research, non-commercial)\\n3License: Creative Common CC-BY 4.0\\n4https://huggingface.co/docs/diffusers/en/using-diffusers/inpaint\\n5https://github.com/IDEA-Research/Grounded-Segment-Anything\\n17'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='Table 5: Comparisons with state-of-the-art methods on text-to-image generation. We compare'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='our FlowTurbo with state-of-the-art diffusion models (15 steps DPM-Solver++ [ 23]) and show our\\nFlowTurbo enjoys favorable trade-offs between sampling quality and speed.\\nMethod Sample Config FLOPs (G) Latency (ms / img) FID ↓\\nSD2.1 [30] 15 steps DPM++ [23] 11427 286.0 33.03\\nSDXL [29] 15 steps DPM++ [23] 24266 427.2 29.46\\nPixArt- α[4] 15 steps DPM++ [23] 17523 366.8 37.96\\nPixArt- σ[3] 15 steps DPM++ [23] 17957 365.9 33.62\\nFlowTurbo H1P6R3 4030 104.8 28.60\\nFlowTurbo H3P6R3 5386 137.0 27.60\\nD.1 More Quantitative Results\\nAblation of the number of the velocity refiners. In Table 4, we investigate how to choose the\\nnumber of velocity refiners to get a better sampling quality. We adopt two basic configurations of\\nH7R10andH5R10, and vary the number of velocity refiners from 3 to 6. We find that the FID will\\nfirst decrease and then increase when NRbecomes larger, and there exists an optimal NR= 5where\\nwe reach the lowest FID. These results indicate that we can always tune this hyper-parameter to\\nexpect a better result.\\nMore comparisons on text-to-image generation. In Table Table 5, we compare the sampling\\nquality and speed of FLowTurbo with state-of-the-art diffusion models on text-to-image generation.\\nFor all the diffusion models, we adopt a 15-step DPM-Solver++ [ 23] as the default sampler. The\\nFLOPs reported also take the multi-step sampling into account. Our results show that our FlowTurbo\\ncan achieve the lowest FID and inference latency.\\nD.2 More Qualitative Results\\nTo better illustrate the sampling quality of our FlowTurbo, we provide more qualitative results on\\nboth class-conditional image generation and text-to-image generation.\\nClass-conditional image generation. We use SiT-XL [ 24] as our flow-based model for class-\\nconditional image generation. In Figure 5, we provide random samples from FlowTurbo of the\\nsample config H8P9R5, which inference at 100 ms/img. We also demonstrate the sampling quality\\ntrade-offs in Figure 6, we compare the sampling quality of two different configurations H1P5R3(38\\nms / img) and H8P9R5(100 ms / img). We generate the images from the same initial noise for better\\ncomparisons. Our result demonstrates that our FlowTurbo can achieve real-time image generation,\\nand the sampling quality can be further improved with more computational budgets.\\nText-to-image generation. We adopt Lumina-Next-T2I [ 9] to achieve text-to-image generation.\\nWe compare the sampling quality and speed of Heun’s method and our FlowTurbo in Figure 7. We\\nfind that FlowTurbo can consistently generate images with better quality and more visual details,\\nwhile requiring less inference time.\\nE Code\\nOur code is implemented in PyTorch6. We use the codebase of [ 24] to conduct experiments. The\\ncode is available at https://github.com/shiml20/FlowTurbo .\\n6https://pytorch.org\\n18'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='Table 5: Comparisons with state-of-the-art methods on text-to-image generation. We compare\\nour FlowTurbo with state-of-the-art diffusion models ('),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='-Solver++ [ 23]) and show our\\nFlowTurbo enjoys favorable trade-offs between sampling quality and speed.\\nMethod Sample Config FLOPs (G) Latency (ms / img) FID ↓\\nSD2.1 [30]'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='Ablation of the number of the velocity refiners. In Table 4, we investigate how to choose the\\nnumber of velocity refiners to get a better sampling quality. We adopt two basic configurations of\\nH7R10andH5R10, and vary the number of velocity refiners from'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='6. We find that the FID will\\nfirst decrease and then increase when NRbecomes larger, and there exists an optimal NR= 5where\\nwe reach the lowest FID. These results indicate that we can always tune this hyper-parameter to\\nexpect a better result.\\nMore comparisons on text-to-image generation. In Table Table 5, we compare the sampling\\nquality and speed of FLowTurbo with state-of-the-art diffusion models on text-to-image generation.\\nFor all the diffusion models, we adopt a 15-step DPM-Solver++ [ 23] as the default sampler. The'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='FLOPs reported also take the multi-step sampling into account. Our results show that our FlowTurbo\\ncan achieve the lowest FID and inference latency.\\nD.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='To better illustrate the sampling quality of our FlowTurbo, we provide more qualitative results on\\nboth class-conditional image generation and text-to-image generation.\\nClass-conditional image generation. We use SiT-XL [ 24] as our flow-based model for class-\\nconditional image generation. In Figure 5, we provide random samples from FlowTurbo of the\\nsample config H8P9R5, which inference at 1'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='/img. We also demonstrate the sampling quality\\ntrade-offs in Figure 6, we compare the sampling quality of two different configurations H1P5R3('),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='/ img). We generate the images from the same initial noise for better\\ncomparisons. Our result demonstrates that our FlowTurbo can achieve real-time image generation,\\nand the sampling quality can be further improved with more computational budgets.\\nText-to-image generation. We adopt Lumina-Next-T2I [ 9] to achieve text-to-image generation.\\nWe compare the sampling quality and speed of Heun’s method and our FlowTurbo in Figure 7. We\\nfind that FlowTurbo can consistently generate images with better quality and more visual details,\\nwhile requiring less inference time.\\nE Code'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='while requiring less inference time.\\nE Code\\nOur code is implemented in PyTorch6. We use the codebase of [ 24] to conduct experiments. The\\ncode is available at https://github.com/shiml20/FlowTurbo .\\n6https://pytorch.org\\n18'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 18}, page_content='Figure 5: Random samples from FlowTurbo on ImageNet 256 ×256. We use a classifier-free'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 18}, page_content='guidance scale of 4.0 and the sample config of H8P9R5(100 ms / img )\\n19'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 18}, page_content='Figure 5: Random samples from FlowTurbo on ImageNet 256 ×256. We use a classifier-free\\nguidance scale of 4.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 19}, page_content='(a) Sample Config H1P5R3(38 ms / img )'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 19}, page_content='(b) Sample Config H8P9R5(100 ms / img )\\nFigure 6: Uncurated 256 ×256 samples from FlowTurbo (CFG = 4.0). For better visualization. We\\ncompare two sample configurations ( H1P5R3andH8P9R5). The same initial noise is used for both\\nsample configurations for better comparisons.\\n20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 19}, page_content='(CFG = 4.0). For better visualization. We\\ncompare two sample configurations ( H1P5R3andH8P9R5). The same initial noise is used for both\\nsample configurations for better comparisons.\\n20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 20}, page_content='A beautiful panorama from inside a camper with beautiful beach and cliffs'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 20}, page_content=\"The moon, a silver boat, sails through the sea of stars\\n A single brown bird perched on a mossy branch\\n Digital watercolor of a summer scape sunset, with flowery pastel colors\\nA legendary fruit castle in a fruit kingdom\\nInside the glass sphere, Pirate Ship in a storm with waves in the dark\\nA beautiful girl flying through a paradox with piercing eyes \\nUnderwater landscape with colorful mechanical parts, cable, wires\\nA cute kitten wearing a witch's robe and hat, holding a magic book\\nA tree house on a beautiful beach surrounded by mountains and waterfalls\\n A sailboat in the ocean with a moon in the sky\\nA rough linen knightess, wielding dual axes resembling two moonsFigure 7: More visual comparisons between Heun’s method (2.6 s / img, left) and our FlowTurbo\\n(1.8 s / img, right ).\\n21\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 20}, page_content=\"A beautiful panorama from inside a camper with beautiful beach and cliffs\\n The moon, a silver boat, sails through the sea of stars\\n A single brown bird perched on a mossy branch\\n Digital watercolor of a summer scape sunset, with flowery pastel colors\\nA legendary fruit castle in a fruit kingdom\\nInside the glass sphere, Pirate Ship in a storm with waves in the dark\\nA beautiful girl flying through a paradox with piercing eyes \\nUnderwater landscape with colorful mechanical parts, cable, wires\\nA cute kitten wearing a witch's robe and hat, holding a magic book\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 20}, page_content='A tree house on a beautiful beach surrounded by mountains and waterfalls\\n A sailboat in the ocean with a moon in the sky\\nA rough linen knightess, wielding dual axes resembling two moonsFigure 7: More visual comparisons between Heun’s method (2.')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents_by_sections(documents: list):\n",
    "    # List of section keywords\n",
    "    section_keywords = [\n",
    "        'Introduction', 'Methodology', 'Results', 'Discussion', 'Conclusion', 'Related Work', 'References'\n",
    "    ]\n",
    "    \n",
    "    # Join the section keywords into a regex pattern\n",
    "    section_keywords_pattern = r\"|\".join(section_keywords)\n",
    "    \n",
    "    # Regex for sections with a number and one of the section keywords\n",
    "    section_regex = fr\"(\\n\\d{{1,2}}\\s+({section_keywords_pattern}))\"\n",
    "    \n",
    "    all_splits = []\n",
    "    \n",
    "    for document in documents:\n",
    "        text = document.page_content\n",
    "        \n",
    "        # Split by sections using the regex pattern\n",
    "        sections = re.split(section_regex, text)\n",
    "        \n",
    "        for section in sections:\n",
    "            # Skip sections that are too small\n",
    "            if len(section.strip()) < 100:\n",
    "                continue\n",
    "            \n",
    "            # Further split the section into smaller chunks if needed\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=800, \n",
    "                chunk_overlap=100, \n",
    "                length_function=len\n",
    "            )\n",
    "            \n",
    "            # Split the section into smaller chunks\n",
    "            split_chunks = text_splitter.split_text(section)\n",
    "            all_splits.extend(split_chunks)\n",
    "    \n",
    "    return all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=60,\n",
    "        length_function=len,          \n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 0}, page_content='Multi-View and Multi-Scale Alignment for Contrastive\\nLanguage-Image Pre-training in Mammography\\nYuexi Du1, John Onofrey1,2,3, Nicha C. Dvornek1,2\\n1Department of Biomedical Engineering,\\n2Department of Radiology & Biomedical Imaging,3Department of Urology,\\nYale University, New Haven, CT, USA\\nAbstract\\nContrastive Language-Image Pre-training (CLIP) shows promise in medical image\\nanalysis but requires substantial data and computational resources. Due to these\\nrestrictions, existing CLIP applications in medical imaging focus mainly on modal-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 0}, page_content='ities like chest X-rays that have abundant image-report data available, leaving many\\nother important modalities under-explored. Here, we propose the first adaptation of\\nthe full CLIP model to mammography, which presents significant challenges due to\\nlabeled data scarcity, high-resolution images with small regions of interest, and data\\nimbalance. We first develop a specialized supervision framework for mammogra-\\nphy that leverages its multi-view nature. Furthermore, we design a symmetric local\\nalignment module to better focus on detailed features in high-resolution images.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 0}, page_content='Lastly, we incorporate a parameter-efficient fine-tuning approach for large lan-\\nguage models pre-trained with medical knowledge to address data limitations. Our\\nmulti-view and multi-scale alignment (MaMA) method outperforms state-of-the-art\\nbaselines for three different tasks on two large real-world mammography datasets,\\nEMBED and RSNA-Mammo, with only 52% model size compared with the largest\\nbaseline. The code is available at https://github.com/XYPB/MaMA .1\\n1 Introduction\\nContrastive learning [ 5,17,16] has become one of the most popular self-supervised representation'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 0}, page_content='learning paradigms due to its intuitive concept and robust performance. Contrastive learning removes\\nthe reliance on a supervised signal by optimizing the semantic distance for similar pairs in the\\nrepresentation space in a contrastive manner. More recently, the introduction of natural language\\nsignals to contrastive learning [ 38] has given rise to modern visual-language models [ 26,25,29].\\nContrastive Language-Image Pre-training (CLIP) [ 38] has also been widely applied in the medical\\nimaging domain [ 53,19,51,56,54,55,15] and shows promising improvement in medical image'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 0}, page_content='understanding when large-scale medical imaging datasets are available [ 22,20,15,55]. However, the\\nCLIP model in the natural image domain usually demands more than hundreds of millions of image-\\ntext pairs to be properly trained [ 38,44,46,45], which is almost impossible in the medical domain\\ndue to privacy and security concerns. Existing medical CLIP methods either build general-purpose\\nCLIP models with multiple anatomical sites and modalities from public online databases [ 15,55]\\nor focus on imaging modalities with large-scale (less than a million) datasets, e.g., chest X-ray or'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 0}, page_content='pathology images [ 56,19,51,54,53,60,52,50,24]. This means other imaging modalities, such as\\nmammography, have yet to fully benefit from such visual-language pre-trained models.\\nMammography is a critical medical imaging modality for breast cancer screening and diagnosis,\\nas breast cancer is one of the most commonly diagnosed cancers globally and a leading cause\\n1This work is also the basis of the overall best solution for the MICCAI 2024 CXR-LT Challenge.\\nPreprint. Under review.arXiv:2409.18119v1  [cs.CV]  26 Sep 2024'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='ℒ𝑉𝑇\\nImage𝑓𝑉\\nText𝑓𝑇𝑣 𝑡\\n𝒯1 𝒯2ℒ𝑉𝑇\\nImage Text𝑓𝑇 𝑓𝑉𝑣2 𝑣 𝑣1 𝑡ℒ𝑉𝑉\\nText𝑓𝑇𝑡\\n𝑓𝑉\\nMulti -view Images𝑣 \\u0de4𝑣ℒ𝑉𝑇 ℒ𝑉𝑉ℒ𝑉𝑇\\n𝒯1 𝒯2\\n(a) CLIP Style (b) SLIP (c) MaMA  (Ours)Figure 1: Comparison of Three Visual-Language Contrastive Learning Frameworks . (a)\\nCLIP [ 38] style; (b) SLIP [ 34] style; (c) Proposed MaMA that aligns image-image and image-text\\nfeatures, exploiting the multi-view nature of mammography and aligning images from the same study.\\nof cancer-related mortality in women [ 47]. While visual-language pre-training (VLP) has the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='potential to improve mammography interpretation, there are two major obstacles: 1) Limited data\\nand annotation : Recent work has introduced a large-scale mammography image and tabular dataset\\nof more than 110,000 patients, i.e., EMBED [ 21], but no corresponding clinical reports are available.\\n2)Nature of mammography : Different from the single view natural image or chest X-ray, each\\nmammography study usually contains four high-resolution ( ∼2,000-by-2,000 pixels) views of the\\nsame patient: left and right side, each with craniocaudal (CC) and mediolateral oblique (MLO)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='views. Such multi-view mammography has the critical properties of bilateral asymmetry [12] and\\nipsilateral correspondence [32]. Bilateral asymmetry means images from different sides of the same\\npatient can contain different information, e.g., density, calcification, and mass findings. Ipsilateral\\ncorrespondence means different views of the same side share similar information from different\\nviewpoints. Clinicians consider both properties and all four images at once as a cross reference\\nwhen reading a study. Meanwhile, lesions of interest are often relatively small compared with'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='high-resolution mammograms, which further challenges a model’s ability to focus on local details.\\nThis pixel-level imbalance compounds the problem of image-level imbalance, in which the vast\\nmajority of mammograms will not contain cancer. While one recent work [ 6] attempts to address\\nthese issues by leveraging VLP, they take a fine-tuning approach in which they simply adopt a\\npre-trained CLIP model and perform supervised fine-tuning to a zero-shot classification task for\\nmulti-view mammography, rather than capitalizing on the mammography domain information to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='perform contrastive language-image pre-training for mammography representation learning [58].\\nTo address these challenges, we propose a novel Multi-view and Multi-scale Alignment i.e., MaMA,\\ncontrastive language-image pre-training framework that exploits the multi-view property of mam-\\nmography and aligns multi-scale features simultaneously. We first propose an intuitive method for\\ntemplate-based report construction from tabular data to resolve the lack of clinical reports and enable\\nVLP. The proposed model then optimizes both multi-view image-image and symmetric text-image'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='contrastive loss simultaneously (Fig. 1), learning the correspondence between the multi-view images\\nand image-report relationship from the same mammography study. We then propose a novel symmet-\\nric local alignment module that actively learns sentence-patch relationships by computing similarity\\nscores for each image-text pair. We also incorporate parameter-efficient fine-tuned large-language\\nmodels (LLMs) pre-trained with medical domain knowledge to improve the understanding of the\\nreport while addressing data scarcity. We validate our method on two large-scale mammography'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='datasets, EMBED [ 21] and RNSA-Mammo [ 4], with multiple settings compared with state-of-the-art\\nmedical CLIP methods. The proposed method surpasses all the baselines with a considerable gap\\nwith only 52% model size, showing promise on multiple mammography-related tasks.\\n2 Related Works\\nMedical Visual-Language Pre-training Existing medical VLP methods can be divided into\\ntwo types depending on the training data. The first type is the general-purpose medical CLIP\\nmodel trained with a large-scale medical-image dataset with multiple anatomical sites and imaging'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 1}, page_content='modalities derived from PubMed [ 15,55]. This approach mainly focuses on scaling dataset size\\nwhile using a vanilla CLIP design [ 38]. These models show promising generalization ability on\\nmultiple sites but are often suboptimal compared with modality-specific models due to the lack of a\\nspecific design for the individual image modality. The other type of VLP models mainly focuses on\\n2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='(b) Symmetric Local Alignment (SLA)#Patches\\n⋯\\n⋯\\n⋯Text \\nlocalization#Sentences⋯\\nVisual \\nlocalization\\nℒ\\u0bdf\\u0be2\\u0bd6\\u0bd4\\u0bdf=1\\n2(ℒ\\u0bdf\\u0be2\\u0bd6\\u0bd4\\u0bdf்+ℒ\\u0bdf\\u0be2\\u0bd6\\u0bd4\\u0bdf\\u0bcf)Avg.Avg.\\n(a) MaMA𝐶\\u0bdc,\\u0bdc\\n𝑐\\u0bdc,\\u0bdc்𝑐\\u0bdc,\\u0bdc\\u0bcf𝑝\\u0bdc𝑠\\u0bdc\\nText\\nEncoder\\n𝑓்Image \\nEncoder\\n𝑓\\u0bcf⋯ ⋯ ⋯ℒ\\u0bcf்ℒ\\u0bcf்ℒ\\u0bcf\\u0bcf\\nIntra-Study\\nRandom SamplingRandom\\nMeta Masking⋯\\n⋯SLA\\nStructured Mammo. \\nReport Constructor\\n‘[BOS]Procedure reported : MG Diagnostic Bilateral \\nw/ CAD [SEP] Reason for procedure: diagnostic \\n[SEP]Patient info : …[SEP] Image info : …[SEP]\\nBreast composition : …[SEP] Findings : …[SEP]\\nImpression : BI-RADS Category 5: highly suggestive \\nof malignancy [SEP] Overall Assessment : Highly'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='of malignancy [SEP] Overall Assessment : Highly \\nsuggestive of malignancy [SEP][EOS]’Study-level\\nTabular Annotation\\nMulti-view Images\\n𝑔\\u0bcf(⋅)\\n𝑔்(⋅)[SEP] Tokensℎ\\u0bcf(⋅)𝑔\\u0bcf(⋅)\\nℎ்(⋅)\\n⋯⋯\\nAvg.Avg.Avg.𝑣\\u0de4\\u0bdc\\n𝑣\\u0bdc\\n𝑡\\u0bdc𝑥\\u0bdc\\n𝑦\\u0bdc\\nData Augmentation𝑥\\u0de4\\u0bdcPairedFigure 2: Proposed Multi-view and Multi-scale (MaMA) VLP Framework . (a) We utilize\\nthe multi-view information of mammography to conduct symmetric image-image and image-text\\ncontrastive learning. (b) We localize the most relevant sentence for each image patch and the most'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='relevant patch for each sentence and align these matched local features via symmetric local alignment.\\nchest X-ray [ 56,19,51,54,53,60,52,50] due to the availability of large datasets, trained on either\\nMIMIC-CXR [ 22] or CheXpert [ 20] datasets. While these methods show impressive performance\\non chest-specific tasks, they are specially designed for single-view images like regular CLIP [ 38].\\nSome of the methods further require full clinical reports paired with the image [ 51,50,60], which\\nmakes them harder to adopt. Recently, Chen et al. [6] proposed a first attempt to introduce CLIP to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='mammography. It fine-tunes a pre-trained CLIP model with an added multi-view image aggregation\\nmodule to a zero-shot classification task. However, the method does not perform contrastive pre-\\ntraining, ignores pixel-level data imbalance, and cannot correlate the medical report with fine-grained\\nROIs. Furthermore, they only fine-tuned a pre-trained CLIP model with a few thousand private cases.\\nMulti-view Contrastive Learning To obtain a more robust self-supervised contrastive learning\\nframework, methods like SLIP [ 34] (Fig. 1 (b)) and DeCLIP[ 27] exploit image-image contrastive'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='learning along with image-text contrastive learning simultaneously. Such ideas have been applied to\\n3D shape recognition [ 9,42] by exploiting the nature of 3D shapes from different viewpoints and also\\nto the action recognition task in the real world [ 40]. These methods all exploit the multi-view nature\\nof the specific image modality, where images of the same object from different viewpoints share the\\nsame semantic meaning while having different appearances. Multi-view contrastive learning has\\nalso been utilized in mammography [ 28,14,43], where the multi-view consistency is leveraged to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='actively learn high-level shared information within the multi-view mammography. However, to the\\nbest of our knowledge, none of the existing works combine multi-view mammography contrastive\\nlearning with CLIP to fully utilize the supervising signal from the multimodal data.\\nUnsupervised Local Contrastive Learning Correlating a dense visual representation with fine-\\ngrained semantic meaning is not only helpful for image understanding but vital to tasks like semantic\\nsegmentation. Recent work address this problem in the challenging unsupervised scenario [ 19,51,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 2}, page_content='59,52,31,57,40,30]. Some methods rely on a pre-trained object detector or segmentation model to\\nextract the region of interest [ 57]. Other methods either aggregate dense similarity scores and conduct\\nimage-level contrastive learning [ 59,52,30], which may ignore too much visual information during\\ntraining, or exhaustively conduct token-level language-image matching and optimize patch-level\\ncontrastive loss [19, 51, 40], with the cost of additional computation.\\n3'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='3 Method\\nIn this section, we introduce the proposed MaMA (Fig. 2). We begin with the construction of the\\nstructured mammography report from the tabular data. We then introduce the multi-view contrastive\\nimage-text pre-training framework, followed by the proposed symmetric local alignment (SLA).\\n3.1 Structured Report Construction\\nDifferent from chest X-ray datasets that provide paired images with corresponding clinical reports,\\ne.g., MIMIC-CXR [ 22], large-scale mammography datasets with the full report available are rare.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='Rather, existing datasets in this domain [ 21,4,35] mainly provide a tabular structure annotation\\nincluding both the anonymized meta information as well as the clinical findings, e.g., breast density\\ntype, calcification findings, tumor description, and Breast Imaging Reporting and Data System\\n(BI-RADS) assessment category [ 41]. Clinical findings serve as cross-validation evidence for the\\nfinal diagnosis. Using a CLIP-style [ 58] caption with only the simple class label for cancer will result'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='in a highly simplified caption and limit the model’s understanding of the image due to missing details.\\nWe propose a template-based caption construction method following the standard clinical report\\nstructure [ 36] (Fig. 2 (a)). We first create a report template with segments describing study procedure ,\\npatient meta-information ,image meta-information ,breast composition ,findings ,clinical impression\\nand the final overall assessment in a natural language report style. Each segment contains keywords'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='that can be replaced with the corresponding meta-information in the tabular data. By replacing\\nthese keywords and concatenating these segments, we can build a complete clinical report for each\\nspecific image, and provide more details for language-image contrastive learning. We provide the full\\ntemplate and a few image-caption examples in the appendix.\\nMeta-Info Masking The increased information from patient and image-specific meta-data may\\nbe memorized by the model during the contrastive training and result in learning shortcuts for the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='model decision. To focus more on the diagnosis and disease-related information, we propose a data\\naugmentation method that randomly masks each patient or image meta-information keyword with a\\nprobability of mwhen constructing the caption.\\n3.2 Multi-view VLP\\nWe introduce the multi-view contrastive VLP framework here. Let D={(xi, yi), i= 0,1, . . . , N }\\nbe a multimodal dataset, where there are Nindividual images xiand corresponding text captions yi.\\nOur framework optimizes both image-to-image and symmetric image-to-text contrastive loss.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='Multi-view Visual Contrastive Loss We first optimize the contrastive loss within the multi-view\\nimages (Fig. 2 (a)). We define a study to include the data from the same imaging session for a patient,\\nincluding one or more image-text pairs. For a random image-text pair (xi, yi)from the dataset D, we\\nuniformly sample another image ˜xifrom the same study that xibelongs to as the positive sample\\nofxi. Note that ˜xicould be xias the augmented view of the same image is naturally a positive\\nsample. We augment both images with random data augmentation and then feed into the vision'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='encoder fVandd-dimensional global embedding projection head gVfollowed by average pooling to\\nget corresponding visual embedding vi,˜vi∈Rd,i.e.,vi= avg( gV(fV(xi))). We then compute the\\ncosine similarity for each pair of visual embeddings and optimize the InfoNCE [ 5] loss for viin a\\nmini-batch of size B:\\nLV V(vi,˜vi) = logexp(sim(vi,˜vi)/τ1)PB\\nj=1exp(sim(vi, vj)/τ1),where sim(vi, vj) =vT\\nivj\\n∥vi∥∥vj∥, (1)\\nwhere τ1is the visual temperature constant and vjis the j-th visual embedding in the batch. Since'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='two views of the same side of a study have ipsilateral correspondence, it is natural to treat them as\\npositive samples of each other, as the features, like tumors, present in one view, are often present\\nin the other view as well. On the other hand, even if considering bilateral asymmetry for images\\nfrom different sides, they still share much high-level information such as patient-level features (e.g.,\\nglobal breast shape similarity, age) and similar breast density. Introducing multi-view mammography'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 3}, page_content='contrastive learning forces the model to learn semantically similar features from images within the\\nsame study. This also provides a stronger self-supervised signal than using random augmented images.\\nOur image-to-image contrastive learning framework follows the design of SimCLR [ 5] for simplicity.\\n4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='Symmetric Visual-Text Contrastive Loss While existing methods like SLIP [ 34] also optimize\\nboth image-image and image-text contrastive loss, we note there is a potential contradiction between\\nimage-image and image-text objectives when computed for different examples (Fig. 1 (b)), i.e.,LV V\\nandLV Tare independent and the extra image will introduce unnecessary memory cost. To address\\nthis, we propose re-using viwhen optimizing LV Tand symmetrically optimizing this loss.\\nWe feed caption yito the tokenizer and text encoder fTand then the text global projection head gT'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='with average pooling to get the text embedding ti∈Rd. We optimize the CLIP [ 38] loss (Fig. 2 (a)):\\nLV T(vi, ti) =−1\\n2(logexp(sim(vi, ti)/τ2)PB\\nj=1exp(sim(vi, tj)/τ2)+ logexp(sim(ti, vi)/τ2)PB\\nj=1exp(sim(ti, vj)/τ2)),(2)\\nwhere τ2is the learnable language temperature constant. We compute LV Tfor both viand˜vifor the\\nsame tisymmetrically. Namely, we minimize the semantic distance between two images from the\\nsame view and the corresponding report simultaneously. We note that even if the information in yiis'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='not completely matched with ˜xi,e.g., different side and view caption, they still share a large overlap\\nin patient-level information. This encourages the model to mine the high-level shared patient-related\\nfeatures via minimizing LV T(˜vi, ti)while focusing on diagnosis-related information by minimizing\\nLV T(vi, ti).\\n3.3 Symmetric Local Alignment (SLA)\\nMammography usually contains high-frequency details and the region of interest is usually very small.\\nThese properties require a higher image resolution for the deep learning method to work properly. It'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='also challenges the model’s ability to extract important local information and filter out less meaningful\\nbackground and tissue unrelated to diagnosis. To address these challenges, we propose a symmetric\\nlocal alignment (SLA) module. Specifically, the SLA module allows the model to determine the local\\ncorrespondence relationship between each sentence and image patch (Fig. 2 (b)).\\nWe start with extracting local features from input (xi, yi). We feed the image and caption to the\\nvision encoder fVand text encoder fTrespectively, followed by corresponding local projection head'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='hVandhTwithout pooling to produce output feature sequence vlocal\\ni∈RNV×dandtlocal\\ni∈RNT×d,\\nwhere NVandNTare the length of visual tokens and text tokens, respectively. We then extract\\nsentence-level features by selecting the embedding corresponding to the [SEP] token, which results\\nin a sequence of sentence embeddings si∈RS×d, where Sis the number of sentences. We extract\\nthe image patch-level features by removing the extra functional tokens like [CLS] tokens, resulting\\nin a sequence of patch embeddings pi∈RP×d, where Pis the number of patches. We then compute'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='the sentence-patch correspondence matrix Ci,i∈RS×Pin the form of cosine similarity, which\\nreveals the relationship between local patches and each sentence in the report. However, we cannot\\ndirectly supervise the learning of this matrix since we have no access to the local correspondence\\nbetween the image and the report. Thus, we aggregate the patch-sentence level correspondence matrix\\nCi,ito an image-report level similarity score. We start by localizing the patch that has the highest\\ncorrespondence for each sentence. Namely, we find the most relevant region in the image for each'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='sentence. We call this process Visual Localization. We then average the similarity score for each\\nsentence to obtain a correspondence score which describes the similarity of the most relevant patch\\nfor the whole report cV\\ni,i=1\\nSP\\njmaxkCi,i(j, k), where Ci,i(j, k)is the similarity between the j-th\\nsentence and the k-th patch. Similarly, we conduct Text Localization by finding the most similar\\nsentence feature for each patch and averaging it to get a score for the similarity of the most relevant\\nsentence for the whole image cT\\ni,i=1\\nPP\\nkmaxjCi,i(j, k). We compute the aggregated visual and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='PP\\nkmaxjCi,i(j, k). We compute the aggregated visual and\\ntext localization scores for all pandsin the mini-batch and optimize the InfoNCE [17] loss:\\nsLV\\nlocal(i) =−1\\n2(exp(cV\\ni,i/τlocal)\\nPB\\nj=1exp(cV\\ni,j/τlocal)+exp(cV\\ni,i/τlocal)\\nPB\\nj=1exp(cV\\nj,i/τlocal)), (3)\\nandLT\\nlocal is defined similarly, where τlocal is the local temperature constant. The final local loss\\nwill then be Llocal=1\\n2(LV\\nlocal+LT\\nlocal). We note that introducing this local loss from the beginning\\nof the training can lead to unstable behavior as the initial visual and language embeddings are not'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 4}, page_content='aligned. Thus, we add this loss after ksteps of training.\\nThe intuition behind this design is to mimic the process of radiologic interpretation of a medical\\nimage in the real world. On the one hand, in mammography, the clinician will look for the image\\n5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='Table 1: Linear Classification Results on EMBED [ 21]We evaluate linear classification results\\nwith different amounts of fine-tuning data for both BI-RADS and density prediction tasks. We report\\nboth balanced accuracy (bACC) and AUC metrics. The best and second-best results are highlighted\\nin bold and underlined, respectively. Our method is shaded in gray.\\nModelsEMBED BI-RADS [21] EMBED Density [21]\\nbACC (%) AUC (%) bACC (%) AUC (%)\\n1% 10% 100% 1% 10% 100% 1% 10% 100% 1% 10% 100%\\nVision only\\nRandom-ViT [13] 20.84 20.68 22.10 57.15 61.54 61.76 45.81 45.11 47.01 72.83 72.62 72.92'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='DiNOv2-ViT [37] 22.63 25.17 29.33 61.83 66.00 70.11 66.71 70.80 71.20 89.18 90.46 90.47\\nDeiT-based [48]\\nCLIP [38] 19.33 21.97 22.26 55.52 61.02 61.65 48.95 50.33 50.77 75.41 76.31 76.92\\nConVIRT [56] 25.08 27.63 29.56 65.43 70.49 71.54 72.66 73.46 73.53 91.69 92.11 92.10\\nMGCA [51] 24.17 27.28 28.09 65.18 71.08 71.49 74.03 74.49 74.53 91.80 92.25 92.21\\nDiNOv2-based [37]\\nCLIP [38] 26.66 31.65 34.35 70.35 74.98 74.11 74.64 75.00 75.97 91.50 90.62 92.39\\nSLIP [34] 22.94 27.86 30.93 64.43 69.48 71.95 73.24 74.79 75.23 91.56 92.37 92.46'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='MM-MIL [52] 25.85 30.94 35.11 67.16 71.99 76.12 74.23 76.69 75.77 91.96 93.34 91.65\\nConVIRT [56] 24.62 30.38 31.27 65.09 73.33 74.03 74.34 74.95 74.74 92.21 92.56 92.58\\nMGCA [51] 23.66 30.11 30.27 64.19 72.24 72.54 71.43 72.25 72.20 90.83 91.21 91.24\\nMaMA 28.46 35.12 39.75 70.63 75.98 77.50 76.26 78.11 78.09 93.11 93.62 93.65\\nregions and local features that appear most suspicious for cancer. On the other hand, the clinician\\nwill write the radiology report in a few sentences based on the findings across the whole image, while'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='matching each description with a specific feature of the image. Our proposed SLA gives the model\\nthe ability to perceive fine-grain local image detail with sentence-level description. The derived local\\nsimilarity map could also be used as a guide of the relevance between specific image details and each\\nsentence in the provided report and therefore improve the interpretability of the model.\\n3.4 LLM with PEFT as Text Encoder\\nLastly, we incorporate parameter-efficient fine-tuning (PEFT) of a pre-trained large language model'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='(LLM) as our text encoder ( e.g., BioMedLM [ 3]) rather than use a small pre-trained BERT encoder [ 2].\\nUsing a pre-trained LLM with strong domain knowledge can help improve the model’s understanding\\nof the text caption and provide a more robust supervised signal for the visual-language pre-training.\\nMoreover, PEFT ( e.g., LoRA [18]) can greatly reduce the cost of adapting LLM to scenarios with a\\nshortage of computing resources while maintaining a strong performance after fine-tuning. Adapting\\nan LLM with PEFT thus has the potential to greatly improve performance while reducing trainable'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='parameters and GPU memory costs compared to learning the commonly adopted BERT-style encoder.\\n4 Experiments\\n4.1 Pre-training Settings\\nDataset We pre-trained our model on the Emory EMBED [21] dataset, which is a large-scale\\nmammography dataset with public access and one of the largest open mammography datasets\\navailable. The current release contains 72,768 multi-view mammography studies for 23,356 patients\\ncollected from 4 hospitals. We focus on 2D mammography, which has 364,564 individual images\\nin total. The dataset provides tabular annotation about the patient, imaging meta-information, and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='corresponding image-level findings including breast density, BI-RADS assessment, and calcification\\nfindings. We split the dataset by patient into train/validation/test partitions, each with 70%/10%/20%\\nimages. All the images are resized and padded to 518×518without changing the aspect ratio.\\nImplementation Details We choose to use DiNOv2-ViT-B-14 [ 37] and BioMedLM [ 3] as our\\nimage and text encoder respectively. We adapt LoRA [ 18] to the text encoder to fine-tune it efficiently.\\nWe choose DiNOv2 [ 37] ViT as it is pre-trained with a larger image size which is suitable for'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 5}, page_content='mammography. Note that our method does not depend on a specific text encoder design. We also\\nreport the performance of our model with a more common BioClincialBERT [ 2] encoder. The meta\\nmasking ratio mis 0.8 during training. We train our model with the AdamW optimizer [ 33] using a\\nlearning rate of 4E-5, weight-decay of 0.1, and cosine annealing scheduler for 40k steps. We also\\nadapt warm-up from 1E-8 for 4k steps. The SLA loss is added after k= 8k steps. We use a batch\\n6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='Table 2: Zero-shot and Full Fine-tuning Results on EMBED [ 21]We evaluate zero-shot and fully\\nfine-tuned classification results for both BI-RADS and density prediction tasks. We report balanced\\naccuracy (bACC) and AUC. The best and second-best results are highlighted in bold and underlined,\\nrespectively. Our method is shaded in gray.\\nModelsEMBED BI-RADS [21] EMBED Density [21]\\nZero-shot Full Fine-tune Zero-shot Full Fine-tune\\nbACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%)\\nDeiT-based [48]\\nCLIP [38] 23.86 67.08 25.05 63.43 71.72 91.52 71.90 89.74'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='CLIP [38] 23.86 67.08 25.05 63.43 71.72 91.52 71.90 89.74\\nConVIRT [56] 23.72 62.85 31.80 72.82 64.61 86.62 77.07 93.34\\nMGCA [51] 22.73 62.24 33.05 74.20 68.47 87.86 77.29 93.47\\nDiNOv2-based\\nCLIP [38] 23.05 59.81 34.25 71.61 73.56 92.37 77.47 93.69\\nSLIP [34] 24.14 67.47 21.75 61.96 75.45 92.17 64.72 86.37\\nMM-MIL [52] 21.78 62.41 33.05 71.26 69.73 89.07 75.92 92.59\\nConVIRT [56] 25.27 65.13 34.54 74.05 64.85 87.66 77.93 93.60\\nMGCA [51] 26.55 63.76 34.15 73.89 69.00 88.36 77.74 93.64\\nMaMA 31.04 74.83 40.31 77.36 75.40 93.46 78.02 93.65'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='MaMA 31.04 74.83 40.31 77.36 75.40 93.46 78.02 93.65\\nsize of 144 and train the model on 4 RTX A5000 GPUs with BFloat-16 precision. We set d= 512\\nandτ1=τ2=τlocal= 0.07. We provide more details for hyper-parameters in the appendix.\\n4.2 Downstream Evaluation Settings\\nTasks and Datasets We primarily evaluate our method on the EMBED [21] dataset for both\\nBI-RADS assessment category (7 classes) and breast density (4 classes) prediction tasks. Note that\\nthe real-world distribution of labels for both tasks is extremely imbalanced. To demonstrate the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='behavior of each model in a more realistic scenario, we sample 7,666 images for BI-RADS prediction\\nand 7,301 images for breast density prediction from the test split following the dataset distribution.\\nTo avoid insufficient test data and possible bias, we use all the images with BIRADS 5 and 6 in\\nthe BIRADS prediction test set. Detailed class distribution is provided in the appendix. We also\\nuse the RSNA-Mammo [4] dataset for out-of-domain evaluation for binary cancer detection, which\\nonly released a training set with 54k images. We split it into a training set of 85% data and used'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='the remaining as the evaluation. Given the extremely imbalanced distribution of both datasets, we\\nchoose to report balanced accuracy and AUC as our primary metrics. We also report the sensitivity\\nand specificity of the RSNA-Mammo cancer detection task. We do not assess zero-shot classification\\non this dataset since only a binary cancer label is available for all samples.\\nEvaluation Settings We evaluate all methods under zero-shot, linear classification, and full fine-\\ntuning settings. For zero-shot classification, we provide patient and imaging meta-information'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='along with the class-wise captions, as this meta-information is readily available without a clinician’s\\ndiagnosis. For linear classification, we attach a linear classifier and fine-tune it using 1%, 10%, or\\n100% of the training data. Following [ 56,19,51,53,54,50], we perform this full data efficiency\\nstudy with linear classification and present as our primary results since this experiment mainly focuses\\non the quality of the pre-trained embedding and it can best demonstrate the difference between each'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='VLP method. For full fine-tuning, we again attach a linear classifier and fine-tune the whole model\\nusing 100% of the training data. Our learning rate is set to 5E-4 and weight decay to 1E-3 using the\\nSGD optimizer with cosine annealing scheduler for 8k steps with batch size 36. A warm-up of 100\\nsteps with a minimum learning rate of 1E-5 is applied. The fine-tuning uses 2 RTX A5000 GPUs.\\nBaselines As the very first attempt at full contrastive language-image pre-training for mammog-\\nraphy, we choose to compare with the following baselines: 1) ViT [13,37]: we compare with'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='vision-only baselines with both random initialization and DiNOv2 [ 37] pre-training. 2) CLIP [39]:\\nthe vanilla CLIP model without other additional design; 3) SLIP [34]: a contrastive learning frame-\\nwork that optimizes both image-image and image-text loss; 4) MM-MIL [52]: a CLIP model that\\nlearns local image-language relationship via a multiple instance learning paradigm; 5) ConVIRT [56]:\\none of the first Chest X-ray specific CLIP models; 6) MGCA [51]: one of the SoTA CLIP models\\nfor Chest X-ray that applies multi-granularity feature alignment. We pre-train and fine-tune all'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 6}, page_content='these baselines with the same settings as our model. We also replaced the original DeiT [ 48] ViT\\nwith DiNOv2 [ 37] for a fair comparison since DeiT-ViT [ 48] is only trained with a smaller image\\nsize. All the baseline methods use fully fine-tuned BioClinicalBERT [ 2] as text encoder. While we\\nacknowledge that there are other recent medical VLP methods [ 19,54,50,53], they either adapt\\n7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='Table 3: Classification Results on RSNA-Mammo [ 4]We evaluate linear classification and fully\\nfine-tuned settings for the cancer prediction task. We report balanced accuracy (bACC), AUC,\\nsensitivity (SEN), and specificity (SPE). The best and second-best results are highlighted in bold and\\nunderlined, respectively. Our method is shaded in gray.\\nModelsRSNA-Mammo [4]\\nLinear Classification Full Fine-tune\\nbACC (%) AUC (%) SEN (%) SPE (%) bACC (%) AUC (%) SEN (%) SPE (%)\\nVision only\\nRandom-ViT 51.90 56.34 72.60 31.21 56.71 57.62 77.88 35.53\\nDiNOv2-ViT 63.23 68.59 59.62 66.84 55.12 58.18 70.19 40.06'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='DiNOv2-ViT 63.23 68.59 59.62 66.84 55.12 58.18 70.19 40.06\\nDeiT-based [48]\\nCLIP [38] 53.97 58.20 85.58 22.37 56.83 61.00 64.42 49.24\\nConVIRT [56] 65.96 69.81 66.83 65.10 53.31 69.16 8.65 97.96\\nMGCA [51] 63.01 69.16 62.50 63.52 53.88 73.04 12.02 95.74\\nDiNOv2-based\\nCLIP [38] 63.89 70.28 58.17 69.61 56.86 61.20 69.23 44.49\\nSLIP [34] 62.48 67.51 78.37 46.60 56.74 60.05 63.94 49.53\\nMM-MIL [52] 64.02 70.67 58.17 69.86 59.97 65.04 57.21 62.73\\nConVIRT [56] 65.89 70.70 66.83 64.96 54.53 69.85 11.06 98.01\\nMGCA [51] 60.79 67.45 71.15 50.43 55.99 68.67 14.90 97.07'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='MGCA [51] 60.79 67.45 71.15 50.43 55.99 68.67 14.90 97.07\\nMaMA 67.50 73.99 72.60 62.40 65.20 73.01 67.31 63.10\\ndomain-specific design and require annotations not presented in our dataset [ 53,50,54] or were\\nshown to perform worse in other studies than the chosen baselines [ 19,60]. We also do not compare\\nto related work that has no official implementation released [30, 6].\\n4.3 Results\\nLinear Classification We report the performance of both EMBED BI-RADS and density classifi-\\ncation tasks for each baseline in Tab. 1. We note MaMA achieves the best performance overall under'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='different amounts of training data. Our method shows a non-trivial improvement of more than 4%\\nof balanced accuracy on the BI-RADS prediction task when fine-tuned with full training data. We\\nnote that reducing the amount of training data has a greater influence on the BI-RADS prediction\\ntask than the density prediction task, as the BI-RADS distribution is more imbalanced, e.g., there\\nare only 6 training images for BI-RADS category 5 and 2 images for category 6 when using 1%\\ntraining data. However, our method still maintains the best overall performance even when trained'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='with only 1% data on the BI-RADS prediction task. This demonstrates the strong generalization\\nability and robustness of MaMA. Even if comparing with baselines also with local awareness [ 52,51],\\nour method is still the best. We also notice that the DiNOv2 [ 37]-based models tend to outperform\\nthe DeiT [ 48]-based models even if using the same VLP model design. This is not only because\\nDiNOv2 ViT [ 37] was trained on more data, but also due to the use of a larger image size, which is\\ncritical for high-resolution mammography.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='critical for high-resolution mammography.\\nZero-shot Classification We report the zero-shot classification performance for each of the meth-\\nods on both EMBED [ 21] tasks in Tab. 2. While our method still outperforms all the baselines,\\nwe highlight the zero-shot performance of the BI-RADS score prediction task, where our model\\noutperforms the best baseline by ∼5% in terms of balanced accuracy and more than 7% in AUC\\nscore. Compared with baselines using the fully fine-tuned small BioClinicalBERT [ 2], our method'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='with pre-trained LLM with PEFT shows much better zero-shot performance as the LLM can provide\\na text-supervised signal with higher quality. Meanwhile, the PEFT helps to prevent the LLM from\\ncollapsing during fine-tuning. As a result, our LLM text encoder with PEFT can provide better\\nzero-shot text embedding and improve the zero-shot performance greatly. Meanwhile, we note that\\nthe adopted LLM with PEFT encoder only has 2.6 M trainable parameters, which is only 3% of the\\nBioClinicalBERT [2] in terms of size.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 7}, page_content='BioClinicalBERT [2] in terms of size.\\nFull Fine-tuning Classification We also report the classification results after full-fine-tuning for\\nEMBED [ 21] tasks in Tab. 2. We note that while the gap between each method is somewhat reduced\\ndue to full fine-tuning, our model still beats all other baselines on both tasks.\\nOut-of-Domain Data Analysis We report performance of each method on the out-of-domain\\nRSNA-Mammo dataset in Tab. 3. Since RSNA-Mammo [ 4] is an extremely imbalanced dataset\\n(48:1 negative to positive ratio), we report the sensitivity and specificity as well. We note our model\\n8'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='Table 4: Ablation of Model Design We ablate different model designs on the EMBED [ 21] BI-RADS\\nprediction task and report balanced accuracy (bACC) and AUC score. The best and second best\\nresults are highlighted in bold and underlined, respectively. Our full method is shaded in gray.\\nMethods EMBED BI-RADS [21]\\nSLA Symm. LV T LV V PEFT-LLMZero-shot Linear Classification Full Fine-tune\\nbACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%)\\n✓ ✓ ✓ 29.28 71.16 38.71 77.50 30.55 70.69\\n✓ ✓ ✓ 31.03 72.79 39.57 77.39 39.47 76.23\\n✓ ✓ ✓ 27.32 70.18 37.21 77.95 23.78 63.97'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='✓ ✓ ✓ 27.32 70.18 37.21 77.95 23.78 63.97\\n✓ ✓ ✓ 23.88 62.84 38.96 77.43 22.29 63.77\\n✓ ✓ ✓ ✓ 31.04 74.83 39.75 77.50 40.31 77.36\\nTable 5: Multi-view Ablation We ablate different\\nmulti-view contrastive learning strategies.\\nMethodsEMBED BI-RADS [21]\\nZero-shot Linear Classification Full Fine-tune\\nbACC(%) AUC(%) bACC(%) AUC(%) bACC(%) AUC(%)\\nSame Image 30.48 73.95 39.70 77.73 39.35 76.44\\nIntra-side 30.71 74.21 39.93 77.41 35.17 76.09\\nIntra-study 31.04 74.83 39.75 77.50 40.31 77.36Table 6: Caption Ablation We ablate different\\ntext caption construction strategies.\\nMethodsEMBED BI-RADS [21]'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='MethodsEMBED BI-RADS [21]\\nZero-shot Linear Classification Full Fine-tune\\nbACC(%) AUC(%) bACC(%) AUC(%) bACC(%) AUC(%)\\nCLIP-style 35.99 77.66 37.74 77.25 24.00 65.35\\nNo Meta Mask 27.19 68.20 36.94 76.33 24.06 64.85\\nStruct. Cap. 31.04 74.83 39.75 77.50 40.31 77.36\\nperforms best in terms of balanced accuracy and AUC with a notable gap. While some of the\\nbaselines outperform our model on either the sensitivity or specificity metric, we note these models\\nare not informative, i.e., they tend to collapse and predict the majority of images to one of the classes.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='This will lead to a high score in one of the sensitivity or specificity metrics while result in a low\\nperformance in the other. In contrast, our approach shows reasonable results for both metrics and is\\nthe only method with both sensitivity and specificity greater than 60% under both the linear and full\\nfine-tuning settings. Furthermore, the other few methods that demonstrated higher sensitivity than\\nours all resulted in a specificity of ∼45% or worse.\\n4.4 Ablation Experiments\\nModel Design We ablate the influence of each component in Tab. 4. Compared with these baselines,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='we note each component has an important contribution to the overall model performance, as removing\\nany one resulted in inferior performance. We note that the baseline without PEFT-LLM instead\\nemploys BioClinicalBERT [ 2] and shows a clear drop in zero-shot performance, which validates the\\nimportance of using a PEFT-LLM. However, this model still performs well on the linear classification\\nand full fine-tuning tasks, which demonstrates the effectiveness of our other design choices.\\nMulti-view Ablation We ablate the multi-view sampling strategy here by using: 1) the same image,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='2) an intra-side image, and 3) the complete intra-study image (Tab. 5). We can see that the model\\ntrained with only one image loses the multi-view understanding. The model using only intra-side\\nimages only considers ipsilateral correspondence and also results in a worse performance.\\nCaption Ablation We evaluate the influence of using different caption construction strategies\\nin Tab. 6. We note that a CLIP style caption that only focuses on class labels shows a better\\nzero-shot performance, but degenerates greatly in the linear classification and full fine-tuning tasks.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='Meanwhile, if simply using the full meta-information during training, the model will fail with\\nzero-shot classification since it may mainly rely on the meta-information during the training and\\nignore more important clinical information. Our full design of using a structural caption with\\nmeta-information masking shows the best performance.\\n5 Discussion and Conclusion\\nIn this work, we presented a complete and novel multi-view and multi-scale alignment contrastive\\nlanguage-image pre-training method for mammography. We proposed utilizing the multi-view nature'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 8}, page_content='of mammography and providing local image-sentence correspondence to help address the challenges\\nof small ROIs and high image resolution and provide fine-grained visual clues for decisions. The\\nproposed method greatly outperforms multiple existing medical CLIP baselines.\\n9'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='Limitation and Future Work As we mainly focus on image representation learning, we have yet to\\nevaluate other downstream tasks like image-text retrieval, object detection, and segmentation. While\\nalso limited by accessible data in this domain, our method will be evaluated on more downstream\\ntasks in future work. We plan to extend this current framework to more mammography imaging\\nmodalities including C-view and digital breast tomosynthesis to further enhance its understanding of\\nmammography. Meanwhile, we also plan to integrate this pre-trained component into a multi-modal'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='question-answering and grounding model, to further explore the potential of medical VLP.\\n6 Acknowledgement\\nThis work was supported by NIH grant R21EB032950.\\nReferences\\n[1] AI@Meta. Llama 3 model card, 2024. Accessed: 2024-05-10.\\n[2]Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew\\nMcDermott. Publicly available clinical BERT embeddings. In Proceedings of the 2nd Clinical Natural\\nLanguage Processing Workshop , pages 72–78, Minneapolis, Minnesota, USA, June 2019. Association for\\nComputational Linguistics.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='Computational Linguistics.\\n[3]Elliot Bolton, David Hall, Michihiro Yasunaga, Tony Lee, Chris Manning, and Percy Liang. Biomedlm,\\n2023. Accessed: 2023-03-02.\\n[4] Chris Carr and Yan Chen et.al. Rsna screening mammography breast cancer detection, 2022.\\n[5]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\\ncontrastive learning of visual representations. In International conference on machine learning , pages\\n1597–1607. PMLR, 2020.\\n[6]Xuxin Chen, Yuheng Li, Mingzhe Hu, Ella Salari, Xiaoqian Chen, Richard LJ Qiu, Bin Zheng, and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='Xiaofeng Yang. Mammo-clip: Leveraging contrastive language-image pre-training (clip) for enhanced\\nbreast cancer diagnosis with multi-view mammography. arXiv preprint arXiv:2404.15946 , 2024.\\n[7]Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco\\nSalvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. Meditron-70b: Scaling\\nmedical pretraining for large language models. arXiv preprint arXiv:2311.16079 , 2023.\\n[8]Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='arXiv preprint arXiv:2309.16588 , 2023.\\n[9]Alexandros Delitzas, Maria Parelli, Nikolas Hars, Georgios Vlassis, Sotirios Anagnostidis, Gregor Bach-\\nmann, and Thomas Hofmann. Multi-clip: Contrastive vision-language pre-training for question answering\\ntasks in 3d scenes. arXiv preprint arXiv:2306.02329 , 2023.\\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255.\\nIeee, 2009.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='Ieee, 2009.\\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\\n[12] Jon Donnelly, Luke Moffett, Alina Jade Barnett, Hari Trivedi, Fides Schwartz, Joseph Lo, and Cynthia\\nRudin. Asymmirai: Interpretable mammography-based deep learning model for 1–5-year breast cancer\\nrisk prediction. Radiology , 310(3):e232780, 2024.\\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.\\n[14] Yuexi Du, Regina J Hooley, John Lewin, and Nicha C Dvornek. Sift-dbt: Self-supervised initializa-\\ntion and fine-tuning for imbalanced digital breast tomosynthesis image classification. arXiv preprint\\narXiv:2403.13148 , 2024.\\n[15] Sedigheh Eslami, Christoph Meinel, and Gerard De Melo. Pubmedclip: How much does clip benefit visual'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='question answering in the medical domain? In Findings of the Association for Computational Linguistics:\\nEACL 2023 , pages 1181–1193, 2023.\\n[16] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,\\nCarl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your\\nown latent-a new approach to self-supervised learning. Advances in neural information processing systems ,\\n33:21271–21284, 2020.\\n[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 9}, page_content='visual representation learning. arXiv preprint arXiv:1911.05722 , 2019.\\n10'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 ,\\n2021.\\n[19] Shih-Cheng Huang, Liyue Shen, Matthew P Lungren, and Serena Yeung. Gloria: A multimodal global-\\nlocal representation learning framework for label-efficient medical image recognition. In Proceedings of\\nthe IEEE/CVF International Conference on Computer Vision , pages 3942–3951, 2021.\\n[20] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph\\ndataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial\\nintelligence , volume 33, pages 590–597, 2019.\\n[21] Jiwoong J Jeong, Brianna L Vey, Ananth Bhimireddy, Thomas Kim, Thiago Santos, Ramon Correa, Raman\\nDutt, Marina Mosunjac, Gabriela Oprea-Ilies, Geoffrey Smith, et al. The emory breast imaging dataset\\n(embed): A racially diverse, granular dataset of 3.4 million screening and diagnostic mammographic'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='images. Radiology: Artificial Intelligence , 5(1):e220047, 2023.\\n[22] Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Yifan\\nPeng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. Mimic-cxr-jpg, a large publicly\\navailable database of labeled chest radiographs. arXiv preprint arXiv:1901.07042 , 2019.\\n[23] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi,\\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='critical care database. Scientific data , 3(1):1–9, 2016.\\n[24] Zhengfeng Lai, Zhuoheng Li, Luca Cerny Oliveira, Joohi Chauhan, Brittany N Dugger, and Chen-Nee\\nChuah. Clipath: Fine-tune clip with visual feature fusion for pathology image analysis towards minimizing\\ndata collection efforts. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\\npages 2374–2380, 2023.\\n[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='with frozen image encoders and large language models. In International conference on machine learning ,\\npages 19730–19742. PMLR, 2023.\\n[26] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\\nfor unified vision-language understanding and generation. In International conference on machine learning ,\\npages 12888–12900. PMLR, 2022.\\n[27] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie\\nYan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='arXiv preprint arXiv:2110.05208 , 2021.\\n[28] Zheren Li, Zhiming Cui, Sheng Wang, Yuji Qi, Xi Ouyang, Qitian Chen, Yuezhi Yang, Zhong Xue,\\nDinggang Shen, and Jie-Zhi Cheng. Domain generalization for mammography detection via multi-style\\nand multi-view contrastive learning. In Medical Image Computing and Computer Assisted Intervention–\\nMICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021,\\nProceedings, Part VII 24 , pages 98–108. Springer, 2021.\\n[29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='information processing systems , 36, 2024.\\n[30] Jiarun Liu, Hong-Yu Zhou, Cheng Li, Weijian Huang, Hao Yang, Yong Liang, and Shanshan Wang.\\nMlip: Medical language-image pre-training with masked local representation learning. arXiv preprint\\narXiv:2401.01591 , 2024.\\n[31] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,\\nHang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object\\ndetection. arXiv preprint arXiv:2303.05499 , 2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='detection. arXiv preprint arXiv:2303.05499 , 2023.\\n[32] Yuhang Liu, Fandong Zhang, Chaoqi Chen, Siwen Wang, Yizhou Wang, and Yizhou Yu. Act like a\\nradiologist: towards reliable multi-view correspondence reasoning for mammogram mass detection. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence , 44(10):5947–5961, 2021.\\n[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101 , 2017.\\n[34] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='image pre-training. In European conference on computer vision , pages 529–544. Springer, 2022.\\n[35] HQ Nguyen, HH Pham, LT Le, M Dao, and K VinDr-CXR Lam. An open dataset of chest x-rays with\\nradiologist annotations. PhysioNet https://doi. org/10.13026/3akn-b287 , 2021.\\n[36] Michael Onken, Marco Eichelberg, Jörg Riesmeier, and Peter Jensch. Digital imaging and communications\\nin medicine. In Biomedical Image Processing , pages 427–454. Springer, 2010.\\n[37] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V . V o, Marc Szafraniec, Vasil Khalidov, Pierre'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 10}, page_content='Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu\\nXu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel\\n11'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.\\nDinov2: Learning robust visual features without supervision, 2023.\\n[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\\nnatural language supervision. In International conference on machine learning , pages 8748–8763. PMLR,\\n2021.\\n[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\\n[40] Ketul Shah, Anshul Shah, Chun Pong Lau, Celso M de Melo, and Rama Chellappa. Multi-view action\\nrecognition using contrastive learning. In Proceedings of the IEEE/CVF Winter Conference on Applications\\nof Computer Vision , pages 3381–3391, 2023.\\n[41] E. A. Sickles, C. J. D’Orsi, L. W. Bassett, et al. ACR BI-RADS mammography. In ACR BI-RADS Atlas,\\nBreast Imaging Reporting and Data System . American College of Radiology, Reston, V A, 5th edition,\\n2013.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='2013.\\n[42] Dan Song, Xinwei Fu, Weizhi Nie, Wenhui Li, and Anan Liu. Mv-clip: Multi-view clip for zero-shot 3d\\nshape recognition. arXiv preprint arXiv:2311.18402 , 2023.\\n[43] Lilei Sun, Jie Wen, Junqian Wang, Zheng Zhang, Yong Zhao, Guiying Zhang, and Yong Xu. Breast\\nmass classification based on supervised contrastive learning and multi-view consistency penalty on\\nmammography. IET Biometrics , 11(6):588–600, 2022.\\n[44] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques\\nfor clip at scale. arXiv preprint arXiv:2303.15389 , 2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='for clip at scale. arXiv preprint arXiv:2303.15389 , 2023.\\n[45] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques\\nfor clip at scale. arXiv preprint arXiv:2303.15389 , 2023.\\n[46] Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang.\\nEva-clip-18b: Scaling clip to 18 billion parameters. arXiv preprint arXiv:2402.04252 , 2024.\\n[47] Hyuna Sung, Jacques Ferlay, Rebecca L Siegel, Mathieu Laversanne, Isabelle Soerjomataram, Ahmedin'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='Jemal, and Freddie Bray. Global cancer statistics 2020: Globocan estimates of incidence and mortality\\nworldwide for 36 cancers in 185 countries. CA: a cancer journal for clinicians , 71(3):209–249, 2021.\\n[48] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve\\nJegou. Training data-efficient image transformers and distillation through attention. In Marina Meila and\\nTong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning , volume 139\\nofProceedings of Machine Learning Research , pages 10347–10357. PMLR, 18–24 Jul 2021.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='[49] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\\nfine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\\n[50] Zhongwei Wan, Che Liu, Mi Zhang, Jie Fu, Benyou Wang, Sibo Cheng, Lei Ma, César Quilodrán-\\nCasas, and Rossella Arcucci. Med-unic: Unifying cross-lingual medical vision-language pre-training by\\ndiminishing bias. Advances in Neural Information Processing Systems , 36, 2024.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='[51] Fuying Wang, Yuyin Zhou, Shujun Wang, Varut Vardhanabhuti, and Lequan Yu. Multi-granularity cross-\\nmodal alignment for generalized medical visual representation learning. Advances in Neural Information\\nProcessing Systems , 35:33536–33549, 2022.\\n[52] Peiqi Wang, William M Wells, Seth Berkowitz, Steven Horng, and Polina Golland. Using multiple instance\\nlearning to build multimodal representations. In International Conference on Information Processing in\\nMedical Imaging , pages 457–470. Springer, 2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='Medical Imaging , pages 457–470. Springer, 2023.\\n[53] Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun. Medclip: Contrastive learning from\\nunpaired medical images and text. arXiv preprint arXiv:2210.10163 , 2022.\\n[54] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Medklip: Medical knowledge\\nenhanced language-image pre-training. medRxiv , pages 2023–01, 2023.\\n[55] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh\\nRao, Mu Wei, Naveen Valluri, et al. Biomedclip: a multimodal biomedical foundation model pretrained'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='from fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915 , 2023.\\n[56] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. Contrastive\\nlearning of medical visual representations from paired images and text. In Machine Learning for Healthcare\\nConference , pages 2–25. PMLR, 2022.\\n[57] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai. Groundhog:\\nGrounding large language models to holistic segmentation. arXiv preprint arXiv:2402.16846 , 2024.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 11}, page_content='[58] Zihao Zhao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Xiang Li, Zhiming\\nCui, Qian Wang, et al. Clip in medical imaging: A comprehensive survey. arXiv preprint arXiv:2312.07353 ,\\n2023.\\n12'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 12}, page_content='[59] Kecheng Zheng, Yifei Zhang, Wei Wu, Fan Lu, Shuailei Ma, Xin Jin, Wei Chen, and Yujun Shen. Dreamlip:\\nLanguage-image pre-training with long captions. arXiv preprint arXiv:2403.17007 , 2024.\\n[60] Hong-Yu Zhou, Chenyu Lian, Liansheng Wang, and Yizhou Yu. Advancing radiograph representation\\nlearning with masked record modeling. arXiv preprint arXiv:2301.13155 , 2023.\\n13'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='A Appendix\\nIn the appendix, we provide more detailed training settings, evaluation settings, model configurations,\\nand additional analysis.\\nA.1 Further Limitations\\nThe EMBED data comes from the Atlanta, GA region. While the dataset is highly ethnically\\ndiverse, the geographic focus could limit generalizability to other populations, e.g., the breast density\\ndistribution may differ from data gathered in other regions of the world. Additionally, The caption\\nis created from the templated-based method, which may potentially harm the model due to limited'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='caption diversity. Future works may consider augment the template-based prompt with LLM.\\nA.2 Broader Impacts\\nThis paper proposed a promising visual language pre-training scheme for mammography that can be\\nused for various downstream tasks. It can also potentially speed up the real-world mammography\\nscreening or diagnostic process by filtering out low-risk studies and highlighting high-risk images for\\nthe clinician. While the EMBED dataset is one of the largest and most diverse public mammography'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='datasets available, it is notable that the data were collected from four specific hospitals and thus\\nthe trained model may have a specific bias towards a specific group of people due to training data\\ncomposition. Any user who wants to use this model in their own research may need to carefully\\nanalyze such bias and their own application and tasks and avoid using the model in real-world clinical\\ntrials without further approval.\\nAlgorithm 1 SLA Loss Pseudocode\\n1:# fp, fs: local patch, sentence projectors\\n2:# N, tau_local: batch size and SLA loss temperature'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='2:# N, tau_local: batch size and SLA loss temperature\\n3:# patch_feats: patch-wise image feature. (N, num_patch, C)\\n4:# sent_feats: sentence-wise text feature. list of N tensors, (num_sent, C)\\n5:def SLA_loss(patch_feats, sent_feats):\\n6: t2v_scores = [] # cV: visual localization correspondence\\n7: v2t_scores = [] # cT: textual localization correspondence\\n8: patch_feats = normalize(fp(patch_feats))\\n9: # Each report may have different num_sent\\n10: for sent in sent_feats:\\n11: sent = normalize(fs(sent))\\n12: score = torch.bmm(path_feats, sent.T) # (N, num_patch, num_sent)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='13: # Visual localization: Max over patches + Avg over sentences\\n14: t2v_scores.append(score.max(dim=1, keepdim=True).mean(dim=2).squeeze())\\n15: # Textual localization: Max over sentences + Avg over patches\\n16: v2t_scores.append(score.max(dim=2, keepdim=True).mean(dim=1).squeeze())\\n17: t2v_scores = torch.stack(t2v_scores, dim=0) / tau_local # (N, N)\\n18: v2t_scores = torch.stack(v2t_scores, dim=0) / tau_local # (N, N)\\n19: labels = torch.arange(N)\\n20: loss0 = cross_entropy(t2v_scores, labels)\\n21: loss1 = cross_entropy(v2t_scores, labels)\\n22: return 0.5 * (loss0 + loss1)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='22: return 0.5 * (loss0 + loss1)\\nA.3 Pseudo-Code of SLA Module\\nWe provide the pytorch pseudo-code of the SLA module in the Algorithm 1 to better illustrate the\\ndesign of the SLA module.\\nA.4 Pre-training Implementation Details\\nDataset and Pre-processing As mentioned in Sec. 4.1, we use the EMBED [ 21] dataset for pre-\\ntraining. We only use the 2D mammography and split the dataset into 70%/10%/20% for training,\\nvalidation, and testing at the patient level. We filter out the studies for males or those that have'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 13}, page_content='missing BI-RADS or density labels. We provide the detailed distribution of BI-RADS score and\\n14'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 14}, page_content='Table 7: Model Trainable Parameters We provide the number of trainable parameters for each\\nmodel here below. Our method as described in the main paper is shaded in gray.\\nModels#Trainable Parameters (M)\\nVisual Encoder Language Encoder Total\\nVision only\\nRandom-ViT [13] 89.6 - 86.6\\nDiNOv2-ViT [37] 89.6 - 86.6\\nDeiT-based [48]\\nCLIP [38] 86.6 84.6 172.5\\nConVIRT [56] 86.6 84.6 173.2\\nMGCA [51] 86.6 84.6 174.4\\nDiNOv2-based [37]\\nCLIP [38] 89.6 84.6 174.5\\nSLIP [34] 89.6 84.6 174.8\\nMM-MIL [52] 89.6 84.6 174.9\\nConVIRT [56] 89.6 84.6 176.2\\nMGCA [51] 89.6 84.6 177.4\\nMaMA-BioClinicalBERT [2] 89.6 84.6 177.5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 14}, page_content='MaMA-BioClinicalBERT [2] 89.6 84.6 177.5\\nMaMA-LoRA-BioMedLM [18, 3] 89.6 2.6 92.8\\nMaMA-LoRA-Meditron [18, 7] 89.6 4.2 94.3\\nMaMA-LoRA-Llama3 [18, 1] 89.6 3.4 93.4\\nBreast density in Fig. 3, displaying the extremely imbalanced labels. Each of the sampled splits\\nshares roughly the same distribution. More details about the dataset can be found in [ 21]. For the\\ndata pre-processing, we first convert each original DICOM image file to JPEG format and resize the\\nimage based on its long side to 1,024 pixels without changing its aspect ratio. These images are then\\nused directly for training.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 14}, page_content='used directly for training.\\nPre-training Data Augmentation Different from CLIP [ 39], we use a strong data augmentation\\nduring the pre-training stage for both images. We first apply the OTSU threshold masking to cut the\\nunnecessary background regions and only keep the breast tissue. This image is then resized to 518\\npixels on its long side and padded with zeros on the short side to have a square shape of 518,×518.\\nWe then apply SimCLR [ 5] style augmentation including random horizontal and vertical flips, color'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 14}, page_content='jitter, grayscale, and Gaussian blur. During test time, we only keep the resize operation and drop all\\nrandom augmentations.\\nModel Details As mentioned in Sec. 4.1, we use DiNOv2 pre-trained ViT-B-reg [ 13,8] model with\\nimage size 518 and patch size 14 as our visual encoder. We use BioMedLM [ 3], a 3M level GPT-2\\ndecoder-only transformer of 32 layers as our language encoder. We adapt LoRA [ 18] to fine-tune\\nthis encoder. As for the baselines, we choose to experiment with both a DeiT [ 48]-based and a'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 14}, page_content='DiNOv2-based visual encoder. The DeiT-based transformer was pre-trained with a patch size of 16\\nand image size of 384 on ImageNet [ 10]. The input for the corresponding baselines is resized to 384\\nas well. For the DiNOv2 [ 37] visual encoder for the baselines, the setting is the same as our model.\\nAll the baselines use BioClinicalBERT [ 2], a BERT-style encoder-only transformer without PEFT.\\nEMBED Dataset Distribution\\nFigure 3: Data Distribution of EMBED [ 21] Dataset . We visualize the data distribution of the\\nEMBED [21] dataset for both BI-RADS and Density labels.\\n15'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='Full Fine-tuned Model Confusion Matrix\\nFigure 4: Confusion Matrix of Our Full Fine-tuned Model . We visualize the class-wise confusion\\nmatrix of our model fully fine-tuned with BI-RADS and density classification tasks, respectively.\\nWe use the online implementation for ConVIRT [ 56] and MGCA [ 51]2and adjust the vision encoder\\npart, and we re-implement the CLIP [ 39], SLIP [ 34], and MM-MIL [ 52] following the corresponding\\npapers under our environment. We provide the model size comparison in Tab. 7. We can easily see'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='that our model has the smallest number of trainable parameters, only ∼52% compared with other\\nbaselines. We choose to use the last checkpoint for all models in downstream evaluations.\\nPEFT Settings As for the parameter-efficient fine-tuning (PEFT) module, we use the LoRA\\nimplemented by HuggingFace with default hyperparameters: r= 8,α= 32 ,dropout = 0.1. We\\nchoose to use LoRA as it is one of the most popular PEFT methods and has been proven to be\\neffective in prior research.\\nOverall Pre-training Target As a supplement to the method section, we here provide the overall'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='pre-training optimization target in Eq. (4).\\nL(xi,˜vi, ti) =LV V(vi,˜vi) +LV T(vi, ti) +LV T(˜vi, ti) +wLlocal. (4)\\nWe set w= 0.0in the first 8,000 steps of training and w= 1.0in the latter process.\\nA.5 Downstream Evaluation Details\\nZero-shot Caption During zero-shot evaluation, we prepend the meta-information to the class-wise\\ndescription sentence, since this meta-information can be readily obtained with the images without\\nneeding the clinician’s diagnosis. More specifically, we prepend the information including: Procedure'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='reported ,Reason for procedure ,Patient info , and Image info before the class description sentence\\nof each BI-RADS or density class. This improves the zero-shot balanced accuracy of the BI-RADS\\nclassification from 29.65% to 31.04% and improves the corresponding AUC from 68.05% to 74.83%.\\nLinear Classifier We attach a linear classifier to each of the baseline models for linear classification\\nand full fine-tuned tasks. The linear classifier uses the average of all patch tokens as input rather than'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='using [CLS] token since the [CLS] token is not used during training as well. We use the full training\\nset and balanced weighted sampling during training for all the linear classification and fine-tuning\\nexperiments.\\nBI-RADS Prediction For EMBED [ 21] BI-RADS score prediction task, we sample 10% data\\nrandomly from the test set. However, we added more images for BI-RADS scores 5 and 6 to ensure\\nthese 2 classes at least have 200 images. This is to avoid bias due to limited evaluation samples. The'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 15}, page_content='final distribution of this dataset is: [901, 4472, 1166, 517, 210, 200, 200] for BI-RADS scores from 0\\nto 6 respectively. The pre-processing is the same as described in Appendix A.4.\\n2https://github.com/HKU-MedAI/MGCA\\n16'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='Table 8: Ablation with Different Meta Masking Ratio on EMBED BI-RADS [ 21]We evaluate\\nthe influence of using different meta-masking ratios on the input text during pre-training and test the\\nmodel on zero-shot settings. Our method as described in the main paper is shaded in gray.\\nModel Settings bACC(%) AUC(%)\\nm= 0.0 27.19 68.20\\nm= 0.2 29.52 71.23\\nm= 0.5 30.37 72.44\\nm= 0.2 31.04 74.83\\nTable 9: Ablation with Different Visual Contrastive Learning Style on EMBED [ 21]We evaluate\\nthe influence of using different visual contrastive pre-training schemes. We evaluate the zero-shot'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='and linear classification performance for each method. Our method as described in the main paper is\\nshaded in gray.\\nModel SettingsEMBED [21] BI-RADS EMBED [21] Density\\nZero-shot Linear classification Zero-shot Linear Probing\\nbACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%) bACC (%) AUC (%)\\nSimCLR [5] style 31.04 74.83 39.75 77.50 75.40 93.46 78.09 93.65\\nMoCo [17] style 29.04 74.67 36.74 78.16 76.18 92.58 78.03 93.49\\nDensity Prediction Similar to BI-RADS prediction, we randomly sample another 10% data from'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='the test set stratified by density label to create the density prediction set. The distribution of this\\ntest set is: [738, 3103, 3043, 417] for density from 1 to 4. We use the full training set and balanced\\nweighted sampling during training.\\nRSNA-Mammo [ 4] Cancer Detection Similar to EMBED pre-processing, we convert the DICOM\\nmammography to a JPEG image and resize its long side to 1,024 without changing the aspect ratio.\\nSince this dataset does not provide the corresponding meta-information, we only evaluate the linear'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='classification and full fine-tuning tasks. We use the full 15% test set for the RSNA-Mammo [ 4]\\nevaluation, where the distribution of this test set is [7979, 208] for normal and cancerous samples,\\nrespectively. We use the full training set and balanced weighted sampling during training.\\nA.6 Classification Results Analysis\\nWe visualize the confusion matrix for classification results of the fully fine-tuned model on both\\nEMBED [ 21] prediction tasks in Fig. 4. While the overall accuracy for the BI-RADS prediction task'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='still needs improvement, we note that the misclassification mainly happens for BI-RADS categories\\n2, 3, and 4, which is reasonable since these classes are semantically close to each other (“Benign”,\\n“Probably Benign”, and “Suspicious Abnormality”). Meanwhile, we note our model shows a high\\nrecall for BI-RADS category 6, i.e., “Known biopsy-proven malignancy”, which indicates the\\npotential application of the model to filter out high-risk abnormal mammography quickly.\\nMisclassifications for the density predictions are also reasonable, as mammographic density increases'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='with the higher density class label. Notably, most errors for the middle two density classes are for\\nthe more extreme version of that class (e.g., 3 corresponding to \"heterogeneously dense\" is more\\noften mistaken for 4 \"extremely dense\" compared to 2 \"scattered density\"); thus the binary dense\\n(labels 3/4) and non-dense (labels 1/2) prediction does well. This is important as women with dense\\nbreasts are required to be notified by US regulations, and this has ramifications for potential follow-up\\nscreening recommendations.\\nA.7 Additional Ablation Experiments'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 16}, page_content='A.7 Additional Ablation Experiments\\nMeta Masking Ratio To better understand the influence of masking the meta-information, we here\\nprovide an extra zero-shot evaluation on different mask ratios mduring the pre-training stage in\\nTab. 8. As shown above, the zero-shot performance increases as the meta-information masking ratio\\nincreases, which means the model tends to rely more on clinical-related information, and therefore,\\ndoes better in the zero-shot classification task.\\n17'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='Table 10: Ablation with Different Multi-view Contrastive Learning Probability on EMBED [ 21]\\nWe evaluate the influence of using different multi-view contrastive learning probabilities pon EMBED\\nBI-RADS prediction. We evaluate the zero-shot and linear classification performance for each pre-\\ntrained model. Our method as described in the main paper is shaded in gray.\\nModel SettingsEMBED [21] BI-RADS\\nZero-shot Linear Probing\\nbACC (%) AUC (%) bACC (%) AUC (%)\\np= 0.0 30.48 73.95 39.70 77.23\\np= 0.2 30.26 73.35 39.37 77.50\\np= 0.5 31.04 74.83 39.75 77.50\\np= 0.8 30.76 74.26 39.41 77.45'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='p= 0.8 30.76 74.26 39.41 77.45\\np= 1.0 29.33 73.21 38.20 77.49\\nTable 11: Comparison with Medical Pre-trained Visual Encoder on EMBED [ 21]We compare\\nour method with SimCLR [ 5] pre-trained visual encoder on the EMEBD [ 21] dataset under linear\\nclassification settings. Our method as described in the main paper is shaded in gray.\\nModel SettingsEMBED [21] BIRADS EMBED Density\\nbACC (%) AUC (%) bACC (%) AUC (%)\\nSimCLR Pre-trained 26.19 65.06 77.06 92.64\\nMaMA 39.75 77.50 78.09 93.65\\nDifferent Visual Contrastive Learning Scheme We here provide additional analysis of the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='influence of using different visual contrastive learning schemes by comparing a variation of the\\nproposed model, i.e., MoCo-style image-to-image contrastive loss [ 17], where a memory queue\\nof size 4096 is used to store the negative samples during pre-training. This can properly address\\nthe sensitivity of the image-to-image contrastive loss to the batch size, as there will always be a\\nlarge number of negative examples during pre-training (see Tab. 2 in [ 17], where a batch size of\\n256 was sufficient). Here, we provide a comparison between the proposed method (SimCLR style'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='image-to-image loss) and MoCo-style variation in Tab. 9.\\nWe note that there is no clear difference between the two models. The chosen SimCLR method\\nis slightly better from a general perspective. This result potentially suggests that the batch size\\nmay not be that important in our task, or that the used batch size was large enough. We provide\\ntwo possible explanations for this result: 1) Different from natural images, where the difference\\nbetween each sample is fairly large, the inter-sample difference for mammograms is much smaller.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='Mammography generally has very similar global content. Thus, fewer negative samples are sufficient\\nto provide a robust contrastive signal during image-to-image contrastive pre-training. 2) Apart from\\nthe image-to-image loss, the symmetric image-to-text loss between the caption and two images also\\nindirectly minimizes the distance between the two images, which helps alleviate the necessity of a\\nlarge batch size.\\nDifferent Multi-view Probability Additionally, we here provide more analysis on the multi-view'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='sampling strategy. We adjust the probability of using intra-study sampling and the augmented view of\\nthe same image as the extra image ˜xi, which is p= 0.5in the proposed method. When p= 0.0, the\\nmodel always samples the same augmented image as the other view during pre-training (equivalent\\nto the \"Single Image\" baseline in Tab. 5). In contrast, when p= 1.0, the model always samples\\none of the other images from the same study as the other view. We here provide the results of the\\nZero-shot and Linear classification BI-RADS prediction evaluation in Tab. 10. It is clear that either'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='using no inter-study sampling ( p= 0.0) or using only the multi-view sampling ( p= 1.0) will harm\\nthe performance. An equal-weight mix of both sampling methods shows the best performance, as it\\nprovides a more diverse contrastive image and reduces the potential contradictory image pairs (by\\nusing the augmented view of the same image).\\nVisual Constrastive Only Baseline We here include the linear classification results in comparison\\nto the ViT baseline pre-trained with the SimCLR [ 5] method on the EMBED dataset in Tab. 11. The'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 17}, page_content='vision-only pre-trained model performs worse compared with our method according to the results.\\n18'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='Table 12: Linear Classification Results on EMBED [ 21] for Different Text Encoder We evaluate\\nlinear classification results with different amounts of fine-tuning data for both BI-RADS and density\\nprediction tasks of our model with different text encoder. All methods are based on DiNOv2 [ 37]\\nvision encoder for a fair comparison. We report both balanced accuracy (bACC) and AUC metrics.\\nThe best and second-best results are highlighted in bold and underlined respectively. Our method as\\ndescribed in the main paper is shaded in gray.\\nModelsEMBED BI-RADS [21] EMBED Density [21]'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='ModelsEMBED BI-RADS [21] EMBED Density [21]\\nbACC (%) AUC (%) bACC (%) AUC (%)\\n1% 10% 100% 1% 10% 100% 1% 10% 100% 1% 10% 100%\\nBioClinicalBERT-based [2]\\nCLIP [38] 26.66 31.65 34.35 70.35 74.98 74.11 74.64 75.00 75.97 91.50 90.62 92.39\\nSLIP [34] 22.94 27.86 30.93 64.43 69.48 71.95 73.24 74.79 75.23 91.56 92.37 92.46\\nMM-MIL [52] 25.85 30.94 35.11 67.16 71.99 76.12 74.23 76.69 75.77 91.96 93.34 91.65\\nConVIRT [56] 24.62 30.38 31.27 65.09 73.33 74.03 74.34 74.95 74.74 92.21 92.56 92.58\\nMGCA [51] 23.66 30.11 30.27 64.19 72.24 72.54 71.43 72.25 72.20 90.83 91.21 91.24'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='MaMA-BERT 27.81 34.25 38.96 68.99 74.61 77.43 74.77 77.50 78.15 92.90 93.50 93.68\\nLoRA-LLM-based [18]\\nMaMA-BioMedLM 28.46 35.12 39.75 70.63 75.98 77.50 76.26 78.11 78.09 93.11 93.62 93.65\\nMaMA-Meditron 26.94 33.28 38.68 68.93 74.45 77.51 74.48 77.77 78.30 92.65 93.54 93.66\\nMaMA-Llama3 28.00 34.30 39.99 70.83 75.47 77.50 74.70 77.93 78.13 93.02 93.70 93.72\\nA.8 Benchmark Different Text Encoders\\nWe evaluate all methods with the same DiNOv2 [ 37] vision encoders but compare the influence of\\nusing different text encoders in Tab. 12.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='using different text encoders in Tab. 12.\\nText Encoders 1)BioClinicalBERT [2]: The standard text encoder used for previous medical CLIP\\nmodels [ 52,56,19,51,50] and also our baseline methods, which is a BERT [ 11]-style transformer\\npre-trained with MIMIC-III [ 23] clinical report. 2) BioMedLM [3]: A 2.7B level GPT-2 [ 39]\\ntransformer pre-trained with PubMed data, which is also one of the best 3B LLM according to\\nmultiple benchmarks [ 7]. 3) Meditron-7B [7]: A newly released Llama2 [ 49] model fine-tuned with'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='PubMed papers. 4) Llama3-8B [1]: Recently released, the most robust open-souced LLM, with\\nroughly the same architecture as Llama2 [ 49] but pre-trained with much more data. All the latter\\nthree LLMs are fine-tuned with LoRA [18]\\nResults We report the results on linear classification in Tab. 12. We note that even our model with\\nBioClinicalBERT [ 2] text encoder outperforms all the baselines in this evaluation; this demonstrates\\nthe effectiveness of the proposed multi-view mammography pre-training and symmetric local align-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='ment module. Comparing three different LLMs with LoRA [ 18], we note that BioMedLM [ 3] and\\nLlama3-8B [ 1] roughly have a similar level of performance, while the BioMedLM-based model has a\\nsmaller GPU memory cost and faster training speed due to its relative size. Meanwhile, we notice\\nthat the Meditron [ 7]-based model is not as good as the other two LLMs, but all these LLM-based\\nmethods outperform the model with smaller BERT-style [ 11] encoder in general. Overall, our choice\\nof BioMedLM [3]-based model has the best balance between performance and model size.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='A.9 Local Similarity Map Analysis\\nWe visualize the learned local patch-sentence similarity map in Fig. 5. As described in Sec. 3.3, the\\nlocal patch-sentence similarity map indicates the relationship between each region of the image and\\nthe corresponding input sentence. We visualize the similarity map for the “Impression” sentence in\\nthe report (see examples in Fig. 6 to Fig. 8), which includes the most important diagnosis information.\\nWe also visualize the same similarity map for MM-MIL [ 52] and a variation of our method that'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='optimizes local similarity with only visual localization (similar to including the MM-MIL local\\nbranch).\\nWe note that our methods generally have a better localization quality with more fine-grained details.\\nThe model can accurately locate the high-density and tumor-related regions in the given maps.\\nWe also see from the examples for patients 3 and 4 that our method has a better correspondence\\nbetween mammograms from different views or sides. Especially for column 3, our method accurately\\nidentified the same region in both views, while the baseline method failed to locate the tissue in the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 18}, page_content='RMLO view (left image). The MM-MIL [ 52] model even failed to detect the tumor for patient 4.\\nOn the other hand, the variation of our model that optimizes only visual localization loss can only\\n19'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 19}, page_content='MM‐MIL\\n Only\\xa0Visual\\xa0Local. MaMA (Ours)Patient \\xa0#1 Patient \\xa0#2 Patient \\xa0#4\\xa0(two\\xa0sides) Patient \\xa0#3\\xa0(two\\xa0views)Figure 5: Visualization of Local Similarity Maps over Input Mammograms . We visualize the\\nlearned local similarity map for the “Impressions” sentence on a few test mammograms from the\\nEMBED dataset [ 21] for MM-MIL [ 52], our method with only visual localization, and our full\\nmethod here. All the heat maps are normalized to [0,1]. The third column shows mammograms from\\nthe same side but a different view and the fourth column shows mammograms from the same view but'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 19}, page_content='from a different side. The white box in the image represents the ROI annotated from the dataset [ 21].\\nTable 13: Zero-shot Visual Grounding Analysis We report the mean intersection-over-union\\n(mIoU), mean DICE score, and ROI recall with 50% coverage for methods with local sentence-region\\nsimilarity map on the EMBED [21] dataset. Our method is shaded in gray.\\nModelsEMBED [21] Visual Grounding\\nmIoU (%) mDICE (%) Recall (%)\\nMM-MIL [52] 5.25 9.72 39.23\\nMaMA 6.22 11.88 47.67\\nprovide a vague and inaccurate similarity map. We believe this is because the asymmetric max and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 19}, page_content='average pooling operation drops too much information during training, resulting in only one of the\\npatches being optimized.\\nQuantitative Visual Grounding Analysis Similar to the analysis in MM-MIL [ 52], we further\\nconduct a zero-shot visual grounding analysis with the pre-trained model. We compare the similarity\\nmap extracted for the image and the “Impressions” description with the provided ROIs from a subset\\nof the EMBED [ 21] dataset, which contains 841 images from the test split, each with one or more'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 19}, page_content='ROI annotations. We report the mean intersection-over-union (mIoU), mean DICE (mDICE) score,\\nand ROI recall for both the MM-MIL [ 52] method and ours. Different from Wang et al. [52], we\\nuse a set of thresholds of [0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85]since the ROI is generally smaller\\nin the mammogram and needs a higher threshold to have better detection results. We compute IoU\\n20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='Table 14: Linear Classification Bootstrap Results for Balanced Accuracy on EMBED [ 21]We\\nconduct the bootstrap evaluation for the linear classification predicted result of our method on both\\nBI-RADS and density prediction tasks. We sample N= 10,000bootstrapped samples and compute\\nthe average balanced Accuracy (bACC) with the corresponding 95% confidence interval for each\\nsetting. This illustrates the statistical stability of our method.\\nTaskbACC (%)\\n1% 10% 100%\\nEMBED BI-RADS [21] 28.46 [27.12,29.84] 35.11 [33.36,36.86] 39.75 [37.81,41.64]'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='EMBED Density [21] 76.25 [74.88,77.60] 78.11 [73.65,75.66] 78.10 [76.82,79.34]\\nTable 15: Linear Classification Bootstrap Results for AUC on EMBED [ 21]We conduct the\\nbootstrap evaluation for the linear classification predicted result of our method on both BI-RADS and\\ndensity prediction tasks. We sample N= 10,000bootstrapped samples and compute the average\\nAUC with the corresponding 95% confidence interval for each setting. This illustrates the statistical\\nstability of our method.\\nTaskAUC (%)\\n1% 10% 100%\\nEMBED BI-RADS [21] 70.64 [69.56,71.69] 75.98 [75.09,76.87] 77.50 [76.61,78.35]'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='EMBED Density [21] 93.11 [92.70,93.52] 93.62 [93.23,94.00] 93.65 [93.26,94.02]\\nand DICE scores for each threshold and then average them to get mIoU and mDICE. For ROI recall,\\nan ROI is considered successfully predicted when the overlap between the binarized similarity map\\n(with a fixed threshold of 50%) and the ROI is greater than 50%. Our method generally shows a better\\nperformance over the MM-MIL [ 52] model and achieves a recall near 50% without training. We note\\nthat the number reported here may look low since this is a parameter-free zero-shot evaluation, and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='the ROI in the mammography is generally small compared with the whole image, which makes the\\ntask more challenging.\\nA.10 Performance Statistical Analysis\\nWe further evaluate the stability of the proposed method by bootstrap sampling test set results from\\nlinear classification and report the 95% confidence interval in Tab. 14 and Tab. 15. Notably, our\\nmethod generally shows a small confidence interval, especially for AUC scores. Comparing our\\nresults with confidence interval with the baselines in Tab. 1, we see that there is still a marked\\nimprovement in performance.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='improvement in performance.\\nA.11 Report Construction Template\\nWe provide here the template used to construct our structured image caption during training. We\\ndescribe each segment below, and the keywords wrapped with “{{” and “}}” will be replaced with\\ncorresponding information from the tabular data.\\n1.Procedure reported : {{PROCEDURE}}.\\n2.Reason for procedure : {{SCREENING/DIAGNOSTIC}}.\\n3.Patient info : This patient is {{RACE}}, {{ETHNIC}}, and {{AGE}} years old.\\n4.Image info : This is a {{IMAGE_TYPE}} full-field digital mammogram of the {{SIDE}}\\nbreast with {{VIEW}} view.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 20}, page_content='breast with {{VIEW}} view.\\n5.Breast composition : The breast is {{DENSITY_DESC}}.\\n6.Findings : The mammogram shows that {{MASS_DESC}}. The mass is {{SHAPE}} and\\n{{DENSITY}}. A {{DISTRI}} {{SHAPE}} calcification is present.\\n7.Impressions : BI-RADS Category {{BIRADS}}: {{BIRADS_DESC}}.\\n8.Overall Assessment : {{BIRADS_DESC}}\\nWe provide more details and corresponding description strings in our implementation file.\\n21'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 21}, page_content='A.12 Example Mammography Images with Captions\\nWe provide 7 randomly sampled mammography images with corresponding captions for each of the\\nBI-RADS categories in Fig. 6 to Fig. 8.\\n22'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 22}, page_content='•Procedure reported : MG Screen Bilat w/Tomo/CAD Stnd\\nProtocol. \\n•Reason for procedure : screening. \\n•Patient info : This patient is African American  or Black, \\nNon-Hispanic or Latino, and 56 years old.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe right breast with MLO view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that an additional imaging is \\nrecommended.  \\n•Impressions : BI-RADS Category 0: additional imaging \\nrequired. \\n•Overall Assessment : Additional imaging is recommended.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 22}, page_content='•Overall Assessment : Additional imaging is recommended.\\n•Procedure reported : MG Screen Bilat w/Tomo/CAD Stnd\\nProtocol. \\n•Reason for procedure : screening. \\n•Patient info : This patient is Caucasian or White, Non-\\nHispanic or Latino, and 50 years old.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe right breast with CC view.  \\n•Breast composition : The breast is heterogeneously dense. \\nThis may lower the sensitivity of mammography.  \\n•Findings : The mammogram shows that no significant masses, \\ncalcification, or other abnormalities are present.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 22}, page_content='calcification, or other abnormalities are present.  \\n•Impressions : BI-RADS Category 1: negative. \\n•Overall Assessment : Negative.\\n•Procedure reported : MG Diagnostic Bilateral w/ CAD. \\n•Reason for procedure : diagnostic. \\n•Patient info : This patient is Caucasian or White, Non-\\nHispanic or Latino, and 67 years old.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe left breast with MLO view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a benign finding is \\npresent.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 22}, page_content='present.  \\n•Impressions : BI-RADS Category 2: benign finding. \\n•Overall Assessment : Benign.Figure 6: Example Multi-view Mammography BI-RADS 0-2 with Constructed Caption . We\\nprovide random sampled multi-view mammography with the corresponding caption constructed by\\nus. We highlight the image match exactly with the caption in a green bounding box.\\n23'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 23}, page_content='•Procedure reported : MG Diagnostic Left w/CAD. \\n•Reason for procedure : diagnostic. \\n•Patient info : This patient is African American  or Black, \\nNon-Hispanic or Latino, and 80 years old.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe left breast with CC view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a probably benign \\nfinding is present.  A Grouped Coarse calcification is \\npresent.  \\n•Impressions : BI-RADS Category 3: probably benign finding. \\n•Overall Assessment : Probably benign.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 23}, page_content='•Overall Assessment : Probably benign.\\n•Procedure reported : MG Diagnostic Right w/CAD. \\n•Reason for procedure : diagnostic. \\n•Patient info : This patient is Caucasian or White, Non-\\nHispanic or Latino, and 41 years old.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe right breast with MLO view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a suspicious abnormality \\nis present.  \\n•Impressions : BI-RADS Category 4: suspicious abnormality. \\n•Overall Assessment : Suspicious abnormality.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 23}, page_content='•Overall Assessment : Suspicious abnormality.\\n•Procedure reported : MG Diagnostic Mammo Bilateral. \\n•Reason for procedure : diagnostic. \\n•Patient info : This patient is African American  or Black, \\nNon-Hispanic or Latino, and 59 years old.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe left breast with MLO view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a highly suggestive of \\nmalignancy is present, a biopsy is recommended.  \\n•Impressions : BI-RADS Category 5: highly suggestive of \\nmalignancy.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 23}, page_content='malignancy. \\n•Overall Assessment : Highly suggestive of malignancy.Figure 7: Example Multi-view Mammography BI-RADS 3-5 with Constructed Caption . We\\nprovide random sampled multi-view mammography with the corresponding caption constructed by\\nus. We highlight the image match exactly with the caption in a green bounding box.\\n24'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 24}, page_content='•Procedure reported : MG Diagnostic Bilateral w/ CAD. \\n•Reason for procedure : diagnostic. \\n•Patient info : This patient is African American  or Black, \\nNon-Hispanic or Latino, and 68 years old.  \\n•Image info : This is a 2D full-field digital mammogram of \\nthe left breast with CC view.  \\n•Breast composition : The breast is scattered fibro glandular \\ndensities.  \\n•Findings : The mammogram shows that a known biopsy-proven \\nmalignant mass is present.  \\n•Impressions : BI-RADS Category 6: known biopsy-proven \\nmalignancy.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18119.pdf', 'page': 24}, page_content='malignancy. \\n•Overall Assessment : Known biopsy-proven malignancy.Figure 8: Example Multi-view Mammography BI-RADS 6 with Constructed Caption . We\\nprovide random sampled multi-view mammography with the corresponding caption constructed by\\nus. We highlight the image match exactly with the caption in a green bounding box.\\n25'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 0}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nLOTUS : DIFFUSION -BASED VISUAL FOUNDATION MODEL\\nFOR HIGH-QUALITY DENSE PREDICTION\\nJing He1✱Haodong Li1✱Wei Yin2Yixun Liang1Leheng Li1Kaiqiang Zhou3\\nHongbo Zhang3Bingbing Liu3Yingcong Chen1,4 \\x00\\n1HKUST(GZ)2University of Adelaide3Huawei Noah’s Ark Lab4HKUST\\n{jhe812, hli736 }@connect.hkust-gz.edu.cn; yingcongchen@ust.hk\\nDepthAnything  V2\\nDepthAnything  V2Lotus (Ours)\\nLotus (Ours)\\nLotus (Ours)\\nLotus (Ours)DSINE\\nDSINE'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 0}, page_content='Lotus (Ours)\\nLotus (Ours)\\nLotus (Ours)DSINE\\nDSINE\\nAvg. RankOmnidataDPTHDNGenPerceptDepthAnything V2DepthAnythingLotus-DGeoWizardMarigold(LCM)MarigoldLotus-GTraining DataTraining DataOASISOmnidataEESNUGenPerceptOmnidata V2Lotus-DDSINEMarigoldGeoWizardStableNormalLotus-GAvg. RankAvg. Rank\\nAvg. Rank\\nFigure 1: We present Lotus , a diffusion-based visual foundation model for dense geometry predic-\\ntion. With minimal training data, Lotus achieves SoTA performance in two key geometry perception\\ntasks, i.e., zero-shot depth and normal estimation. “Avg. Rank” indicates the average ranking across'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 0}, page_content='all metrics, where lower values are better. Bar length represents the amount of training data used.\\n✱Both authors contributed equally (order randomized).\\x00Corresponding author.\\n1arXiv:2409.18124v1  [cs.CV]  26 Sep 2024'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nABSTRACT\\nLeveraging the visual priors of pre-trained text-to-image diffusion models offers a\\npromising solution to enhance zero-shot generalization in dense prediction tasks.\\nHowever, existing methods often uncritically use the original diffusion formula-\\ntion, which may not be optimal due to the fundamental differences between dense\\nprediction and image generation. In this paper, we provide a systemic analysis of\\nthe diffusion formulation for the dense prediction, focusing on both quality and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='efficiency. And we find that the original parameterization type for image gener-\\nation, which learns to predict noise, is harmful for dense prediction; the multi-\\nstep noising/denoising diffusion process is also unnecessary and challenging to\\noptimize. Based on these insights, we introduce Lotus , a diffusion-based visual\\nfoundation model with a simple yet effective adaptation protocol for dense predic-\\ntion. Specifically, Lotus is trained to directly predict annotations instead of noise,\\nthereby avoiding harmful variance. We also reformulate the diffusion process into'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='a single-step procedure, simplifying optimization and significantly boosting infer-\\nence speed. Additionally, we introduce a novel tuning strategy called detail pre-\\nserver, which achieves more accurate and fine-grained predictions. Without scal-\\ning up the training data or model capacity, Lotus achieves SoTA performance in\\nzero-shot depth and normal estimation across various datasets. It also significantly\\nenhances efficiency, being hundreds of times faster than most existing diffusion-\\nbased methods. Lotus’ superior quality and efficiency also enable a wide range of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='practical applications, such as joint estimation, single/multi-view 3D reconstruc-\\ntion, etc. Project page: lotus3d.github.io .\\n1 I NTRODUCTION\\nDense prediction is a fundamental task in computer vision, benefiting a wide range of applications,\\nsuch as 3D/4D reconstruction [Huang et al. (2024); Long et al. (2024); Wang et al. (2024); Lei\\net al. (2024)], tracking [Xiao et al. (2024); Song et al. (2024)], and autonomous driving [Yurtsever\\net al. (2020); Hu et al. (2023)]. Estimating pixel-level geometric attributes from a single image re-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='quires comprehensive scene understanding. Although deep learning has advanced dense prediction,\\nprogress is limited by the quality, diversity, and scale of training data, leading to poor zero-shot gen-\\neralization. Instead of merely scaling data and model size, recent works [Lee et al. (2024); Ke et al.\\n(2024); Fu et al. (2024); Xu et al. (2024)] leverage diffusion priors for zero-shot dense prediction.\\nThese studies demonstrate that text-to-image diffusion models like Stable Diffusion [Rombach et al.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='(2022)], pretrained on billions of images, possess powerful and comprehensive visual priors to ele-\\nvate dense prediction performance. However, most of these methods directly inherit the pre-trained\\ndiffusion models for dense prediction tasks, without exploring more suitable diffusion formulations.\\nThis oversight often leads to challenging issues. For example, Marigold [Ke et al. (2024)] directly\\nfine-tunes Stable Diffusion for image-conditioned depth generation. While it significantly improves\\ndepth estimation, its performance is still constrained by overlooking the fundamental differences'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='between dense prediction and image generation. Especially, its efficiency is also severely limited by\\nstandard iterative denoising processes and ensemble inferences.\\nMotivated by these concerns, we systematically analyze the diffusion formulation, trying to find a\\nbetter formulation to fit the pre-trained diffusion model into dense prediction. Our analysis yields\\nseveral important findings: ①The widely used parameterization, i.e., noise prediction, for diffusion-\\nbased image generation is ill-suited for dense prediction. It results in large prediction errors due'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='to harmful prediction variance at initial denoising steps, which are subsequently propagated and\\nmagnified throughout the entire denoising process (Sec. 4.1). ②Multi-step diffusion formulation is\\ncomputation-intensive and is prone to sub-optimal with limited data and resources. These factors\\nsignificantly hinder the adaptation of diffusion priors to dense prediction tasks, leading to decreased\\naccuracy and efficiency (Sec. 4.2). ③Though remarkable performance achieved, we observed that\\nthe model usually outputs vague predictions in highly-detailed areas (Fig. 8). This vagueness is'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 1}, page_content='attributed to catastrophic forgetting: the pre-trained diffusion models gradually lose their ability to\\ngenerate detailed regions during fine-tuning (Sec. 4.3).\\n2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content=\"Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nImage x Annotation y ℰ\\n𝐳𝐱𝐳𝐲\\n𝑡=Taddnoiseconcat.𝐳𝒕𝐲denoiserU-Net 𝑓!𝐳𝐲−𝑓!𝐳𝒕𝐲,𝐳𝐱,𝑡,𝑠%&Training Objective:𝑡=Tswitcher 𝑠𝐳𝐱−𝑓!𝐳𝒕𝐲,𝐳𝐱,𝑡,𝑠'&+((\\n❄\\n🔥\\n𝒛$𝐲𝒛$𝐱single-step\\nsingle-step𝑥(-prediction(image reconstruction)(predict annotation)detail preserver\\nFigure 3: Adaptation protocol of Lotus. After the pre-trained V AE encoder Eencodes the image\\nxand annotation yto the latent space: ①the denoiser U-Net model fθis fine-tuned using x0-\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content='prediction; ②we employ single-step diffusion formulation at time-step t=Tfor better coverage; ③\\nwe propose a novel detail preserver, to switch the model either to reconstruct the image or generate\\nthe dense prediction via a switcher s, ensuring a more fine-grained prediction. The noise zy\\nTin\\nbracket is used for our generative Lotus-G and is omitted for the discriminative Lotus-D .\\n1.On Single A800\\n2.We keep the original shape of input \\nimage during inference.\\n3.DA在2048下爆显存啦！\\nFigure 2: Inference time comparison in depth esti-\\nmation between Lotus and SoTA methods. Lotus'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content='mation between Lotus and SoTA methods. Lotus\\nis hundreds of times faster than Marigold and slightly\\nfaster than DepthAnything V2 at high resolutions (Our\\nperformance at high resolutions is also promising, as\\nevidenced on the ETH3D dataset presented in Tab. 1\\nand Fig. 11). DepthAnything V2’s inference time at\\n2048×2048 is not plotted because it requires >80GB\\ngraphic memory.Following our analysis, we propose Lo-\\ntus, a diffusion-based visual foundation\\nmodel for dense prediction, featuring a\\nsimple yet effective fine-tuning protocol\\n(see Fig. 3). First, Lotus is trained to di-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content='(see Fig. 3). First, Lotus is trained to di-\\nrectly predict annotations, thereby avoid-\\ning the harmful variance associated with\\nstandard noise prediction. Next, we intro-\\nduce a one-step formulation, i.e., one step\\nbetween pure noise and clean output, to\\nfacilitate model convergence and achieve\\nbetter optimization performance with lim-\\nited high-quality data. It also consider-\\nably boosts both training and inference ef-\\nficiency. Moreover, we implement a novel\\ndetail preserver through a task switcher,\\nallowing the model either to generate an-\\nnotations or reconstruct the input images.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content='notations or reconstruct the input images.\\nIt preserves fine-grained details in the in-\\nput image during dense annotation gener-\\nation, achieving higher performance with-\\nout compromising efficiency, requiring ad-\\nditional parameters, or being affected by\\nsurface textures.\\nTo validate Lotus, we conduct extensive experiments on two primary geometric dense predic-\\ntion tasks: zero-shot monocular depth and normal estimation. The results demonstrate that Lotus\\nachieves SoTA performance on these tasks across a wide range of evaluation datasets. Compared to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content='traditional discriminative methods, Lotus delivers remarkable results with only 59K training sam-\\nples by effectively leveraging the powerful diffusion priors. Among generative approaches, Lotus\\nalso outperforms previous methods in both accuracy and efficiency, being significantly faster than\\nmethods like Marigold [Ke et al. (2024)] (Fig. 2). Beyond these improvements, Lotus seamlessly\\nsupports various applications, such as joint estimation, single/multi-view 3D reconstruction, etc.\\nIn conclusion, our key contributions are as follows:'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 2}, page_content='In conclusion, our key contributions are as follows:\\n• We systematically analyze the diffusion formulation and find their parameterization type,\\ndesigned for image generation, is unsuitable for dense prediction and the computation-\\nintensive multi-step diffusion process is also unnecessary and challenging to optimize.\\n3'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\n• We propose a novel detail preserver that ensures more accurate dense predictions especially\\nin detail-rich areas, without compromising efficiency, introducing additional network pa-\\nrameters, or being affected by surface textures.\\n• Based on our insights, we introduce Lotus , a diffusion-based visual foundation model for\\ndense prediction with simple yet effective fine-tuning protocol. Lotus achieves SoTA per-\\nformance on both zero-shot monocular depth and surface normal estimation. It also enables'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='a wide range of applications.\\n2 R ELATED WORKS\\n2.1 T EXT-TO-IMAGE GENERATIVE MODELS\\nIn the field of text-to-image generation, the evolution of methodologies has transitioned from gen-\\nerative adversarial networks (GANs) [Goodfellow et al. (2014); Zhang et al. (2017; 2018; 2021);\\nHe et al. (2022); Karras et al. (2019; 2020; 2021); Zhang et al. (2017; 2018); Xu et al. (2018);\\nZhang et al. (2021)] to advanced diffusion models [Ho et al. (2020); Ramesh et al. (2022); Saharia\\net al. (2022); Ramesh et al. (2021); Nichol et al. (2021); Chen et al. (2023); Rombach et al. (2022);'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='Ramesh et al. (2021)]. A series of diffusion-based methods such as GLIDE [Nichol et al. (2021)],\\nDALL·E2 [Ramesh et al. (2022)], and Imagen [Saharia et al. (2022)] have been introduced, offering\\nenhanced image quality and textual coherence. The Stable Diffusion (SD) [Rombach et al. (2022)],\\ntrained on large-scale LAION-5B dataset [Schuhmann et al. (2022)], further enhances the generative\\nquality, becoming the community standard. In our paper, we aim to leverage the comprehensive and\\nencyclopedic visual priors of SD to facilitate zero-shot generalization for dense prediction tasks.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='2.2 G ENERATIVE MODELS FOR DENSE PERCEPTION\\nCurrently, a notable trend involves adopting pre-trained generative models, particularly diffusion\\nmodels, into dense prediction tasks. Marigold [Ke et al. (2024)] and GeoWizard [Fu et al. (2024)]\\ndirectly apply the standard diffusion formulation and the pre-trained parameters, without address-\\ning the inherent differences between image generation and dense prediction, leading to constrained\\nperformance. Their efficiency is also severely limited by standard iterative denoising processes and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='ensemble inferences. In this paper, we propose a novel diffusion formulation tailored to the charac-\\nteristics of dense prediction. Aiming to fully leveraging the pre-trained diffusion’s powerful visual\\npriors, Lotus enables more accurate and efficient predictions, finally achieving SoTA performance.\\nMore recent works, GenPercept [Xu et al. (2024)] and StableNormal [Ye et al. (2024)], also adopted\\nsingle-step diffusion. However, GenPercept [Xu et al. (2024)] first removes noise input for de-\\nterministic characteristic based on DMP [Lee et al. (2024)], and then adopts one-step strategy to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='avoid surface texture interference. It lacks systematic analysis of the diffusion formulation, only\\ntreats the U-Net as a deterministic backbone and still falls short in performance. In contrast, Lotus\\nsystematically analyzes the standard stochastic diffusion formulation for dense prediction and pro-\\nposes innovations such as the detail preserver to improve accuracy especially in detailed area, finally\\ndelivering much better results (Tab. 1 and Fig. 11). Additionally, Lotus is a stochastic model, in con-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='trast to GenPercept’s deterministic nature, enabling uncertainty predictions. StableNormal [Ye et al.\\n(2024)] predicts normal maps through a two-stage process. While the first stage produces coarse\\nnormal maps with single-step diffusion, the second stage performs refinement still with iterative\\ndiffusion which is computation-intensive. In comparison, Lotus not only achieves fine-grained pre-\\ndictions via the novel detail preserver without extra stages or parameters, but also delivers much\\nsuperior results (Tab. 2 and Fig. 12) thanks to our designed diffusion formulation that better fits'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 3}, page_content='pre-trained diffusion for dense prediction.\\n2.3 M ONOCULAR DEPTH AND NORMAL PREDICTION\\nMonocular depth and normal prediction are two crucial dense prediction tasks. Solving them typ-\\nically demands comprehensive scene understanding capability. Starting from Eigen et al. (2014),\\nearly CNN-based methods for depth prediction, such as Fu et al. (2018), Lee et al. (2019), Yuan\\net al. (2022), focus only on specific domains. Subsequently, in pursuit of a generic depth estima-\\ntor, many methods expand model capacity and train on larger and more diverse datasets, such as\\n4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\naddnoise\\nImage x Annotation y ℰ\\n𝐳𝐱𝐳𝐲\\n𝑡∈[1,T]concat.𝐳𝒕𝐲denoiserU-Net 𝑓!𝜖−𝑓!𝐳𝒕𝐲,𝐳𝐱,𝑡%Training Objective:𝑡∈[1,T]\\n❄\\n🔥\\n𝜖multi-step\\nmulti-step𝜖-prediction(predict noise)\\nFigure 4: Adaptation protocol of Direct Adaptation. Starting with a pre-trained Stable Diffusion\\nmodel, image xand annotation yare encoded using the pre-trained V AE. The noisy annotation zy\\ntis\\nobtained by adding noise at level t∈[1, T]. The U-Net input layer is coupled to accommodate the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='concatenated inputs and then fine-tuned using the standard diffusion objective, ϵ-prediction, under\\nthe original multi-step formulation.\\nDiverseDepth [Yin et al. (2021a)] and MiDaS [Ranftl et al. (2020)]. DPT [Ranftl et al. (2021)] and\\nOmnidata [Eftekhar et al. (2021)] are further proposed based on vision transformer [Ranftl et al.\\n(2021)], significantly enhancing performance. LeRes [Yin et al. (2021b)] and HDN [Zhang et al.\\n(2022)] further introduce novel training strategies and multi-scale depth normalization to improve'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='predictions in detailed areas. More recently, the DepthAnything series [Yang et al. (2024a;b)] and\\nMetric3D series [Yin et al. (2023); Hu et al. (2024)] collect and leverage millions of labeled data\\nto develop more powerful estimators. Normal prediction follows the same trend. Starting with the\\nearly CNN-based methods like OASIS [Chen et al. (2020)], EESNU [Bae & Davison (2021)] and\\nOmnidata series [Eftekhar et al. (2021); Kar et al. (2022)] expand the model capacity and scale up\\nthe training data. Recently, DSINE [Bae & Davison (2024)] achieves SoTA performance by rethink-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='ing inductive biases for surface normal estimation. In our paper, we focus on leveraging pre-trained\\ndiffusion priors to enhance zero-shot dense predictions, rather than expanding model capacity or\\nrelying on large training data, which avoids the need for intensive resources and computation.\\n3 P RELIMINARIES\\nDiffusion Formulation for Dense Prediction. Following Ke et al. (2024) and Fu et al. (2024), we\\nalso formulate dense prediction as an image-conditioned annotation generation task based on Stable'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='Diffusion [Rombach et al. (2022)], which performs the diffusion process in low-dimensional latent\\nspace for computational efficiency. First, there is a pair of auto-encoders {E(·),D(·)}trained to\\nmap between RGB space and latent space, i.e.,E(x) =zx,D(zx)≈x. The auto-encoder also maps\\nbetween dense annotations and latent space effectively, i.e.,E(y) =zy,D(zy)≈y[Ke et al. (2024);\\nFu et al. (2024); Xu et al. (2024); Ye et al. (2024)]. Following Ho et al. (2020), Stable Diffusion\\nestablishes a pair of forward nosing and reversal denoising processes in latent space. In forward'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='process, Gaussian noise is gradually added at levels t∈[1, T]into sample zyto obtain the noisy\\nsample zy\\nt:\\nzy\\nt=√αtzy+√\\n1−αtϵ, (1)\\nwhere ϵ∼ N(0, I),αt:=Qt\\ns=1(1−βs), and{β1, β2, . . . , β T}is the noise schedule with Tsteps.\\nAt time-step T, the sample zyis degraded to pure Gaussian noise. In the reversal process, a neural\\nnetwork fθ, usually a U-Net model [Ronneberger et al. (2015)), is trained to iteratively remove noise\\nfrom zy\\ntto predict the clean sample zy. The network is trained by sampling a random t∈[1, T]and\\nminimizing the loss function Lt.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 4}, page_content='minimizing the loss function Lt.\\nParameterization Types. To enable gradient computation for network training, there are two basic\\nparameterizations of the loss function Lt.①ϵ-prediction [Ho et al. (2020)]: the model fθlearns to\\npredict the added noise ϵ;②x0-prediction [Ho et al. (2020)]: the model fθlearns to directly predict\\nthe clean sample zy. The loss functions for these parameterizations are formulated as:\\nLϵ\\nt=||ϵ−fϵ\\nθ(zy\\nt,zx, t)||2, (2)\\nLz\\nt=||zy−fz\\nθ(zy\\nt,zx, t)||2, (3)\\n5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\n𝜏=1𝜏\\t=1000𝜏\\t=600𝜏\\t=200\\nInput Image\\n𝜀-prediction, seed 1𝜀-prediction, seed 2𝑥!-prediction, seed 1𝑥!-prediction, seed 2𝜏=1𝜏\\t=1000𝜏\\t=600𝜏\\t=200\\nFigure 5: Comparisons among different parameterizations using various seeds. All models are\\ntrained on Hypersim [Roberts et al. (2021)] and tested on the input image for depth estimation. The\\nstandard DDIM sampler is used with 50 denoising steps. Four steps are selected for clear illustration.\\nFrom left (larger τ) to right (smaller τ) is the iterative denoising process.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='where f∗\\nθis the denoiser model to be learnt, ∗ ∈ { ϵ,z}.ϵ-prediction is commonly chosen as the\\nstandard for parameterizing the denoising model, as it empirically achieves high-quality image gen-\\neration with fine details and realism.\\nDenoising Process. DDIM [Song et al. (2020)] is a key technique for multi-step diffusion models\\nto achieve fast sampling, which implements an implicit probabilistic model that can significantly\\nreduce the number of denoising steps while maintaining output quality. Formally, the denoising\\nprocess from zy\\nτtozy\\nτ−1is:\\nzy\\nτ−1=p\\nατ−1ˆ zy\\nτ+direction (zy'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='τtozy\\nτ−1is:\\nzy\\nτ−1=p\\nατ−1ˆ zy\\nτ+direction (zy\\nτ) +στϵτ, (4)\\nwhere ˆ zy\\nτis the predicted clean sample at the denoising step τ, direction (zy\\nτ)represents the direction\\npointing to zy\\nτandστcan be set to 0if deterministic inference is needed. And τ∈ {τ1, τ2, . . . , τ S},\\nan increasing sub-sequence of the time-step set [1, T], is used for fast sampling. During inference,\\nDDIM iteratively denoises the sample from τStoτ1to obtain the clean one.\\n4 M ETHODOLOGY\\nWe start our analysis by directly adapting the original diffusion formulation with minimal modifica-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='tions as illustrated in Fig. 41. We call this starting point as “ Direct Adaptation ”. Direct Adaptation\\nis optimized using the standard diffusion objective as formulated in Eq. 2 and inferred by standard\\nmulti-step DDIM sampler. As shown in Tab. 3, Direct Adaptation fails to achieve satisfactory per-\\nformance. In following sections, we will systematically analyze the key factors that affect adaptation\\nperformance step by step: parameterization types (Sec. 4.1); number of time-steps (Sec. 4.2); and\\nthe novel detail preserver (Sec. 4.3).\\n4.1 P ARAMETERIZATION TYPES'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='4.1 P ARAMETERIZATION TYPES\\nThe type of parameterization is a vital configuration, it not only determines the loss function dis-\\ncussed in Sec. 3, but also influences the inference process (Eq. 4). During inference, the predicted\\nclean sample ˆ zy\\nτ, a key component in Eq. 4, is calculated according to different parameterizations2\\nϵ-prediction: ˆ zy\\nτ=1√ατ(zy\\nτ−√\\n1−ατfϵ\\nθ(zy\\nτ,zx, τ))\\nx0-prediction: ˆ zy\\nτ=fz\\nθ(zy\\nτ,zx, τ)(5)\\nIn the community, ϵ-prediction is chosen as the standard for image generation. However, it is not'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 5}, page_content='effective for dense prediction task. In the following, we will discuss the impact of different parame-\\nterization types in denoising inference process for dense prediction task.\\n1Details of “Direct Adaptation” will be provided in the supplementary materials.\\n2The latest parameterization, v-prediction, combines ϵ-prediction and x0-prediction, producing results that\\nare intermediate between the two. Please see the supplementary materials for more details.\\n6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\n0 200 400 600 800 1000\\nDenoising Step ()\\n89101112NYUv2 AbsRel (%) \\nNYUv2 AbsRel vs. Denoising Step\\nPred. Type=x0\\nPred. Type=\\nFigure 6: Quantitative evaluation of the predicted\\ndepth maps ˆ zy\\nτalong the denoising process. The\\nexperimental settings are same as Fig. 5. Six steps\\nare selected for illustration. The banded regions\\naround each line indicate the variance, wider areas\\nrepresenting larger variance.Insights from the literature [Benny & Wolf\\n(2022); Salimans & Ho (2022)] reveal that'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='(2022); Salimans & Ho (2022)] reveal that\\nϵ-prediction introduces larger pixel variance\\ncompared to x0-prediction, especially at the\\ninitial denoising steps (large τ). This vari-\\nance mainly originates from the noise in-\\nput. Specifically, for ϵ-prediction in Eq. 5,\\nat initial denoising step, τ→T, the value\\n1√ατ→+∞. Even small prediction vari-\\nance from fϵ\\nθ(zy\\nτ,zx, τ)will be amplified\\nsignificantly, resulting in large variance of\\npredicted ˆ zy\\nτ. In contrast, there is no coeffi-\\ncient for x0-prediction to re-scale the model\\noutput, achieving more stable predictions of\\nˆ zy'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content=\"output, achieving more stable predictions of\\nˆ zy\\nτat initial denoising steps. Subsequently,\\nthe predicted ˆ zy\\nτis used in Eq. 4. This it-\\nerative denoising process will preserve and\\namplify the influence of large variance. In\\nEq. 4, the coefficients√ατ−1of predicted\\nˆ zy\\nτare same across two parameterizations,\\nand other terms are of the same order of magnitude. Therefore, the ˆ zy\\nτpredicted by ϵ-prediction,\\nwhich has larger variance, exerts a more significant influence on the denoising process.\\n5K 10K 19K 39K\\nTraining Data67891011NYUv2 AbsRel (%) \\nNYUv2 AbsRel vs. Training Data\\nT'=1\\nT'=2\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content=\"NYUv2 AbsRel vs. Training Data\\nT'=1\\nT'=2\\nT'=5\\nT'=10\\nT'=100\\nT'=1000\\nFigure 7: Comparisons among various training\\ntime-steps and data scales evaluated on NYUv2\\nin depth estimation. All models are fine-tuned on\\nHypersim using x0-prediction. During inference, if\\nT′>50, the DDIM sampler is used with 50 denois-\\ning steps; otherwise, the number of denoising steps\\nis equal to T′. The results demonstrate improved\\nperformance with decreased training time-steps. The\\nsingle-step diffusion formulation ( T′= 1) exhibits\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='single-step diffusion formulation ( T′= 1) exhibits\\nbest performance across different data volumes.We take the depth estimation as an example.\\nDuring the inference process, we compute\\nthe predicted depth map ˆ zy\\nτat each denois-\\ning step τ. As illustrated in Fig. 5, the depth\\nmaps predicted by ϵ-prediction significantly\\nvary under different seeds while those pre-\\ndicted by x0-prediction are more consistent.\\nAlthough the large variance enhances diver-\\nsity for image generation, it lead to unsta-\\nble predictions in dense prediction tasks, po-\\ntentially resulting in significant errors. For'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='tentially resulting in significant errors. For\\nexample in Fig. 5, the “dark gray cabinet”\\n(highlighted in red circles) maybe wrongly\\nconsidered as an “opened door” with sig-\\nnificantly larger depth. While the predicted\\ndepth map looks more and more “plausi-\\nble”, the error gradually propagates to the fi-\\nnal prediction ( τ= 1) along the denoising\\nprocess, indicating the persistent influence\\nof the large variance. We further quantita-\\ntively measure the predicted depth maps by\\nthe absolute mean relative error (AbsRel) on\\nNYUv2 dataset [Silberman et al. (2012)]. As'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='NYUv2 dataset [Silberman et al. (2012)]. As\\nshown in Fig. 6, ϵ-prediction exhibits higher\\nerror with much larger variance compared to\\nx0-prediction at the initial denoising steps\\n(τ→T), and the prediction error propagates along the denoising process with higher slope. In\\ncontrast, x0-prediction, directly predicting ˆ zy\\nτwithout any coefficients to amplify the prediction\\nvariance, yields more stable and correct dense predictions than ϵ-prediction.\\nIn conclusion, to mitigate the errors from large variance that adversely affect the performance of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 6}, page_content='dense prediction, we replace the standard ϵ-prediction with the more tailored x0-prediction.\\n7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nInput Image Reconstruction\\nw/o Preserver w/ PreserverInput Image Reconstruction\\nw/o Preserver w/ Preserver\\nFigure 8: Depth maps w/andw/o the detail preserver and reconstruction outputs. Fine-tuning\\nthe diffusion model for dense prediction tasks can potentially degrade its ability to generate highly\\ndetailed images, resulting in blurred predictions in regions with rich detail. To preserve these fine-\\ngrained details, we introduce a detail preserver that incorporates an additional reconstruction task,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='enhancing the model’s capacity to produce more accurate dense annotations.\\n4.2 N UMBER OF TIME-STEPS\\nAlthough x0-prediction can improve the prediction quality, the multi-step diffusion formulation still\\nleads to the propagation of predicted errors during the denoising process (Fig. 5, 6). Furthermore,\\nutilizing multiple time-steps enhances the model’s capacity, typically requiring large-scale training\\ndata to optimize and is beneficial—or even necessary—for complex tasks such as image generation.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='However, for simpler tasks like dense prediction, where large-scale, high-quality training data is also\\nscarce, employing multiple time-steps can make the model difficult to optimize. Additionally, train-\\ning/inferring a multi-step diffusion model is slow and computation-intensive, hindering its practical\\napplication.\\nTherefore, to address these challenges, we propose fine-tuning the pre-trained diffusion model with\\nfewer training time steps. Specifically, the original set of training time-steps is defined as [1, T] ='),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='{1,2,3, . . . , T }, where Tdenotes the total number of original training time-steps. We fine-tune the\\npre-trained diffusion model using a sub-sequence derived from this set. We define the length of this\\nsub-sequence as T′, where T′≪TandTis divisible by T′. This sub-sequence is obtained by\\nevenly sampling the original set at intervals, defined as:\\n{ti=i·k|i= 1,2, . . . , T′}, (6)\\nwhere k=T/T′is the sampling interval. During inference, the DDIM denoises the sample from\\nnoise to annotation using the same sub-sequence.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='noise to annotation using the same sub-sequence.\\nAs illustrated in Fig. 7, we conduct experiments by varying the number of time-steps T′under\\nx0-prediction. The results clearly show that the performance gradually improves as the number of\\ntime-steps is reduced, no matter the training data scales, culminating in the best result when re-\\nduced to only a single step. We further consider more strict scenarios with more limited training\\ndata to assess its impact on model optimization. As depicted in Fig. 7, these experiments reveal'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='that the multi-step formulation is more sensitive to increases in training data scales compared with\\nsingle-step. Notably, the single-step formulation consistently yields lower prediction errors and\\ndemonstrates greater stability. Although it is conceivable that multi-step and single-step formula-\\ntions might achieve comparable performance with unlimited high-quality data, it’s expensive and\\nsometimes impractical in dense prediction.\\nDecreasing the number of denoising steps can reduce the optimization space of the diffusion model,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 7}, page_content='leading to more effective and efficient adaption, as suggested by the above phenomenon. There-\\nfore, for better adaptation performance under limited resource, we reduce the number of training\\ntime-steps of diffusion formulation to only one, and fixing the only time-step ttoT. Addition-\\nally, the single-step formulation is much more computationally efficient. It also naturally prevents\\nthe harmful error propagation as discussed in Sec. 4.1, further enhancing the diffusion’s adaptation\\nperformance in dense prediction.\\n8'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nInput Image Seed 0 Seed 1 Seed 2 Seed 3 Uncertainty Map\\nInput Image Seed 0 Seed 1 Seed 2 Seed 3 Uncertainty Map\\nFigure 9: Depth maps of multiple inferences and uncertainty maps. Areas like the sky, object\\nedges, and intricate details ( e.g., cat whiskers) typically exhibit high uncertainty.\\n4.3 D ETAIL PRESERVER\\nDespite the effectiveness of the above designs, the model still struggles with processing detailed\\nareas (Fig. 8, w/o Preserver). The original diffusion model excels at generating detailed images.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='However, when adapted to predict dense annotations, it can lose such detailed generation ability, due\\nto unexpected catastrophic forgetting [Zhai et al. (2023); Du et al. (2024)]. This leads to challenges\\nin predicting dense annotations in intricate regions.\\nTo preserve the rich details of the input images, we introduce a novel regularization strategy called\\nDetail Preserver . Inspired by previous works [Long et al. (2024); Fu et al. (2024)], we utilize a task\\nswitcher s∈ {sx, sy}, enabling the denoiser model fθto either generate annotation or reconstruct'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='the input image. When activated by sy, the model focuses on predicting annotation. Conversely,\\nwhen sxis selected, it reconstructs the input image. The switcher sis a one-dimensional vector\\nencoded by the positional encoder and appended with the time embeddings of diffusion model,\\nensuring seamless domain switching without mutual interference. This dual capability enables the\\ndiffusion model to make detailed predictions and thus leading to better performance. Overall, the\\nloss function Ltis:\\nLt=||zx−fθ(zy\\nt,zx, t, sx)||2+||zy−fθ(zy\\nt,zx, t, sy)||2, (7)\\nwhere t=Tand thus zy'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='t,zx, t, sy)||2, (7)\\nwhere t=Tand thus zy\\ntis a pure Gaussian noise.\\n4.4 S TOCHASTIC NATURE OF DIFFUSION MODEL\\nImagexℰ\\n𝐳𝐱\\nconcat.𝐳𝑻𝐲denoiserU-Net 𝑓!𝑡=Tswitcher 𝑠\"Prediction#𝒛𝐲𝒟((\\nFigure 10: Inference Pipeline of Lotus. The\\nnoise zy\\nTin bracket is used for Lotus-G and\\nomitted for Lotus-D .One major characteristic of generative models is\\ntheir stochastic nature, which, in image generation,\\nenables the production of diverse outputs. In per-\\nception tasks like dense prediction, this stochasticity\\nhas the potential to allow the model generating pre-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='has the potential to allow the model generating pre-\\ndictions with uncertainty maps. Specifically, for any\\ninput image, we can conduct multiple inferences us-\\ning different initialization noises and aggregate these\\npredictions to create its uncertainty map. Thanks\\nto our systematic analysis and tailored fine-tuning\\nprotocol, our method effectively reduces excessive\\nflickering (large variance), only allowing for more\\naccurate uncertainty calculations in naturally uncer-\\ntain areas, such as the sky, object edges, and fine\\ndetails ( e.g.cat whiskers), as shown in Fig. 9.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 8}, page_content='details ( e.g.cat whiskers), as shown in Fig. 9.\\nMost existing perception models are deterministic.\\nTo align with these, we can remove the noise input zy\\ntand only input the encoded image features zx\\nto the U-Net denoiser. The model still performs well. In this paper, we finally present two versions\\n9'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nof Lotus: Lotus-G (generative) with noise input and Lotus-D (discriminative) without noise input,\\ncatering to different needs.\\n4.5 I NFERENCE\\nThe inference pipeline is illustrated in Fig. 10. We initialize the annotation map with standard\\nGaussian noise zy\\nT, and encode the input image into its latent code zx. The noise zy\\nTand the image\\nzxare concatenated and fed into the denoiser U-Net model. In our single-step formulation, we'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='sett=Tand the switcher to sy. The denoiser U-Net model then predicts the latent code of the\\nannotation map. The final annotation map is decoded from the predicted latent code via the V AE\\ndecoder. For deterministic prediction, we eliminate the Gaussian noise zy\\nTand only feed the latent\\ncode of the input image into the denoiser.\\n5 E XPERIMENTS\\n5.1 E XPERIMENTAL SETTINGS\\nImplementation details. We implement Lotus based on Stable Diffusion V2 [Rombach et al.\\n(2022)], with text conditioning disabled. During training, we fix the time-step t= 1000 . To optimize'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='the model, we utilize the standard Adam optimizer with the learning rate 3×10−5. All experiments\\nare conducted on 8 NVIDIA A800 GPUs and the total batch size is 128. For our discriminative vari-\\nant, we train for 4,000 steps, which takes ∼8.1 hours, while for the generative variant, we extend\\ntraining to 10,000 steps, requiring ∼20.3 hours.\\nTraining Datasets. Both depth and normal estimation are trained on two synthetic dataset covering\\nindoor and outdoor scenes.\\n①Hypersim [Roberts et al. (2021)] is a photorealistic synthetic dataset featuring 461 indoor scenes.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='We use the official training split, which contains approximately 54K samples. After filtering out\\nincomplete samples, around 39K samples remain, all resized to 576×768for training.\\n②Virtual KITTI [Cabon et al. (2020)] is a synthetic street-scene dataset with five urban scenes under\\nvarious imaging and weather conditions. We utilize four of these scenes for training, comprising\\nabout 20K samples. All samples are cropped to 352×1216 , with the far plane set at 80 meters.\\nFollowing Marigold [Ke et al. (2024)], we employ a mixed dataset strategy for training. For each'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='batch, we probabilistically choose one of the two datasets and then draw samples from it ( Hypersim\\n90% and Virtual KITTI 10%). This approach yields better performance on both indoor and outdoor\\nreal datasets compared to training on a single synthetic dataset.\\nEvaluation Datasets. ①For affine-invariant depth estimation, we evaluate on 4 real-world datasets\\nthat are not seen during training: NYUv2 [Silberman et al. (2012)) and ScanNet [Dai et al. (2017))\\nall contain images of indoor scenes; KITTI [Geiger et al. (2013)) contains various outdoor scenes;'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='ETH3D [Schops et al. (2017)), a high-resolution dataset, containing both indoor and outdoor scenes.\\n②For surface normal prediction, we employ 4 datasets for evaluation: NYUv2 [Silberman et al.\\n(2012)), ScanNet [Dai et al. (2017)), and iBims-1 [Koch et al. (2018)) contain real indoor scenes;\\nSintel [Butler et al. (2012)) contains highly dynamic outdoor scenes.\\nMetrics. ①For affine-invariant depth, we follow the evaluation protocol from [Ranftl et al. (2020);\\nKe et al. (2024); Yang et al. (2024a;b)], aligning the estimated depth predictions with available'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='ground truths using least-squares fitting. The accuracy of the aligned predictions is assessed using\\ntheabsolute mean relative error (AbsRel), i.e.,1\\nMPM\\ni=1|ai−di|/di, where Mis the total number\\nof pixels, aiis the predicted depth map and direpresents the ground truth. We also report δ1and\\nδ2, the proportion of pixels satisfying Max (ai/di, di/ai)<1.25and<1.252respectively.\\n②For surface normal, following [Bae & Davison (2024); Ye et al. (2024)], we evaluate the predic-\\ntions of Lotus by measuring the mean angular error for pixels with available ground truth. Addition-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 9}, page_content='ally, we report the percentage of pixels with an angular error below 11.25◦and30◦.\\n③For all tasks, we report the Avg. Rank , which indicates the average ranking of each method across\\nvarious datasets and evaluation metrics. A lower value signifies better overall performance.\\n10'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nTable 1: Quantitative comparison on zero-shot affine-invariant depth estimation between Lotus\\nand SoTA methods. The upper section lists discriminative methods, the lower lists generative ones.\\nThe best and second best performances are highlighted. Lotus-G outperforms all others methods\\nwhile Lotus-D is slightly inferior to DepthAnything. Please note that DepthAnything is trained on\\n62.6M images while Lotus is only trained on 0.059M images.§indicates results revised by ourselves.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='⋆denotes the method relies on pre-trained Stable Diffusion.\\nMethodTraining NYUv2 (Indoor) KITTI (Outdoor) ETH3D (Various) ScanNet (Indoor) Avg.\\nData AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑Rank\\nDiverseDepth 320K 11.7 87.5 - 19.0 70.4 - 22.8 69.4 - 10.9 88.2 - 9.5\\nMiDaS 2M 11.1 88.5 - 23.6 63.0 - 18.4 75.2 - 12.1 84.6 - 9.5\\nLeRes 354K 9.0 91.6 - 14.9 78.4 - 17.1 77.7 - 9.1 91.7 - 7.6\\nOmnidata 12.2M 7.4 94.5 - 14.9 83.5 - 16.6 77.8 - 7.5 93.6 - 6.4\\nDPT 1.4M 9.8 90.3 - 10.0 90.1 - 7.8 94.6 - 8.2 93.4 - 5.5\\nHDN 300K 6.9 94.8 - 11.5 86.7 - 12.1 83.3 - 8.0 93.9 - 5.0'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='HDN 300K 6.9 94.8 - 11.5 86.7 - 12.1 83.3 - 8.0 93.9 - 5.0\\nGenPercept⋆§74K 5.6 96.0 99.2 13.0 84.2 - 7.0 95.6 98.8 6.2 96.1 99.1 3.8\\nDepthAnything V2 62.6M 4.5 97.9 99.3 7.4 94.6 98.6 13.1 86.5 - 4.2 97.8 99.3 2.9\\nLotus-D (Ours)⋆59K 5.3 96.7 99.2 9.3 92.8 98.8 6.8 95.3 98.9 6.0 96.3 99.1 2.5\\nDepthAnything 62.6M 4.3 98.1 99.6 7.6 94.7 99.2 12.7 88.2 - 4.3 98.1 99.6 2.0\\nGeoWizard⋆§280K 5.6 96.3 99.1 14.4 82.0 96.6 6.6 95.8 98.4 6.4 95.0 98.4 3.3\\nMarigold (LCM)§⋆74K 6.1 95.8 99.0 9.8 91.8 98.7 6.8 95.6 99.0 6.9 94.6 98.6 2.9'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='Marigold⋆74K 5.5 96.4 99.1 9.9 91.6 98.7 6.5 95.9 99.0 6.4 95.2 98.8 1.8\\nLotus-G (Ours)⋆59K 5.4 96.6 99.2 11.3 87.7 97.8 6.2 96.1 99.0 6.0 96.0 99.0 1.5\\nTable 2: Quantitative comparison on zero-shot surface normal estimation between Lotus and\\nSoTA methods. Discriminative methods are shown in the upper section, generative methods in the\\nlower. Both Lotus-D andLotus-G outperform all other methods.‡refers the Marigold normal\\nmodel as detailed in this link.⋆denotes the method relies on pre-trained Stable Diffusion.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='MethodTraining NYUv2 (Indoor) ScanNet (Indoor) iBims-1 (Indoor) Sintel (Outdoor) Avg.\\nData m.↓11.25◦↑30◦↑m.↓11.25◦↑30◦↑m.↓11.25◦↑30◦↑m.↓11.25◦↑30◦↑Rank\\nOASIS 110K 29.2 23.8 60.7 32.8 15.4 52.6 32.6 23.5 57.4 43.1 7.0 35.7 7.0\\nOmnidata 12.2M 23.1 45.8 73.6 22.9 47.4 73.2 19.0 62.1 80.1 41.5 11.4 42.0 5.3\\nEESNU 2.5M 16.2 58.6 83.5 - - - 20.0 58.5 78.2 42.1 11.5 41.2 4.4\\nGenPercept⋆74K 18.2 56.3 81.4 17.7 58.3 82.7 18.2 64.0 82.0 37.6 16.2 51.0 3.7\\nOmnidata V2 12.2M 17.2 55.5 83.0 16.2 60.2 84.7 18.2 63.9 81.1 40.5 14.7 43.5 3.6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='DSINE 160K 16.4 59.6 83.5 16.2 61.0 84.4 17.1 67.4 82.3 34.9 21.5 52.7 1.8\\nLotus-D (Ours)⋆59K 16.8 58.2 83.6 15.3 62.9 85.7 17.7 64.9 82.5 34.6 20.5 55.8 1.6\\nMarigold‡⋆74K 20.9 50.5 - 21.3 45.6 - 18.5 64.7 - - - - 4.2\\nGeoWizard⋆280K 18.9 50.7 81.5 17.4 53.8 83.5 19.3 63.0 80.3 40.3 12.3 43.5 3.2\\nStableNormal⋆250K 18.6 53.5 81.7 17.1 57.4 84.1 18.2 65.0 82.4 36.7 14.1 50.7 2.0\\nLotus-G (Ours)∗59K 16.9 59.1 83.2 15.3 64.0 85.2 17.5 66.1 82.7 35.2 19.9 54.8 1.0\\n5.2 Q UALITATIVE AND QUANTITATIVE COMPARISONS\\nDepth Estimation. As shown in Tab. 1, Lotus-G achieves the best comprehensive performance'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='compared to all generative baselines on zero-shot affine-invariant depth estimation. Notice that we\\nonly require single step denoising process, significantly boosting the inference speed as shown in\\nTable 3: Ablation studies on the step-by-step design of our adaptation protocol for fitting pre-trained\\ndiffusion models into dense prediction. Here we show the results in monocular depth estimation.\\nMethodTraining NYUv2 (Indoor) KITTI (Outdoor) ETH3D (Various) ScanNet (Indoor)\\nData AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='Data AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑AbsRel↓δ1↑δ2↑\\nDirect Adaptation 39K 11.551 87.692 96.122 20.164 70.403 90.996 19.894 76.464 87.960 15.726 78.885 93.651\\n+x0-prediction 39K 8.332 92.769 97.941 17.008 74.969 93.611 11.075 87.952 94.978 10.212 89.130 97.181\\n+Single Time-step 39K 5.587 96.272 99.113 13.262 83.210 97.237 7.586 94.143 97.678 6.262 95.394 98.791\\n+Detail Preserver 39K 5.555 96.303 99.118 13.170 83.657 97.454 7.147 95.000 98.058 6.201 95.470 98.814\\n+Mixture Dataset (Lotus-G )59K 5.425 96.597 99.156 11.324 87.692 97.780 6.172 96.077 98.980 6.024 96.026 99.730'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 10}, page_content='−Noise Input ( Lotus-D ) 59K 5.311 96.733 99.186 9.662 91.637 98.643 6.757 95.382 98.992 5.786 96.339 99.136\\n11'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 11}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nInput Image DepthAnythingV2Marigold(LCM)Lotus-DLotus-GGround TruthNYUv2KITTIScanNetETH3D\\nFigure 11: Qualitative comparison on zero-shot affine-invariant depth estimation. Lotus\\ndemonstrates higher accuracy especially in detailed areas.\\nInput Image DSINEStableNormalLotus-DLotus-GGround TruthNYUv2ScanNetiBims-1Sintel\\nFigure 12: Qualitative comparison on zero-shot surface normal estimation. Lotus offers im-\\nproved accuracy particularly in complex regions.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 11}, page_content='proved accuracy particularly in complex regions.\\nFig. 2. Lotus-D also performs well, though it is slightly inferior to DepthAnything. However,\\nit is worthy to notice that Lotus is trained on only 0.059M images compared to DepthAnything’s\\n62.6M images. In Fig. 11, we further compare the performance of our Lotus with other methods\\nin detailed areas. The quantitative results obviously demonstrate that our method can produce much\\nfiner and more accurate depth predictions, particularly in complex regions with intricate structures,\\nwhich sometimes cannot be reflected by the metrics.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 11}, page_content='which sometimes cannot be reflected by the metrics.\\nNormal Estimation. In Tab. 2, both Lotus-G andLotus-D outperform all other methods on zero-\\nshot surface normal estimation. Also, as illustrated in Fig. 12, Lotus consistently provides accu-\\nrate surface normal predictions, effectively handling complex geometries and diverse environments,\\nhighlighting its robustness on fine-grained prediction.\\n12'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\n5.3 A BLATION STUDY\\nAs shown in Tab. 3, we conduct ablation studies to validate our designs. Starting with “Direct\\nAdaptation”, we incrementally test the effects of different components, such as parameterization\\ntypes, the single-step diffusion process, and the detail preserver. Initially, we train the model using\\nonly the Hypersim dataset to establish a baseline. We then expand the training dataset using a\\nmixture dataset strategy by including Virtual KITTI , aiming to enhance the model’s generalization'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='ability across different domains. The findings from these ablations validate the effectiveness of our\\nproposed adaptation protocol, demonstrating that each design plays a vital role in optimizing the\\ndiffusion models for dense prediction tasks.\\n6 C ONCLUSION AND FUTURE WORKS\\nIn this paper, we introduce Lotus, a diffusion-based visual foundation model for dense prediction.\\nThrough systematic analysis and specifically tailored diffusion formulation, Lotus finds a way to\\nbetter fit the rich visual prior from pre-trained diffusion models into dense prediction. Extensive'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='experiments demonstrate that Lotus achieves SoTA performance on zero-shot depth and normal\\nestimation with minimal training data, paving the way of various practical applications.\\nFuture Work. While we have applied Lotus to two geometric dense prediction tasks, it can be\\nseamlessly adapted to other dense prediction tasks requiring per-pixel alignment with great poten-\\ntial, such as panoramic segmentation and image matting. Additionally, our performance is slightly\\nbehind DepthAnything [Yang et al. (2024a)] which utilizes large-scale training data. In the future,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='scaling up the training data, as reveal in Fig. 7 and Tab. 3 (“Mixture Dataset”), has great potential to\\nfurther enhance Lotus’s performance.\\nREFERENCES\\nGilwon Bae and Andrew J Davison. Aleatoric uncertainty in monocular surface normal estimation.\\nIEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , pp. 1472–1485, 2021.\\nGilwon Bae and Andrew J Davison. Rethinking inductive biases for surface normal estimation.\\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2024.\\nYaniv Benny and Lior Wolf. Dynamic dual-output diffusion models. In Proceedings of the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 11482–11491, 2022.\\nDaniel J Butler, Jonas Wulff, Garrett B Stanley, and Michael J Black. A naturalistic open source\\nmovie for optical flow evaluation. In Computer Vision–ECCV 2012: 12th European Conference\\non Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI 12 , pp. 611–625.\\nSpringer, 2012.\\nYohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint\\narXiv:2001.10773 , 2020.\\nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='Kwok, Ping Luo, Huchuan Lu, et al. Pixart- α: Fast training of diffusion transformer for photore-\\nalistic text-to-image synthesis. arXiv preprint arXiv:2310.00426 , 2023.\\nWeifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton, and Jia Deng. Oasis: A\\nlarge-scale dataset for single image 3d in the wild. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pp. 679–688, 2020.\\nAngela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 12}, page_content='Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the\\nIEEE conference on computer vision and pattern recognition , pp. 5828–5839, 2017.\\nWenyu Du, Shuang Cheng, Tongxu Luo, Zihan Qiu, Zeyu Huang, Ka Chun Cheung, Reynold\\nCheng, and Jie Fu. Unlocking continual learning abilities in language models. arXiv preprint\\narXiv:2406.17245 , 2024.\\n13'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nAinaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: A scalable pipeline\\nfor making multi-task mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision , pp. 10786–10796, 2021.\\nDavid Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using\\na multi-scale deep network. Advances in neural information processing systems , 27, 2014.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal\\nregression network for monocular depth estimation. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pp. 2002–2011, 2018.\\nXiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and\\nXiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from a\\nsingle image. arXiv preprint arXiv:2403.12013 , 2024.\\nAndreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='kitti dataset. The International Journal of Robotics Research , 32(11):1231–1237, 2013.\\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information\\nprocessing systems , 27, 2014.\\nJing He, Yiyi Zhou, Qi Zhang, Jun Peng, Yunhang Shen, Xiaoshuai Sun, Chao Chen, and Rongrong\\nJi. Pixelfolder: An efficient progressive pixel synthesis network for image generation. arXiv\\npreprint arXiv:2204.00833 , 2022.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='preprint arXiv:2204.00833 , 2022.\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\\nneural information processing systems , 33:6840–6851, 2020.\\nMu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang\\nYu, Chunhua Shen, and Shaojie Shen. Metric3d v2: A versatile monocular geometric\\nfoundation model for zero-shot metric depth and surface normal estimation. arXiv preprint\\narXiv:2404.15506 , 2024.\\nYihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 17853–17862, 2023.\\nBinbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting\\nfor geometrically accurate radiance fields. In SIGGRAPH 2024 Conference Papers . Association\\nfor Computing Machinery, 2024. doi: 10.1145/3641519.3657428.\\nO˘guzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and Amir Zamir. 3d common corruptions and data'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pp. 18963–18974, 2022.\\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\\nadversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition , pp. 4401–4410, 2019.\\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-\\ning and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 13}, page_content='computer vision and pattern recognition , pp. 8110–8119, 2020.\\nTero Karras, Miika Aittala, Samuli Laine, Erik H ¨ark¨onen, Janne Hellsten, Jaakko Lehtinen, and\\nTimo Aila. Alias-free generative adversarial networks. Advances in Neural Information Process-\\ning Systems , 34:852–863, 2021.\\nBingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Kon-\\nrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation.\\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.\\n9492–9502, 2024.\\n14'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nTobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco Korner. Evaluation of cnn-based\\nsingle-image depth estimation methods. In Proceedings of the European Conference on Computer\\nVision (ECCV) Workshops , pp. 0–0, 2018.\\nHsin-Ying Lee, Hung-Yu Tseng, and Ming-Hsuan Yang. Exploiting diffusion prior for generalizable\\ndense prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pp. 7861–7871, 2024.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='Recognition , pp. 7861–7871, 2024.\\nJin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale\\nlocal planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326 , 2019.\\nJiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic\\ngaussian fusion from casual videos via 4d motion scaffolds. arXiv preprint arXiv:2405.17421 ,\\n2024.\\nXiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma,\\nSong-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pp. 9970–9980, 2024.\\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741 , 2021.\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen,\\nand Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='Learning , pp. 8821–8831. PMLR, 2021.\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125 , 1(2):3, 2022.\\nRen´e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust\\nmonocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transac-\\ntions on pattern analysis and machine intelligence , 44(3):1623–1637, 2020.\\nRen´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='InProceedings of the IEEE/CVF international conference on computer vision , pp. 12179–12188,\\n2021.\\nMike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan\\nPaczan, Russ Webb, and Joshua M Susskind. Hypersim: A photorealistic synthetic dataset for\\nholistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference\\non computer vision , pp. 10912–10922, 2021.\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\\nence on computer vision and pattern recognition , pp. 10684–10695, 2022.\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-\\nical image segmentation. In Medical image computing and computer-assisted intervention–\\nMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceed-\\nings, part III 18 , pp. 234–241. Springer, 2015.\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 14}, page_content='Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\\ntion Processing Systems , 35:36479–36494, 2022.\\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv\\npreprint arXiv:2202.00512 , 2022.\\n15'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nThomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc\\nPollefeys, and Andreas Geiger. A multi-view stereo benchmark with high-resolution images and\\nmulti-camera videos. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition , pp. 3260–3269, 2017.\\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='open large-scale dataset for training next generation image-text models. Advances in Neural\\nInformation Processing Systems , 35:25278–25294, 2022.\\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and sup-\\nport inference from rgbd images. In Computer Vision–ECCV 2012: 12th European Conference\\non Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V 12 , pp. 746–760.\\nSpringer, 2012.\\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\\npreprint arXiv:2010.02502 , 2020.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='preprint arXiv:2010.02502 , 2020.\\nYunzhou Song, Jiahui Lei, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Track everything every-\\nwhere fast and robustly, 2024.\\nQianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of\\nmotion: 4d reconstruction from a single video. arXiv preprint arXiv:2407.13764 , 2024.\\nYuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou.\\nSpatialtracker: Tracking any 2d pixels in 3d space. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , 2024.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='on Computer Vision and Pattern Recognition (CVPR) , 2024.\\nGuangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen,\\nand Chunhua Shen. Diffusion models trained with large data are transferable visual models. arXiv\\npreprint arXiv:2403.06090 , 2024.\\nTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong\\nHe. Attngan: Fine-grained text to image generation with attentional generative adversarial net-\\nworks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\\n1316–1324, 2018.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='1316–1324, 2018.\\nLihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth\\nanything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , pp. 10371–10381, 2024a.\\nLihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang\\nZhao. Depth anything v2. arXiv preprint arXiv:2406.09414 , 2024b.\\nChongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='Xiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal.\\narXiv preprint arXiv:2406.16864 , 2024.\\nWei Yin, Yifan Liu, and Chunhua Shen. Virtual normal: Enforcing geometric constraints for ac-\\ncurate and robust depth prediction. IEEE Transactions on Pattern Analysis and Machine Intelli-\\ngence , 44(10):7282–7295, 2021a.\\nWei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua\\nShen. Learning to recover 3d scene shape from a single image. In Proceedings of the IEEE/CVF'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 15}, page_content='Conference on Computer Vision and Pattern Recognition , pp. 204–213, 2021b.\\nWei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua\\nShen. Metric3d: Towards zero-shot metric 3d prediction from a single image. In Proceedings of\\nthe IEEE/CVF International Conference on Computer Vision , pp. 9043–9053, 2023.\\nWeihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and Ping Tan. Neural window fully-connected\\ncrfs for monocular depth estimation. In Proceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition , pp. 3916–3925, 2022.\\n16'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 16}, page_content='Lotus : Diffusion-based Visual Foundation Model for High-quality Dense Prediction\\nEkim Yurtsever, Jacob Lambert, Alexander Carballo, and Kazuya Takeda. A survey of autonomous\\ndriving: Common practices and emerging technologies. IEEE access , 8:58443–58469, 2020.\\nYuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. In-\\nvestigating the catastrophic forgetting in multimodal large language models. arXiv preprint\\narXiv:2309.10313 , 2023.\\nChi Zhang, Wei Yin, Billzb Wang, Gang Yu, Bin Fu, and Chunhua Shen. Hierarchical normalization'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 16}, page_content='for robust monocular depth estimation. Advances in Neural Information Processing Systems , 35:\\n14128–14139, 2022.\\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dim-\\nitris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative ad-\\nversarial networks. In Proceedings of the IEEE international conference on computer vision , pp.\\n5907–5915, 2017.\\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dim-\\nitris N Metaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial net-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18124.pdf', 'page': 16}, page_content='works. IEEE transactions on pattern analysis and machine intelligence , 41(8):1947–1962, 2018.\\nHan Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive\\nlearning for text-to-image generation. In Proceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition , pp. 833–842, 2021.\\n17'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 0}, page_content='LLaV A-3D: A Simple yet Effective Pathway to Empowering\\nLMMs with 3D-awareness\\nChenming Zhu1,2Tai Wang2,†Wenwei Zhang2Jiangmiao Pang2Xihui Liu1,†\\n1The University of Hong Kong2Shanghai AI Laboratory\\nhttps://zcmax.github.io/projects/LLaVA-3D\\n†corresponding author\\nFigure 1. Overview of LLaV A-3D. Block (a) shows that LLaV A-3D could perform both 2D and 3D vision-language tasks. The left block (b)\\nshows that compared with previous 3D LMMs, our LLaV A-3D achieves state-of-the-art performance across a wide range of 3D benchmarks'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 0}, page_content='while maintaining a comparable performance on various 2D benchmarks compared with LLaV A-1.5. The middle block (c) demonstrates that\\nLLaV A-3D is built on the 2D LMM: LLaV A, and leverages 3D patches to endow it with 3D spatial awareness, enabling it to perform various\\n3D vision-and-language tasks in the physical world. The right blocks (d) and (e) highlights the significantly faster convergence and inference\\nspeeds of LLaV A-3D compared to existing 3D LMMs.\\nAbstract\\nRecent advancements in Large Multimodal Models\\n(LMMs) have greatly enhanced their proficiency in 2D visual'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 0}, page_content='(LMMs) have greatly enhanced their proficiency in 2D visual\\nunderstanding tasks, enabling them to effectively process and\\nunderstand images and videos. However, the development of\\nLMMs with 3D-awareness for 3D scene understanding has\\nbeen hindered by the lack of large-scale 3D vision-language\\ndatasets and powerful 3D encoders. In this paper, we in-troduce a simple yet effective framework called LLaVA-3D .\\nLeveraging the strong 2D understanding priors from LLaVA,\\nour LLaVA-3D efficiently adapts LLaVA for 3D scene under-\\nstanding without compromising 2D understanding capabili-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 0}, page_content='standing without compromising 2D understanding capabili-\\nties. To achieve this, we employ a simple yet effective repre-\\nsentation, 3D Patch , which connects 2D CLIP patch features\\nwith their corresponding positions in 3D space. By integrat-\\ning the 3D Patches into 2D LMMs and employing joint 2D\\nand 3D vision-language instruction tuning, we establish aarXiv:2409.18125v1  [cs.CV]  26 Sep 2024'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='unified architecture for both 2D image understanding and\\n3D scene understanding. Experimental results show that\\nLLaVA-3D converges 3.5 ×faster than existing 3D LMMs\\nwhen trained on 3D vision-language datasets. Moreover,\\nLLaVA-3D not only achieves state-of-the-art performance\\nacross various 3D tasks but also maintains comparable 2D\\nimage understanding and vision-language conversation ca-\\npabilities with LLaVA.\\n1. Introduction\\nRecent advancements in Large Multimodal Models\\n(LMMs) [ 2,8,30,43] have significantly enhanced their\\nability to understand and reason over visual and language'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='ability to understand and reason over visual and language\\ninputs, leading to remarkable performance in 2D visual tasks,\\nsuch as 2D perception, understanding, and reasoning. De-\\nspite their advanced perceptual and reasoning capabilities,\\nLMMs are primarily confined to virtual interactions through\\nimages or video, lacking the critical ability to interact with\\nthe physical world. To enable their deployment in real-world\\napplications and to facilitate the emergence of new capabili-\\nties through physical interactions, it is imperative to equip\\nLMMs with 3D spatial intelligence.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='LMMs with 3D spatial intelligence.\\nA key aspect of 3D spatial intelligence is the ability to\\nperceive and understand the 3D world. Similar to how 2D\\nLMMs align 2D visual features with language models us-\\ning large-scale 2D vision-language datasets, a common ap-\\nproach to developing 3D LMMs [ 12,19,20] involves inte-\\ngrating 3D features encoded from point clouds into Large\\nLanguage Models (LLMs) and training them on 3D point\\ncloud-language datasets. However, in contrast to the abun-\\ndance of large-scale 2D datasets, 3D datasets remain rela-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='dance of large-scale 2D datasets, 3D datasets remain rela-\\ntively scarce. Meanwhile, there are no powerful pre-trained\\n3D point cloud encoders, akin to CLIP ViT [ 42] in 2D, to\\nserve as a foundational model that can provide strong 3D\\nfeatures to LLMs.\\nSince real-world embodied agents typically rely on ego-\\ncentric, multi-view images as their raw observations, we aim\\nto build a 3D foundation model based on such inputs rather\\nthan 3D point clouds. There have been attempts [ 17,18]\\nto leverage the 2D foundation models, like CLIP, alongside\\nLLMs to advance this goal. These methods resort to 2D'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='LLMs to advance this goal. These methods resort to 2D\\nobject segmentation results [ 26] to extract and aggregate\\nCLIP features from object-centric image patches, construct-\\ning pixel-aligned 3D scene features [ 23]. However, this\\nmulti-view image object segmentation and object-centric\\nfeature extraction pipeline is inherently complex and compu-\\ntationally intensive. In contrast, 2D LMMs [ 2,8,30,37,38]\\ndirectly leverage CLIP’s image patch features with richer,\\nfine-grained information for effective image understanding\\nand visual reasoning. This naturally leads to the question:'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='and visual reasoning. This naturally leads to the question:\\nCan we directly build a 3D LMM upon the strong 2D priors\\nfrom 2D LMMs, bypassing the obstacles in 3D data scaleand 3D encoders ?\\nIn light of recent progress in 2D LMMs, we propose\\na simple yet effective framework, LLaV A-3D, which ex-\\ntends the well-established 2D LLaV A model to efficiently\\ncomprehend the 3D world while preserving its robust 2D\\nmultimodal perception and reasoning capabilities. The core\\ninnovation in our approach is the introduction of 3D Patch ,\\na new 3D representation that bridges 2D features within a'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='a new 3D representation that bridges 2D features within a\\n3D spatial context. This representation is derived by sim-\\nply augmenting 2D patch-wise features with 3D positional\\nembeddings, eliminating the need for additional complex\\nprocessing. After enhancing the 2D visual tokens to 3D\\npatch features, we explore various pooling strategies to com-\\npress the 3D patch features across extensive input frames\\nbefore being fed into LLM. Fine-tuned on the existing 3D\\nvision-language datasets, our model converges rapidly and\\nacquires 3D spatial understanding and grounding capabil-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='acquires 3D spatial understanding and grounding capabil-\\nities. Furthermore, the unified model architecture allows\\nLLaV A-3D to retain the strong 2D understanding and rea-\\nsoning abilities of LLaV A through joint instruction-tuning\\non 2D vision-language datasets.\\nOur experimental results demonstrate that LLaV A-3D\\nachieves state-of-the-art performance on a wide range of\\n3D tasks and benchmarks [ 3,6,13,39–41,51], including\\n3D captioning, 3D question answering, and 3D grounding\\nwhile requiring significantly less training time and fewer\\nepochs than existing 3D LMMs. Additionally, LLaV A-3D'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='epochs than existing 3D LMMs. Additionally, LLaV A-3D\\nachieves comparable capabilities in 2D image understanding,\\nreasoning, and conversation to LLaV A through joint tuning\\non 2D and 3D vision-language instructions.\\n2. Related Work\\n2D LMMs. Building on the success of recent LLMs, nu-\\nmerous studies [ 2,8,30,35,37,38] explored LMMs that\\ncan jointly process visual and linguistic information. For\\nexample, LLaV A [ 37,38] aligned 2D images with language\\nmodels through an image encoder and a projection layer,\\nwhile BLIP2 [ 30] employed a more sophisticated Q-Former'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='while BLIP2 [ 30] employed a more sophisticated Q-Former\\narchitecture to guide the compression of visual features using\\ntextual cues. However, most early 2D LMMs were trained\\non single-image datasets, limiting their ability to tackle the\\ncomplexities of multi-image understanding. Recently, there\\nhas been increasing interest in expanding LMMs to handle\\nmulti-image inputs, addressing the demands of real-world\\nscenarios. For video LMMs [ 28,31,34,47], multi-image\\ninput forms the basis for capturing temporal or action-related\\ndynamics across sequences of video frames. On the other'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 1}, page_content='dynamics across sequences of video frames. On the other\\nhand, multi-view images of the 3D scene can implicitly re-\\nveal 3D spatial relationships and other abstract relations in\\nthe environment. Recent works [ 36,41] explored whether\\n2D LMMs [ 1,44] can leverage multi-view images to perform\\nspatial understanding. However, these methods primarily\\nrelied on implicit learning from the data, without directly'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='modeling the 3D world. In contrast, our LLaV A-3D explic-\\nitly models the 3D world from multi-view images, enabling\\nadvanced 3D spatial understanding and grounding capabili-\\nties.\\nInjecting 3D into LLMs. As 2D LMMs achieved substan-\\ntial progress in visual perception, similar efforts have been\\nmade in the 3D domain. For 3D scene-level understanding,\\nrecent works explored ways to integrate 3D inputs such as\\npoint clouds [ 12,19,20] or multi-view images [ 17,18] into\\nLLMs to enable advanced 3D scene understanding and rea-\\nsoning. An important distinction among these methods is'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='soning. An important distinction among these methods is\\nhow they construct the 3D scene representation. LL3DA [ 12]\\ndirectly used a scene-level 3D point cloud encoder to extract\\nthe 3D scene representation. LEO [20] and Chat3D v2 [19]\\nfirst segmented 3D objects from the scene point cloud us-\\ning the off-the-shelf 3D instance segmentation model, and\\nthen independently extracted 3D object features with object-\\nlevel 3D encoders to represent the 3D scene. On the other\\nhand, starting from multi-view images, 3D-LLM [ 18] and\\nScene-LLM [ 17] resorted to manually crafted 2D object'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='Scene-LLM [ 17] resorted to manually crafted 2D object\\nsegmentation to extract and aggregate CLIP features from\\nobject-centric image patches, constructing pixel-aligned 3D\\npoint representation. Unlike these approaches, our LLaV A-\\n3D directly builds on the well-trained 2D LMM with multi-\\nview images as input. Utilizing the 3D position embeddings,\\nit brings the the 2D patches within a 3D spatial context to\\nconstruct 3D Patches. This 3D representation enables quick\\nadaption of LLaV A for 3D scene understanding while pre-\\nserving its strong 2D image understanding ability.\\n3. Method'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='3. Method\\nPrevious 2D LMMs typically consist of a visual encoder\\nto extract 2D image features, which are then aligned with\\nthe LLM via the projection layer for joint visual and lan-\\nguage reasoning tasks. In this section, we introduce how to\\nbridge the 2D image features within 3D spatial context to\\nconstruct 3D patches (Sec. 3.1, 3.2), and then demonstrate\\nthe 3D-aware pooling strategies to compress the 3D patches\\n(Sec. 3.2) and finally present the 3D-aware position encoding\\nand decoding process (Sec. 3.4), as illustrated in Fig. 2.\\n3.1. Preliminary'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='3.1. Preliminary\\nWe choose LLaV A [ 38], a 2D LMM, to explore building a\\n3D LMM based on it. LLaV A uses the pre-trained CLIP\\nencoder to extract the 2D patch features Xv∈Rc×w×h\\nfrom the input image X∈R3×W×H, and then align the 2D\\npatch features Xvinto with LLM space with the projection\\nlayer. A simple multi-view image adaptation [ 29] for LLaV A\\ncould be extracting multi-view 2D Patch features from multi-\\nview images X′\\nv∈RV×c×w×hand sequentially feeding\\nthem into LLM for multi-image understanding. To empowerLLaV A with 3D awareness, we introduce 3D Patch , a novel'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='3D representation that integrates 3D spatial information into\\n2D patch features.\\n3.2. 3D Patch\\nOur 3D Patch representations are built upon the 2D patch\\nfeatures X′\\nvextracted from multi-view images with CLIP\\nvisual encoder to leverage the strong visual-semantic align-\\nment. To construct the 3D Patches, we inject the 3D po-\\nsition information into the aforementioned 2d patches so\\nthat the 3D Patches can explicitly model 3D spatial infor-\\nmation while preserving the semantic information from 2D\\npatches. As illustrated in left block of Fig. 2, given the\\nmulti-view 2D patch features X′'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='multi-view 2D patch features X′\\nv∈RV×c×w×h, we obtain\\ntheir 3D positions P∈RV×3×w×hin the 3D world, using\\nnearest neighbor depth and known camera intrinsic and ex-\\ntrinsic parameters. The 3D positions Pare then encoded\\ninto 3D position embeddings P′∈RV×w×h×dthrough\\na learnable two-layer MLP, which are subsequently added\\nto the 2D patch visual tokens, resulting in the 3D patches\\nX′\\n3D∈RV×w×h×d:\\nX′\\n3D=X′\\nv+MLP (P′) (1)\\n3.3. 3D Patch Pooling\\nWhile the 3D Patches equip 2D patches with 3D spatial\\nawareness, the number of 3D patches scales directly with'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='awareness, the number of 3D patches scales directly with\\nthe number of input images. Capturing a full 3D scene\\noften necessitates a large set of images, which significantly\\nincreases the computational overhead for large language\\nmodels (LLMs). To address this, we introduce a 3D-aware\\npooling mechanism to reduce the number of 3D patches, as\\nillustrated in the middle block of Fig. 2.\\nIn the 2D image or video domain, pooling is commonly\\napplied along the 2D spatial or temporal dimensions to com-\\npress the number of tokens and extract essential semantic'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='press the number of tokens and extract essential semantic\\ninformation. However, for 3D scene understanding, we must\\npool the 3D patches based on their 3D locations to ensure\\nthese features can cover and preserve the entire scene’s struc-\\nture as completely as possible. We explore two parameter-\\nfree pooling strategies to achieve this:\\nVoxelization Pooling. V oxelization discretizes the 3D\\nspace into a volumetric grid. Within each occupied voxel,\\nthe 3D patches undergo average pooling, resulting in updated\\nvoxel visual tokens. Only the visual tokens from occupied'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 2}, page_content='voxel visual tokens. Only the visual tokens from occupied\\nvoxels are passed to the LLM, with the number of tokens\\nvarying across different 3D scenes. While the number of 3D\\npatches scales with the number of images, the number of\\nvoxel patches is only determined by the partition of voxel\\ngrids. We can easily balance the number of visual tokens and\\nthe preservation of fine-grained scene features by controlling\\nthe voxel size.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='Figure 2. LLaV A-3D Architecture. Based on LLaV A, we directly add the corresponding 3D position embeddings to 2D patch visual tokens\\nof multi-view images to construct the 3D Patches, then the 3D Patches will undergo 3D pooling and be sent into the projection layer of\\nLLaV A to map into the LLM space and align with the LLM using 3D-visual-language data.\\nFPS Pooling. Farthest Point Sampling (FPS) is a widely\\nused sampling strategy to select a representative subset of\\npoints from a larger set of points cloud. We apply FPS\\nto sample 3D patches from multi-view images to a fixed'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='to sample 3D patches from multi-view images to a fixed\\nnumber of tokens, ensuring that the sampled tokens represent\\nthe entire scene structure. While fixing the number of tokens\\nhelps the LLM efficiently process visual information, it may\\nalso result in loss of scene information.\\nCompared to FPS Pooling, V oxelization Pooling offers equiv-\\nalent efficiency in visual token compression while preserving\\nmore detailed scene information. Furthermore, the explicit\\nconstruction of voxel grids can better handle dynamic 3D\\nscene updates, whereas FPS Pooling excels at preserving the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='scene updates, whereas FPS Pooling excels at preserving the\\noverall 3D scene structure. We conduct quantitative experi-\\nments in Sec. 5.7 to thoroughly assess the effectiveness of\\nthese pooling strategies.\\n3.4. 3D-aware Position Encoding & Decoding\\nIn the previous sections, we detailed the construction of the\\n3D scene representation from multi-view images, establish-\\ning the foundation for further interaction with the 3D scene.\\nBuilding on this, the LLM could process multi-modal inputs\\nsuch as the 3D scene, language instructions, and 3D coordi-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='such as the 3D scene, language instructions, and 3D coordi-\\nnate cues to generate outputs such as language responses and\\n3D bounding boxes, as illustrated in the right block of Fig. 2.\\nIn this section, we introduce how the model is equipped to\\ninterpret 3D coordinate information from inputs and sub-\\nsequently output precise 3D bounding boxes when specific\\nlocation-related task requirements are needed.\\nEncoding of 3D Coordinate Input. In scenarios such\\nas 3D dense object captioning or object-centric question\\nanswering, the language instruction contains 3D coordinates.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='To handle such tasks, we introduce the 3D Coordinate Token\\nto allow the model to integrate the provided coordinates\\nas context into its reasoning processes. Specifically, we\\nobtain the 3D coordinate token by feeding the 3D coordinatesthrough a two-layer MLP. These 3D coordinate tokens are\\nfed into LLM together with 3D Patch tokens and text tokens,\\nenabling 3D-aware perception and reasoning.\\nDecoding of 3D Bounding Box Output. The integration\\nof the 3D coordinate token empowers the model to process\\n3D coordinate information from input instructions effec-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='3D coordinate information from input instructions effec-\\ntively. However, experiments show that directly outputting\\n3D bounding boxes is quite challenging for LLM, and em-\\npirically, the performance is poor. To overcome this, we\\nleverage an approach similar to previous method [ 51], intro-\\nducing a specialized location token that guides the grounding\\nmodule to generate accurate 3D bounding boxes. Specifi-\\ncally, the LLM predicts a special location token to represent\\na 3D box prediction when the task necessitates 3D bounding\\nbox outputs. We then derive the last layer embedding of this'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='location token and send it into the grounding module. The\\ngrounding module utilizes the location token embedding and\\nthe 3D scene feature to predict the 3D box coordinates of the\\ntarget object. This process facilitates the precise generation\\nof 3D bounding box outputs. More details can be found in\\nthe appendix.\\n4. Training\\nTo leverage the 2D priors from established 2D LMMs, we\\ntrain our LLaV A-3D model based on the pre-trained LLaV A-\\n1.5. Considering the scarcity of 3D scene-language data, our\\ntraining procedure comprises two stages, each focusing on\\ndifferent training targets of the model.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 3}, page_content='different training targets of the model.\\nStage 1: 3D Patch Language Alignment. During the\\nfirst training stage, we use the region-level and scene-level\\ncaption data that describe spatial relationships among 3D ob-\\njects to align the 3D patches with the LLM for enhanced 3D\\nspatial comprehension. At this stage, the input multi-view\\nimages used are selected from sequences that correspond'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='Figure 3. LLaV A-3D-Instruct-1M. The hybrid 2D and 3D Dataset\\nCollection. Left: Distribution of data across categories, with the\\nouter circle representing all categories and the inner circle illustrat-\\ning data subset distribution. Right: Detailed dataset quantities.\\nto specific regions or entire scenes. We freeze the vision\\nencoder and LLM parameters, and only train the projection\\nlayer and 3D position embedding layer, encouraging effi-\\ncient alignment between 3D patch features and text space.\\nSince 3D patches are derived from CLIP features augmented'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='Since 3D patches are derived from CLIP features augmented\\nwith 3D positional information, the alignment between 3D\\nPatch and LLM converges rapidly.\\nStage 2: Task Instruction Tuning. During the instruction-\\ntuning stage, LLaV A-3D is optimized to respond to complex\\n3D V&L tasks and maintain its inherent 2D image reason-\\ning and instruction-following capabilities. To facilitate this\\ncapability, we collect the LLaV A-3D-Instruct-1M dataset,\\na hybrid collection of 2D and 3D data specifically tailored\\nfor instruction tuning. The overall distribution of the dataset'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='collection is shown in Fig 3, more details about the instruc-\\ntional tuning datasets are listed in the appendix. The 2D Data\\nof LLaV A-3D-Instruct-1M is derived from existing LLaV A-\\n1.5 instruction tuning data, ensuring the preservation of 2D\\nimage comprehension and vision-language conversation abil-\\nities. When tuning with 2D data, we keep the vision encoder\\nfrozen and jointly train the projection layer and LLM. The\\n3D Data of LLaV A-3D-Instruct-1M, on the other hand, com-\\nprises data from diverse 3D QA, 3D dense captioning, and\\n3D grounding tasks. During the 3D data instruction tuning,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='3D grounding tasks. During the 3D data instruction tuning,\\nthe 3D position embedding layer will be added to jointly train\\nwith the other modules. Additionally, for tasks where the\\ninstruction contains 3D coordinate information or requires\\n3D bounding box outputs, the corresponding encoding and\\ndecoding modules will be trained together. During instruc-\\ntion tuning, the 3D data pathway includes the 3D position\\nembeddings and 3D patches, while the 2D data pathway is\\nthe original LLaV A. All modules except for the 3D position\\nembeddings to construct 3D patches are shared across 2D'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='embeddings to construct 3D patches are shared across 2D\\nand 3D data. This training setup ensures that LLaV A-3D\\nis capable of processing both 2D and 3D visual tokens ef-\\nfectively, and is adaptive to various task formulations andTable 1. Quantitative comparison with SOTA models on various\\n3D QA tasks . “C” stands for “CIDEr”, “B-4” for “BLEU-4”, “M”\\nfor “METEOR”, “R” for “ROUGE”, “Sim” for sentence similarity,\\nand “EM@1” for top-1 exact match. Gray indicates evaluation\\nresults with refined exact-match protocol.\\nScanQA (val) SQA3D (test)\\nC B-4 M R EM@1 EM@1\\nTask-specific models'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='C B-4 M R EM@1 EM@1\\nTask-specific models\\nScan2Cap [13] - - - - - 41.0†\\nScanRefer+MCAN [49] 55.4 7.9 11.5 30.0 18.6 -\\nClipBERT [27] - - - - - 43.3\\nScanQA [3] 64.9 10.1 13.1 33.3 21.1 47.2\\n3D-VisTA [52] 69.6 10.4 13.9 35.7 22.4 48.5\\nTask-specific fine-tuned 3D LMMs\\n3D-LLM (FlanT5) [18] 69.4 12.0 14.5 35.7 20.5\\nLL3DA [37] 76.8 13.5 15.9 37.3 -\\nChat-3D v2 [19] 87.6 14.0 - - - 54.7\\nLEO [20] 101.4 13.2 20.0 49.2 24.5 (47.6) 50.0 (52.4)\\nScene-LLM [17] 80 12.0 16.6 40.0 27.2 54.2\\nZero-shot 2D LMMs\\nVideoChat2 [32] 49.2 9.6 9.5 28.2 19.2 37.3\\nLLaV A-NeXT-Video [28] 46.2 9.8 9.1 27.8 18.7 34.2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='LLaV A-NeXT-Video [28] 46.2 9.8 9.1 27.8 18.7 34.2\\nGPT-4V 59.6 - 13.5 33.4 - -\\nGemini 68.3 - 11.3 35.4 - -\\nClaude 57.7 - 10.0 29.3 - -\\nLLaV A-3D 91.7 14.5 20.7 50.1 27.0 (45.0) 55.6 (57.6)\\nrequirements.\\n5. Experiments\\nIn this section, we conduct extensive evaluations to examine\\nthe capabilities of LLaV A-3D. To begin with, we introduce\\nthe implementation details (Sec. 5.1). Then, we compare\\nour model’s 3D scene understanding (Sec. 5.2, 5.3, 5.4)and\\n2D image understanding (Sec. 5.5) capability with previous\\nmethods. Finally, we conduct a thorough analysis to vali-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='methods. Finally, we conduct a thorough analysis to vali-\\ndate the effectiveness of the components and designs of our\\nLLaV A-3D (Sec. 5.6, 5.7).\\n5.1. Implementation Details\\nLLaV A-3D is built upon the LLaV A-1.5-7B, utilizing their\\npre-trained weights from the HuggingFace library. For 3D\\ntasks, we add the 3D position embeddings to the 2D patch\\nvisual tokens, and utilize the voxelization pooling strategy\\nto reduce 3D patch number before passing the input visual\\ntokens to the projection layer and LLM. The number of\\nviewsVis set to be 20 and voxel size is set to 0.2m. Due'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 4}, page_content='viewsVis set to be 20 and voxel size is set to 0.2m. Due\\nto the LLM context length limitation, the maximum number\\nof 3D patch tokens after 3D-aware pooling is set to 3096.\\nFor 2D tasks, LLaV A-3D functions the same as LLaV A. All\\nexperiments are conducted on 8 ×80G A100 GPUs. We\\ntrain our model for 1 epoch with a learning rate of 1e-3 and\\na batch size of 32 in stage 1, and fine-tune on the collected\\nLLaV A-3D-Instruct-1M dataset, with a learning rate of 2e-5\\nand a batch size of 16 in stage 2.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='Table 2. Quantitative comparison on MMScan QA benchmark . “S.-BERT\", “B-1”, “B-4”, “R.-L.”, “MET.” represents “Sentence-BERT\",\\n“BLEU-1”, “BLEU-4”, “ROUGE-L”, “METEOR”, respectively. Here, we report the top-1 exact match with (the refined exact-match protocol\\nresults) for “EM@1”.\\nMethods Setting OverallSingle-target Inter-targetAdvancedData-driven Metrics Traditional Metrics\\nST-attr ST-space OO-attr OO-space OR SimCSE S.-BERT B-1. B-4. R.-L MET. EM@1\\n3D-LLM [18]\\nZero-Shot28.6 37.8 18.8 13.7 26.3 15.4 20.8 40.4 40.3 13.4 1.5 17.3 6.0 6.2 (19.6)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='Chat3D-v2 [19] 27.9 38.1 18.3 9.3 22.4 13.5 25.4 45.4 46.3 18.0 3.0 22.9 7.5 10.2 (19.6)\\nLL3DA [12] 15.8 15.5 14.7 14.2 25.2 4.3 6.4 40.7 43.6 5.4 2.1 16.4 4.4 8.3 (19.4)\\nLEO [20] 22.2 28.9 17.6 18.1 20.4 15.0 16.3 40.4 41.0 11.0 0.7 17.1 4.9 9.6 (18.7)\\nLL3DA [12]Fine-tuning38.5 40.4 46.2 14.7 47.1 26.4 7.1 65.3 67.0 26.4 8.5 44.3 14.7 30.2 (37.6)\\nLEO [20] 47.8 55.5 49.5 36.1 45.6 32.1 38.4 71.2 72.2 32.0 12.5 52.1 17.7 36.6 (44.5)\\nLLaV A-3D Generalist 52.3 61.2 54.4 28.7 61.2 44.5 43.6 74.6 76.3 38.7 13.1 55.5 19.5 45.2 (51.4)\\nTable 3. Quantitative comparison with SOTA models on'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='Table 3. Quantitative comparison with SOTA models on\\nOpenEQA benchmark\\nModels Frame Accuracy\\nLLaMA2 [45] 0 28.3\\nGPT-4 [1] 0 33.5\\nClaude3 20 36.3\\nGemini-Pro [44] 15 44.9\\nGPT-4V [1] 15 54.6\\nGPT-4V [1] 50 55.3\\nHuman Full 86.8\\nLLaV A-3D 20 51.2\\n5.2. Evaluation on 3D Question Answering\\n3D Question Answering requires a model to generate re-\\nsponses to the natural language queries questioning to-\\nwards a 3D scene. In this section, we validate LLaV A-\\n3D performance on various 3D question answering bench-\\nmarks: ScanQA [ 3], SQA3D [ 40], MMScan QA [ 39], and\\nOpenEQA [41].'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='OpenEQA [41].\\nSpatial Understanding with ScanQA and SQA3D.\\nScanQA and SQA3D are both built on ScanNet dataset.\\nThe ScanQA dataset consists of 41363 questions about 800\\nscenes, including 32337 unique questions. Its validation set\\ncontains 4675 questions about 71 scenes. SQA3D comprises\\n20.4k descriptions of 6.8k unique situations collected from\\n650 ScanNet scenes and 33.4k questions about these situa-\\ntions. Questions in ScanQA require basic recognition and\\n3D reasoning capabilities, and SQA3D further incorporates\\nthe situation understanding and situated reasoning into em-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='the situation understanding and situated reasoning into em-\\nbodied 3D scene understanding. Following prior works, we\\nadopt BLEU scores, METEOR, ROUHE-L, CIDEr and EM\\n(“exact match”) as our evaluation metrics for ScanQA and\\nEM for SQA3D respectively. As shown in Tab. 1, current\\n2D LMMs fail to achieve competitive performance with the\\nlatest 3D LMMs trained with 3D awareness, which might be\\nattributed to the lack of explicit 3D representation. Besides,\\ncompared with task-specific fine-tuned 3D LMMs that need'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='compared with task-specific fine-tuned 3D LMMs that need\\nto be further fine-tuned on the corresponding datasets, ourLLaV A-3D could perform as a generalist and achieve the\\nSOTA performance on these benchmarks.\\nCoordinate Spatial Understanding with MMScan QA.\\nMMScan QA includes 5.2k scans from ScanNet, 3RScan,\\nand Matterport3D, along with 116k training questions and\\n29k validation questions. These questions span existential in-\\nquiries, attribute understanding, and more advanced queries.\\nUnlike ScanQA and SQA3D, some MMScan QA questions'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='Unlike ScanQA and SQA3D, some MMScan QA questions\\nrequire 3D reasoning based on object coordinates, rather than\\nrelying solely on text descriptions, demanding the model ca-\\npable of understanding 3D coordinates information. All\\ndata samples are classified into one of the following sub-\\ncategories: ST-attr, ST-space, OO-attr, OO-space, OR, where\\nST stands for Single-target, attr for attribute, OO for Object-\\nObject, and OR for Object Region. Besides, there is a minor\\npart of QA samples for advanced understanding and reason-\\ning, such as situated QA related to everyday life. We present'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='the results under GPT-4 evaluation, data-driven metrics, and\\ntraditional metrics respectively in Tab. 2. In this benchmark,\\nthe well-trained LL3DA and LEO are further fine-tuned on\\nthe full 1.2M MMScan QA training dataset. Our LLaV A-3D,\\ntrained on LLaV A-3D-Instruct-1M (which includes 440K\\nMMScan QA training samples), achieves significantly better\\nperformance on the MMScan QA benchmark compared to\\nthe specially fine-tuned LL3DA and LEO. The results high-\\nlight the training efficiency of LLaV A-3D and its strong 3D\\nunderstanding ability to serve as the generalist model.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='understanding ability to serve as the generalist model.\\nEmbodied Question Answering with OpenEQA.\\nOpenEQA is the first open-vocabulary benchmark designed\\nfor spatial understanding and embodied reasoning in\\nembodied question answering, specifically in the era of\\nfoundation models. It features an automated evaluation\\nprotocol powered by LLMs, which shows strong alignment\\nwith human judgment. Our evaluations are conducted\\nusing the EM-EQA data split of OpenEQA, which includes\\nover 1,600 high-quality, human-generated questions from\\ndiverse real-world environments. In EM-EQA, the model'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 5}, page_content='diverse real-world environments. In EM-EQA, the model\\nis required to generate a textual answer to a question\\nbased on an episode history, which typically consists'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='Table 4. Quantitative Comparisons with SOTA models for 3D\\nDense Captioning on Scan2Cap. “C” stands for “CIDEr”, “B-4”\\nfor “BLEU4”, “M” for “METEOR”, “R” for “ROUGE”, “Sim”\\nfor sentence similarity, and “EM@1” for top-1 exact match. The\\nn-gram metrics for Scan2Cap are governed by IoU@0.5.\\nScan2Cap (Val)\\nC@0.5↑B-4@0.5 ↑M@0.5 ↑R@0.5↑\\nScan2Cap [13] 39.08 23.32 21.97 44.78\\nMORE [24] 40.94 22.93 21.66 44.42\\nSpaCap3D [46] 44.02 25.26 22.33 45.36\\nD3Net [7] 46.07 30.29 24.35 51.67\\nUniT3D [14] 46.69 27.22 21.91 45.98\\n3DJCG [5] 49.48 31.03 24.22 50.80\\n3D-VLP [25] 54.94 32.31 24.83 51.51'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='3D-VLP [25] 54.94 32.31 24.83 51.51\\n3D-VisTA [52] 61.60 34.10 26.80 55.00\\nV ote2Cap-DETR [11] 61.81 34.46 26.22 54.40\\nLL3DA [12] 65.19 36.79 25.97 55.06\\nLEO [20] 72.4 38.2 27.9 58.1\\nLLaV A-3D 79.21 41.12 30.21 63.41\\nTable 5. Quantitative comparison with SOTA models on the\\nMMScan Captioning benchmark.\\nmodel Evaluator Type Color Shape Position Function Design Overall\\nLL3DA [12] GPT 10.0 26.3 40.6 38.9 67.5 21.7 33.6\\nLEO [20] GPT 34.9 29.7 63.0 63.7 75.0 42.7 51.3\\nLLaV A-3D GPT 39.9 79.2 89.1 82.2 94.1 88.0 78.8\\nof RGB images, depth information, camera poses, and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='of RGB images, depth information, camera poses, and\\nintrinsic camera data. The results in Tab. 3 demonstrate\\nthat LLaV A-3D surpasses Claude3 and Gemini-Pro, and\\nachieves comparable performance with powerful GPT-4V on\\nthis benchmark with significantly fewer model parameters.\\n5.3. Evaluation on 3D Dense Captioning\\n3D dense captioning requires the model to localize all the\\nobjects in a 3D scene and then generate descriptive sentences\\nfor each object. To evaluate our model on the 3D dense cap-\\ntioning tasks, we utilize the off-the-shelf segmentation model'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='Mask3D to generate object proposals. Then we further con-\\nstruct the 3D coordinate tokens based on the 3D object center\\ncoordinates to guide the model to perform the task. Addition-\\nally, we utilize two types of textual instructions that prompt\\nthe model to either describe the object or describe and output\\nthe bounding box of the object. We report the performance\\nof various methods on two 3D dense captioning benchmarks:\\nScan2Cap and MMScan Captioning.\\nScan2Cap. Scan2Cap requires the model to describe the\\nobject’s appearance and the spatial relations with nearby'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='object’s appearance and the spatial relations with nearby\\nobjects and output the corresponding 3D bounding box. As\\nillustrated in Tab. 4, our method consistently outperforms\\nthe existing method on the Scan2Cap benchmark.\\nMMScan Captioning. MMScan Captioning focuses on\\nidentifying common aspects of 3D objects such as Object\\nType, Color, Shape, Position, Function, and Design. TheTable 6. Quantitative comparison with SOTA models on various\\n3D VG tasks .\\nScanRefer\\nAcc@0.25 Acc@0.5\\nTask-specific models\\nScanRefer [6] 37.3 24.3\\nMVT [21] 40.8 33.3\\n3DVG-Trans [50] 45.9 34.5\\nViL3DRel [10] 47.9 37.7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='3DVG-Trans [50] 45.9 34.5\\nViL3DRel [10] 47.9 37.7\\nBUTD-DETR [22] 52.2 39.8\\nReGround3D [51] 53.1 41.1\\nTask-specific fine-tuned 3D LLMs\\nLLM-Grounder [48] 17.1 5.3\\n3D-LLM [18] 30.3 -\\nChat3D-v2 [19] 35.9 30.4\\nLLaV A-3D 54.1 42.2\\nbenchmark utilizes GPT4 to assess whether these aspects\\nare correct in the object description. We benchmark var-\\nious methods on the MMScan Captioning benchmark in\\nTab. 5. The results show that our method surpasses existing\\napproaches across all metrics by a substantial margin, espe-\\ncially achieving a 49.5% improvement in the Color score and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='cially achieving a 49.5% improvement in the Color score and\\na 43.3% improvement in the Design score. This significant\\nperformance boost can be attributed to our 2D LMM-based\\narchitecture.\\nUniquely, LLaV A-3D takes multi-view images as inputs,\\nenabling a user-friendly feature where users can simply click\\non selected images to generate both 3D object captions and\\n3D bounding boxes. We refer to this capability as 2D Click-\\nbased 3D Dense Captioning . (as illustrated in Fig. 4).\\n5.4. Evaluation on 3D Visual Grounding\\n3D visual grounding aims to localize the target object in the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='3D scene using natural language descriptions. In this section,\\nwe initially report the performance on the ScanRefer [ 6]\\nbenchmark. For quantitative comparisons, we include both\\ntask-specific approaches and generalist models: the state-\\nof-the-art specialists in 3D VG and generalist models like\\nLLM-Grounder [ 48], 3D-LLM [ 18], and Chat3D-v2 [ 19].\\n3D-LLM [ 18]] uses the location tokens to discretize the 3D\\nscene and predicts the bounding box as the location tokens\\nthat were added to the vocabulary. Chat-3D v2 [ 19] first\\ndetects all the objects and then identifies the object that'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 6}, page_content='detects all the objects and then identifies the object that\\nbest matches the provided description. The results in Tab. 6\\ndemonstrate that by combining with the grounding module,\\nour LLaV A-3D could achieve even better performance com-\\npared with the task-specific specialists model as a generalist.\\nSuch a paradigm allows LLaV A-3D to integrate with various\\npowerful 3D grounding modules and inject the 3D under-\\nstanding capability and world knowledge into the grounding\\nprocess, as illustrated in Fig. 7.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='Figure 4. LLaV A-3D enables the user-friendly interaction with the 3D scene across various 3D understanding and reasoning tasks. It allows\\nthe users to just click on the 2D images or the video frame to simply conduct the interactive 3D question answering and 3D dense captioning.\\nTable 7. Quantitative Comparisons with SOTA models on zero-\\nshot 2D benchmarks .\\nMethod LLM Res. VQATMMB MME MM-Vet\\nMobileVLM [15] MLLaMA 2.7B 336 47.5 59.6 1289 -\\nInstructBLIP [16] Vicuna-7B 224 50.1 36.0 – 26.2\\nInstructBLIP [16] Vicuna-13B 224 50.7 – 1213 25.6\\nQwen-VL [4] Qwen-7B 448 63.8∗38.2 – –'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='Qwen-VL [4] Qwen-7B 448 63.8∗38.2 – –\\nQwen-VL-Chat [4] Qwen-7B 448 61.5∗60.6 1488 –\\nShikra [9] Vicuna-13B 224 – 58.8 – –\\nLLaMA-VID [33] Vicuna-7B 336 – 65.1 1521 –\\nLLaMA-VID [33] Vicuna-13B 336 – 66.6 1542 –\\nLLaV A-1.5 [38] Vicuna-7B 336 58.2 65.2 1511 31.1\\nLLaV A-1.5 [38] Vicuna-13B 336 61.3 69.2 1531/295 36.1\\nLLaV A-3D Vicuna-7B 336 57.8 65.0 1502 30.9\\nTable 8. Architecture Comparison on various 3D V&L Bench-\\nmark.\\n3D Feature Connector LLM / LMM ScanQA SQA3D Inference time\\n(a) (SAM + CLIP) w / PE Q-Former Vicuna-7B 21.9 49.3 900s'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='(a) (SAM + CLIP) w / PE Q-Former Vicuna-7B 21.9 49.3 900s\\n(b) (SAM + CLIP) w / PE Pooling + MLP Vicuna-7B 22.1 49.2 900s\\n(c) CLIP w / PE Pooling + MLP Vicuna-7B 23.4 51.2 0.2s\\n(d) CLIP w / PE Pooling + MLP LLaV A-1.5 27.0 55.6 0.2s\\n(e) CLIP w / PE Pooling + MLP PLLaV A 27.9 56.2 0.2s\\n5.5. Evaluation on 2D benchmarks\\nSince our model is trained on a mix of 2D and 3D datasets,\\nwe evaluate it across various 2D benchmarks to ensure it\\nretains the 2D image understanding capabilities of the origi-\\nnal LLaV A. As demonstrated in Tab. 7, LLaV A-3D achieves'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='nal LLaV A. As demonstrated in Tab. 7, LLaV A-3D achieves\\nperformance comparable to that of LLaV A-1.5 across sev-\\neral 2D image understanding benchmarks. This performance\\nunderscores the architectural superiority of our model com-\\npared to other 3D LMMs.\\n5.6. Architecture Analysis\\nIn this section, we delve deeper into the architectural bene-\\nfits and efficacy of adapting a 2D large multimodal modelTable 9. Effectiveness of 3D Patch Representation . Training\\nusing the instruction tuning datasets, the only difference between\\nmulti-image LLaV A and LLaV A-3D is the patch type.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='multi-image LLaV A and LLaV A-3D is the patch type.\\nMethod Patch Type ScanQA SQA3D MMScan QA Scan2Cap\\nmulti-image LLaV A 2D 24.1 52.3 32.7 29.1\\nLLaV A-3D 3D 27.0 (+2.9) 55.6 (+3.3) 41.5 (+8.8) 79.2 (+50.1)\\n(LMM) to a 3D LMM, as opposed to developing a 3D LMM\\nsolely from LLMs. A notable approach in the latter category\\nis 3D-LLM, which leverages foundational 2D visual models\\nsuch as SAM and CLIP for feature extraction. This method\\nthen employs 3D multi-view fusion to generate 3D point\\nfeatures, followed by the use of a Q-Former to condense\\nthese point features into a fixed number of tokens.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='these point features into a fixed number of tokens.\\nArchitecture Comparison. To ensure the fairness of the\\nexperiment as much as possible, starting from LLM, we\\nfirst ablate different 3D feature encoders, and 3D-language\\nconnectors using the same instruction tuning datasets. As\\nshown in Tab. 8, the Q-Former (a) and Pooling + MLP\\n(b) share a similar performance on 3D V&L benchmarks.\\nNotably, using CLIP (c) alone instead of SAM + CLIP (b)\\nachieves better performance and significantly reduces 3D\\nfeature extraction computation time from 900s to 0.2s.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 7}, page_content='feature extraction computation time from 900s to 0.2s.\\nEffectiveness of 3D Patch. Once we don’t decorate the 2D\\npatches with corresponding 3D position embedding, LLaV A-\\n3D will degenerate into 2D multi-image LMMs. To further\\nascertain the efficacy of our proposed 3D representation: 3D\\nPatch, we conduct additional experiments across a variety\\nof 3D question answering and 3D dense captioning bench-\\nmarks. As shown in Tab. 9, integrating 2D patches within\\na 3D context enhances the model’s 3D spatial understand-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='Table 10. Comparsion on different pooling strategies .\\nPooling Stratgey V oxel Size Token Number ScanQA SQA3D\\nV oxelization 0.4 Dynamic 24.1 53.2\\nV oxelization 0.3 Dynamic 25.9 54.8\\nV oxelization 0.2 Dynamic 27.0 55.6\\nFPS - 576 25.7 54.9\\nFPS - 1024 26.3 55.2\\ning and reasoning capabilities, resulting in 2.9% and 3.3%\\nimprovements on the ScanQA and SQA3D benchmarks, re-\\nspectively. Additionally, 3D patches are crucial for tasks that\\nrequire explicit 3D world modeling, leading to significant\\nimprovements of 8.8% on the MMScan QA benchmark and\\nan impressive 50.1% on the Scan2Cap benchmark.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='an impressive 50.1% on the Scan2Cap benchmark.\\nBenefits from pre-trained 2D LMM. Leveraging our\\nfoundation in 2D LMMs, our framework benefits signifi-\\ncantly from the robust pre-training on extensive image/video-\\ntext datasets. In Tab. 8, we explore the advantages of ini-\\ntializing from a pre-trained 2D LMM compared to start-\\ning directly from an LLM. The experimental results con-\\nsistently demonstrate that starting with a well-tuned 2D\\nLMM substantially could enhance performance in 3D spa-\\ntial understanding tasks. Besides, we observe that stronger'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='tial understanding tasks. Besides, we observe that stronger\\nbase 2D LMM could lead to better 3D spatial understand-\\ning performance. Besides, we find that initializing from\\na pre-trained 2D LMM could achieve 3.5 ×faster training\\nconvergence speed of LLaV A-3D compared with existing\\n3D LMMs [ 12,20]. More details could be found in our\\nappendix.\\n5.7. More Analysis\\nTo better understand the impact of different components of\\nour LLaV A-3D, we conduct a thorough ablation study on\\nthe ScanQA and SQA3D benchmarks.\\nImpact of Pooling Strategy. Given the maximum token'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='Impact of Pooling Strategy. Given the maximum token\\nlength limitation of LLMs, we apply pooling to the 3D patch\\ntokens extracted from multi-view images to reduce the num-\\nber of tokens. However, this pooling process inevitably leads\\nto some information loss. To understand its impact on per-\\nformance, we conducted experiments to evaluate the effects\\nof pooling. As shown in Tab. 10, the voxelization pooling\\nstrategy outperforms the FPS pooling method on 3D QA\\nbenchmarks. Reducing the voxel size in the voxelization\\npooling or increasing the number of 3D patch tokens ob-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='pooling or increasing the number of 3D patch tokens ob-\\ntained through FPS pooling can both enhance the model’s\\nperformance to some extent.\\nMulti-View Images Sampling Strategy. Due to the re-\\ndundancy of information among multi-view images and con-\\nsidering computational costs, we sample Vviews from theTable 11. Comparison on performance on 3D QA tasks under\\ndifferent number of multi-view images .\\nNumber of Views Number of Tokens ScanQA SQA3D\\n16 9216 26.2 55.1\\n20 11520 27.0 55.6\\n24 13824 27.0 55.4\\n40 23040 26.7 55.2\\negocentric images of the 3D scene in our experiment. In this'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='section, we explore the effect of different image sampling\\nstrategies during the inference stage: 1) Uniform Sampling :\\nTo achieve comprehensive coverage of the entire 3D scene,\\na straightforward approach is uniform sampling, which sam-\\nples images evenly. 2) Text-Guided Sampling : such sampling\\nstrategy uses CLIP to select the frames related to the text\\ninstruction during inference based on the image-text CLIP\\nsimilarity score. Our experiments demonstrate that these two\\nsampling strategies share similar performance on ScanQA\\nand SQA3D, so we choose uniform sampling for simplicity.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='and SQA3D, so we choose uniform sampling for simplicity.\\nNumber of Views. An intuitive assumption is that sam-\\npling more views from the 3D scene will preserve more\\ninformation about the 3D scene. We conduct a comparative\\nexperiment varying the number of views sampled from 3D\\nscenes. Tab. 11 presents the Exact Match (EM) scores on\\nScanQA and SQA3D across different settings, revealing that\\nthe increase in EM score is marginal as the number of views\\nincreases. Additionally, the experimental results indicate\\nthat exceeding a certain number of views can degrade the\\nmodel’s performance.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='model’s performance.\\n6. Conclusion\\nWe propose LLaV A-3D, a simple yet effective framework\\nbuilt upon the well-established 2D LLaV A model. LLaV A-\\n3D extends the LLaV A’s capabilities to understand the 3D\\nworld by using 3D patches to bridge 2D features within a\\n3D space, enabling spatial understanding while efficiently\\npreserving the original 2D image understanding and reason-\\ning capability. Experimental results show that our method\\nsets new state-of-the-art performance on a wide range of 3D\\ntasks and benchmarks. We hope that our model will inspire'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 8}, page_content='tasks and benchmarks. We hope that our model will inspire\\nnew ideas for building 3D LMMs, and in the future, we plan\\nto explore the application of LLaV A-3D in more downstream\\nscenarios, such as robot manipulation and navigation.\\nReferences\\n[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,\\nIlge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko\\nAltenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\\ntechnical report. arXiv preprint arXiv:2303.08774 , 2023. 2, 6\\n[2]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='Katherine Millican, Malcolm Reynolds, et al. Flamingo: a\\nvisual language model for few-shot learning. Advances in\\nNeural Information Processing Systems , 35:23716–23736,\\n2022. 2\\n[3]Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki\\nKawanabe. Scanqa: 3d question answering for spatial scene\\nunderstanding. In proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition , pages 19129–\\n19139, 2022. 2, 5, 6\\n[4]Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. Qwen-vl: A frontier large vision-language model with\\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 8\\n[5]Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong\\nXu. 3djcg: A unified framework for joint dense captioning\\nand visual grounding on 3d point clouds. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 16464–16473, 2022. 7\\n[6]Dave Zhenyu Chen, Angel X Chang, and Matthias Nießner.\\nScanrefer: 3d object localization in rgb-d scans using natural'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='language. In European conference on computer vision , pages\\n202–221. Springer, 2020. 2, 7\\n[7]Dave Zhenyu Chen, Qirui Wu, Matthias Nießner, and An-\\ngel X Chang. D 3 net: A unified speaker-listener architecture\\nfor 3d dense captioning and visual grounding. In European\\nConference on Computer Vision , pages 487–505. Springer,\\n2022. 7\\n[8]Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\\nLiu, Pengchuan Zhang, Raghuraman Krishnamoorthi,\\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\\nMinigpt-v2: large language model as a unified interface'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='Minigpt-v2: large language model as a unified interface\\nfor vision-language multi-task learning. arXiv preprint\\narXiv:2310.09478 , 2023. 2\\n[9]Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng\\nZhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s\\nreferential dialogue magic. arXiv preprint arXiv:2306.15195 ,\\n2023. 8\\n[10] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi,\\nCordelia Schmid, and Ivan Laptev. Language conditioned\\nspatial relation reasoning for 3d object grounding. Advances\\nin Neural Information Processing Systems , 35:20522–20535,\\n2022. 7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='2022. 7\\n[11] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu,\\nand Tao Chen. End-to-end 3d dense captioning with vote2cap-\\ndetr. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition , pages 11124–11133,\\n2023. 7\\n[12] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu,\\nHao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da:\\nVisual interactive instruction tuning for omni-3d understand-\\ning reasoning and planning. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 26428–26438, 2024. 2, 3, 6, 7, 9'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='pages 26428–26438, 2024. 2, 3, 6, 7, 9\\n[13] Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel X\\nChang. Scan2cap: Context-aware dense captioning in rgb-\\nd scans. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition , pages 3193–3203,\\n2021. 2, 5, 7[14] Zhenyu Chen, Ronghang Hu, Xinlei Chen, Matthias Nießner,\\nand Angel X Chang. Unit3d: A unified transformer for 3d\\ndense captioning and visual grounding. In Proceedings of\\nthe IEEE/CVF international conference on computer vision ,\\npages 18109–18119, 2023. 7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='pages 18109–18119, 2023. 7\\n[15] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu,\\nYang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang,\\nXiaolin Wei, et al. Mobilevlm: A fast, reproducible and strong\\nvision language assistant for mobile devices. arXiv preprint\\narXiv:2312.16886 , 2023. 8\\n[16] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung,\\nand Steven Hoi. Instructblip: Towards general-purpose vision-\\nlanguage models with instruction tuning, 2023. 8\\n[17] Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wen-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='[17] Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wen-\\nhan Xiong. Scene-llm: Extending language model for\\n3d visual understanding and reasoning. arXiv preprint\\narXiv:2403.11401 , 2024. 2, 3, 5\\n[18] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng,\\nYilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting\\nthe 3d world into large language models. arXiv preprint\\narXiv:2307.12981 , 2023. 2, 3, 5, 6, 7\\n[19] Haifeng Huang, Zehan Wang, Rongjie Huang, Luping Liu,\\nXize Cheng, Yang Zhao, Tao Jin, and Zhou Zhao. Chat-3d\\nv2: Bridging 3d scene and large language models with object'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='v2: Bridging 3d scene and large language models with object\\nidentifiers. arXiv preprint arXiv:2312.08168 , 2023. 2, 3, 5, 6,\\n7\\n[20] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun\\nLinghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baox-\\niong Jia, and Siyuan Huang. An embodied generalist agent in\\n3d world. arXiv preprint arXiv:2311.12871 , 2023. 2, 3, 5, 6,\\n7, 9\\n[21] Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multi-\\nview transformer for 3d visual grounding. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 15524–15533, 2022. 7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='Recognition , pages 15524–15533, 2022. 7\\n[22] Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Kate-\\nrina Fragkiadaki. Bottom up top down detection transformers\\nfor language grounding in images and point clouds. In Com-\\nputer Vision–ECCV 2022: 17th European Conference, Tel\\nAviv, Israel, October 23–27, 2022, Proceedings, Part XXXVI ,\\npages 417–433. Springer, 2022. 7\\n[23] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao\\nGu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer,\\nSoroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al. Con-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al. Con-\\nceptfusion: Open-set multimodal 3d mapping. arXiv preprint\\narXiv:2302.07241 , 2023. 2\\n[24] Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin\\nMa, and Yu-Gang Jiang. More: Multi-order relation mining\\nfor dense captioning in 3d scenes. In European Conference\\non Computer Vision , pages 528–545. Springer, 2022. 7\\n[25] Zhao Jin, Munawar Hayat, Yuwei Yang, Yulan Guo, and\\nYinjie Lei. Context-aware alignment and mutual masking for\\n3d-language pre-training. In Proceedings of the IEEE/CVF'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 9}, page_content='3d-language pre-training. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 10984–10994, 2023. 7\\n[26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\\nthing. In Proceedings of the IEEE/CVF International Confer-\\nence on Computer Vision , pages 4015–4026, 2023. 2\\n[27] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg,\\nMohit Bansal, and Jingjing Liu. Less is more: Clipbert for\\nvideo-and-language learning via sparse sampling. In Proceed-\\nings of the IEEE/CVF conference on computer vision and\\npattern recognition , pages 7331–7341, 2021. 5\\n[28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li,\\nHao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chun-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chun-\\nyuan Li. Llava-onevision: Easy visual task transfer. arXiv\\npreprint arXiv:2408.03326 , 2024. 2, 5\\n[29] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li,\\nWei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave:\\nTackling multi-image, video, and 3d in large multimodal\\nmodels. arXiv preprint arXiv:2407.07895 , 2024. 3\\n[30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-\\n2: Bootstrapping language-image pre-training with frozen\\nimage encoders and large language models. arXiv preprint\\narXiv:2301.12597 , 2023. 2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='arXiv:2301.12597 , 2023. 2\\n[31] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai\\nWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\\nVideochat: Chat-centric video understanding. arXiv preprint\\narXiv:2305.06355 , 2023. 2\\n[32] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,\\nYi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.\\nMvbench: A comprehensive multi-modal video understand-\\ning benchmark. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 22195–\\n22206, 2024. 5\\n[33] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='[33] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An\\nimage is worth 2 tokens in large language models. arXiv\\npreprint arXiv:2311.17043 , 2023. 8\\n[34] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and\\nLi Yuan. Video-llava: Learning united visual represen-\\ntation by alignment before projection. arXiv preprint\\narXiv:2311.10122 , 2023. 2\\n[35] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad\\nShoeybi, and Song Han. Vila: On pre-training for visual\\nlanguage models. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition , pages\\n26689–26699, 2024. 2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='26689–26699, 2024. 2\\n[36] Benlin Liu, Yuhao Dong, Yiqin Wang, Yongming Rao, Yan-\\nsong Tang, Wei-Chiu Ma, and Ranjay Krishna. Coarse corre-\\nspondence elicit 3d spacetime understanding in multimodal\\nlanguage model. arXiv preprint arXiv:2408.00754 , 2024. 2\\n[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2, 5\\n[38] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='ceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pages 26296–26306, 2024. 2, 3, 8\\n[39] Ruiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao,\\nYilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu,\\nDahua Lin, et al. Mmscan: A multi-modal 3d scene dataset\\nwith hierarchical grounded language annotations. arXiv\\npreprint arXiv:2406.09401 , 2024. 2, 6[40] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao\\nLiang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Sit-\\nuated question answering in 3d scenes. arXiv preprint\\narXiv:2210.07474 , 2022. 6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='arXiv:2210.07474 , 2022. 6\\n[41] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta,\\nSriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mc-\\nvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa:\\nEmbodied question answering in the era of foundation models.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 16488–16498, 2024.\\n2, 6\\n[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748–8763. PMLR, 2021. 2\\n[43] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-\\nactor: A multi-task transformer for robotic manipulation. In\\nConference on Robot Learning , pages 785–799. PMLR, 2023.\\n2\\n[44] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui\\nWu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\\nSchalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a\\nfamily of highly capable multimodal models. arXiv preprint\\narXiv:2312.11805 , 2023. 2, 6\\n[45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-\\njad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\\nBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\\nOpen foundation and fine-tuned chat models. arXiv preprint\\narXiv:2307.09288 , 2023. 6\\n[46] Heng Wang, Chaoyi Zhang, Jianhui Yu, and Weidong Cai.\\nSpatiality-guided transformer for 3d dense captioning on\\npoint clouds. arXiv preprint arXiv:2204.10688 , 2022. 7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='point clouds. arXiv preprint arXiv:2204.10688 , 2022. 7\\n[47] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng,\\nand Jiashi Feng. Pllava: Parameter-free llava extension from\\nimages to videos for video dense captioning. arXiv preprint\\narXiv:2404.16994 , 2024. 2\\n[48] Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan,\\nMadhavan Iyengar, David F Fouhey, and Joyce Chai. Llm-\\ngrounder: Open-vocabulary 3d visual grounding with large\\nlanguage model as an agent. arXiv preprint arXiv:2309.12311 ,\\n2023. 7\\n[49] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='modular co-attention networks for visual question answering.\\nInProceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition , pages 6281–6290, 2019. 5\\n[50] Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvg-\\ntransformer: Relation modeling for visual grounding on point\\nclouds. In Proceedings of the IEEE/CVF International Con-\\nference on Computer Vision , pages 2928–2937, 2021. 7\\n[51] Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, and\\nXihui Liu. Empowering 3d visual grounding with reasoning\\ncapabilities. arXiv preprint arXiv:2407.01525 , 2024. 2, 4, 7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 10}, page_content='[52] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan\\nHuang, and Qing Li. 3d-vista: Pre-trained transformer for 3d\\nvision and text alignment. arXiv preprint arXiv:2308.04352 ,\\n2023. 5, 7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 11}, page_content='Figure 5. LLaV A-3D could perform 2D Click-based 3D dense captioning, generating the corresponding object caption and 3D bounding box.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 12}, page_content='Figure 6. LLaV A-3D could perform 2D Click-based 3D question answering, now users could click on the 2D images and ask the question.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18125.pdf', 'page': 13}, page_content='Figure 7. LLaV A-3D exhibits powerful 3D visual grounding capability, enabling accurate 3D bounding boxes output.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 0}, page_content='Technical Report\\nEGOLM: M ULTI -MODAL LANGUAGE MODEL OF EGO-\\nCENTRIC MOTIONS\\nFangzhou Hong1,2, Vladimir Guzov1,3, Hyo Jin Kim1, Yuting Ye1\\nRichard Newcombe1, Ziwei Liu2 \\x00, Lingni Ma1 \\x00\\n1Meta Reality Labs Research,2S-Lab, Nanyang Technological University\\n3University of Tuebingen\\n“The person is standing straight as she puts the piece of clothing on the hanger.”“The person turns around then walks out of the bedroom.”Input#1:Sparse Motion Sensors\\nInput#2:Egocentric VideosTask#1:Motion Tracking\\nTask#2:Motion Understanding\\nEgoLMab\\nab'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 0}, page_content='Task#2:Motion Understanding\\nEgoLMab\\nab\\nFigure 1: We propose EgoLM , a multi-modal language model that unifies egocentric motion track-\\ning and understanding from wearable sensor data, e.g., sparse motion sensors and egocentric videos.\\nABSTRACT\\nAs the prevalence of wearable devices, learning egocentric motions becomes es-\\nsential to develop contextual AI. In this work, we present EgoLM , a versatile\\nframework that tracks andunderstands egocentric motions from multi-modal\\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich con-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 0}, page_content='texts for the disambiguation of egomotion tracking and understanding, which\\nare ill-posed under single modality conditions. To facilitate the versatile and\\nmulti-modal framework, our key insight is to model the joint distribution of\\negocentric motions and natural languages using large language models (LLM).\\nMulti-modal sensor inputs are encoded and projected to the joint latent space\\nof language models, and used to prompt motion generation or text generation\\nfor egomotion tracking or understanding, respectively. Extensive experiments'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 0}, page_content='on large-scale multi-modal human motion dataset validate the effectiveness of\\nEgoLM as a generalist model for universal egocentric learning. Project Page:\\nhttps://hongfz16.github.io/projects/EgoLM .\\n1 I NTRODUCTION\\nWith the recent explosive advancement of large language models, their values as intelligent agents\\nhave been thoroughly studied (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; Tou-\\nvron et al., 2023a;b). To better play the role of everyday smart assistant, the contextualization of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 0}, page_content='AI is proposed and studied (Vercauteren et al., 2019; Deepika et al., 2020). Agents are expected\\nto interact with users in a context-aware style, through multi-modal sensors on wearable devices,\\ne.g., smartwatches, smart glasses (Somasundaram et al., 2023). Human motions play an important\\nrole in the user-agent interaction (Plizzari et al., 2023), which requires egocentric human motion\\nlearning (Li et al., 2015).\\nIn this work, we propose a versatile framework EgoLM that approaches human motions learning'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 0}, page_content='from egocentric perspective. Specifically, EgoLM unifies two aspects of egocentric motion learning,\\ni.e., tracking and understanding. a)Egocentric motion tracking aims to recover full-body motions\\n1arXiv:2409.18127v1  [cs.CV]  26 Sep 2024'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='Technical Report\\nfrom sparse motion sensors, e.g., three-points (head and both wrists) 6-DoF poses (Jiang et al., 2022;\\nCastillo et al., 2023; Du et al., 2023; Jiang et al., 2023) or one-point (only head) 6-DoF poses (Li\\net al., 2023). b)Egocentric motion understanding aims to recognize or describe human motions\\nfrom wearable sensors, e.g., egocentric videos (Damen et al., 2021; 2022; 2018; Nagarajan et al.,\\n2024; Xue et al., 2023; Escobar et al., 2022; Grauman et al., 2022; Rodin et al., 2021; Yonetani'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='et al., 2016; Del Molino et al., 2016; Chen et al., 2023). Both tasks are highly challenging due to the\\nincomplete observation from egocentric perspectives . To this end, we propose to approach the\\nchallenges in an unified way by incorporating multiple modalities andmulti-task training , which\\nare elaborated below.\\nEgocentric motion tracking from sparse sensors is an ill-posed problem. Three-points (Jiang et al.,\\n2022) and one-point inputs (Li et al., 2023) miss information of the lower body parts and even'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='hand positions, making it a one-to-many mapping problem. In order to disambiguate the tracking\\nof unobserved body parts, we explore the environment contexts by egocentric videos captured from\\nhead-mounted cameras. Although other body parts are not always visible from egocentric videos,\\nthe semantics of the environment provides valuable clues to disambiguate full body motions.\\nFor egocentric motion understanding, the common input setting is the egocentric video (Jia et al.,\\n2022). However, egocentric videos lacks the accurate information of full-body motion, for their'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='restricted viewing angles. For better understanding of human motion, sparse motion sensor data is\\nvaluable in terms of providing accurate body part positions (Tan et al., 2024). Therefore, we unify\\nthe input conditions as egocentric videos and sparse sensors for both tracking and understanding.\\nFurther unifying the training of both tasks can also be beneficial, especially for the understanding\\npart. The supervision signals of full-body motions from motion tracking training can contribute to\\nmotion understanding.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='motion understanding.\\nIn summary, we aim at a multi-modal multi-tasking generative framework. As shown in Fig. 1,\\nEgoLM takes sparse motion sensor data (three-points or one-point) and egocentric videos as inputs.\\nThen motion and natural languages are generated for motion tracking and understanding, respec-\\ntively. To facilitate this versatile framework, there are two main challenges in the framework design,\\nwhich are large modality gaps andlarge task gaps . To that end, our key insight is to use a lan-\\nguage model to handle the multi-modal inputs and multi-task training .'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='Unlike recent VLMs (Liu et al., 2023b;a), our setting is more complex and challenging, where four\\nmodalities are involved, including sparse motion sensor data, egocentric videos, motion represen-\\ntations, and texts. These modalities provide different granularity of information. Human motions\\nand sparse motion sensor data are low-level and contiguous representations with physical meanings.\\nNatural languages, on the other hand, are unstructured and discrete representations. To bridge the\\ngap, we adopt three strategies: a)Treat motions as languages. A motion VQ-V AE is trained to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='tokenize motions, which can be generated autoregressively by a language model. b)Unify differ-\\nent inputs to the language model space. Sparse sensor data and egocentric videos are encoded and\\nprojected by light-weight temporal encoders. c)Use instruction tuning for multi-task joint training.\\nTo validate the proposed framework, we perform extensive experiments on a large-scale motion\\ndataset, Nymeria (Ma et al., 2024). Compared with previous motion tracking and understanding\\nmethods, our newly proposed multi-modal setup shows its advantages. Our contributions are sum-\\nmarized below.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='marized below.\\n1)We propose a versatile multi-modal generative framework, EgoLM, that unifies egocentric motion\\ntracking and understanding tasks with a language model.\\n2)A new egocentric motion tracking setup is proposed. We combine sparse sensor inputs with\\negocentric videos to provide more contexts that disambiguate this ill-posed problems.\\n3)We propose a practical paradigm of motion understanding by combining sparse sensor data and\\negocentric videos, which provides more accurate full-body motion narration.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 1}, page_content='4)Extensive experiments and studies are performed to show the effectiveness of the proposed frame-\\nwork. Our setup achieves the best performance compared with the state-of-the-art methods.\\n2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='Technical Report\\n2 R ELATED WORK\\nMotion Regression. Large amounts of efforts are devoted to detect and track 2D or 3D keypoints\\nfrom human images and videos (Toshev & Szegedy, 2014; Martinez et al., 2017; Pavllo et al., 2019).\\nTo incorporate more human structure prior, parametric human models, e.g., SMPL (Loper et al.,\\n2023), are used as the regression target (Bogo et al., 2016; Kanazawa et al., 2018). Other than\\nthe cameras, wearable motion sensors are straight-forward in terms of motion capture (Ponton et al.,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='2023; Mollyn et al., 2023; Milef et al., 2023; Yi et al., 2023; Jiang et al., 2023). Recent advancements\\nin VR/AR devices and applications have developed a new setup for motion tracking, i.e., three-points\\nbody tracking (Du et al., 2023; Jiang et al., 2022; Castillo et al., 2023). EgoEgo (Li et al., 2023)\\nproposes to track motions from only head poses. In this work, we also target motion tracking from\\nsparse sensors. The difference is that we propose to use egocentric videos to disambiguate ill-posed\\nscenarios in this setup.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='scenarios in this setup.\\nMotion Generation. There have been many efforts in generating motions from various conditions,\\ni.e., action labels (Petrovich et al., 2021; Guo et al., 2020), natural languages (Zhang et al., 2024;\\nTevet et al., 2022; Punnakkal et al., 2021; Guo et al., 2022a; Zhang et al., 2023b; Guo et al., 2022b).\\nRecently, researchers take advantage of powerful LLMs to model the joint motion-language dis-\\ntribution for text-to-motion generation (Jiang et al., 2024; Zhang et al., 2023c; Zhou et al., 2023).'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='In this work, we also adopt the similar idea of modeling motion together with language models.\\nAs a by-product, we can also perform text-to-motion generation. But our main focus is on motion\\ntracking and understanding from multi-modal inputs.\\nMotion Understanding. There have been many different setups in motion understanding. From\\nthe input side, human videos, either from third-person view (Soomro et al., 2012; Kuehne et al.,\\n2011; Tran et al., 2015; Wang et al., 2016; Yan et al., 2018) or first-person view (Damen et al.,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='2021; 2022; 2018), are used for this task. From the output side, action recognition/classification\\nhas been a classic task definition (Soomro et al., 2012; Damen et al., 2018). More recently, with\\nthe development of language models, some researches also propose to use natural languages as\\noutput (Jia et al., 2022; Xu et al., 2024; Grauman et al., 2022; Xue et al., 2023; Chen et al., 2023).\\nIn our work, from the input side, we propose to combine egocentric videos, which provide high-\\nlevel semantic information, with motion sensor inputs, which carries low-level motion clues, for'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='more holistic motion understanding. For the output, we use natural language responses for more\\nversatility and diversity.\\nLanguage Models. Language models have been a huge success in recent years with the large-scale\\npre-training (Radford et al., 2019; Brown et al., 2020) and alignment (cha, 2022; Achiam et al.,\\n2023). To take advantage of the powerful text generation ability, image (Liu et al., 2023b;a) or\\nvideo understanding (Zhang et al., 2023a) are defined as conditional text generation. LLaV A (Liu'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='et al., 2023b) proposes to encode images with powerful pre-trained vision encoders (Radford et al.,\\n2021) and inject the features to language models. By tuning from a powerful LLM (Touvron et al.,\\n2023a), LLaV A achieves wonderful abilities of vision question answering. In this work, we also\\nadopt the similar idea of encoding and injecting features of other modalities in the language model\\nand unifying different tasks with instruction tuning.\\n3 M ETHOD\\nThe overview of EgoLM is demonstrated in Fig. 2. There are three steps in EgoLM training. In the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='first step, we train a motion VQ-V AE as the motion tokenizer (Sec. 3.2). The second step is motion\\npre-training for motion distribution learning (Sec. 3.3). The last step is multi-modal instruction\\ntuning to guide the model to perform motion tracking and understanding (Sec. 3.4).\\n3.1 P RELIMINARIES\\nLanguage Model. Language models model the distribution of natural languages. Recent\\nbreakthroughs in language models suggest the effectiveness of the transformer-based architec-\\nture (Vaswani et al., 2017). The language model consists of three parts. The first is a look-up table'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 2}, page_content='(LM embedding) that stores the embeddings for each text token. The second part is the transformer\\n3'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='Technical Report\\nMotion VQ-VA E\\n1) Motions Tokenization\\nLanguage Model\\n<BOS>\\n<EOS>2) Motion Pre-Training\\nEgoLM3) Multi-Modal Instruction Tuning\\nEncoder\\nVision EncoderInstructions\\nMotion Narrations\\nFigure 2: Overview of EgoLM. Three steps are designed for the training of EgoLM, i.e., motion\\ntokenizer training, motion pre-training and multi-modal instruction tuning.\\nbackbone that takes text embeddings as inputs. The output features are mapped to probabilities of\\nthe next tokens by the third part of LM head.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='the next tokens by the third part of LM head.\\nMotion Representation. Human motions are represented as sequences of poses, global translations\\nand rotations defined on the root joint. Each frame of pose is represented by joint angles, defined\\non a kinematic tree. For better learning of motion dynamics, we also include joint angle velocity in\\nthe representation. To avoid the normalization of global translation, we use the translation velocity\\nVr\\nt∈R3for each frame, which can be integrated back to global translations. To ease the regression'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='difficulty of rotation angles, we use 6D rotation representations (Hempel et al., 2022) for the root\\nrotation Rr\\nt∈R6, root rotation velocity Rrv\\nt∈R6, joint angles Rj\\nt∈R22×6, and joint angle\\nvelocity Rjv\\nt∈R22×6. Formally, we represent human motions with Tframes as M={Pt}T\\nt=1,\\nwhere Pt= [Vr\\nt;Rr\\nt;Rrv\\nt;Rj\\nt;Rjv\\nt]∈R279. Forward kinematics (FK) together with integration of\\nroot velocity can be used to recover the joint positions J=FK(M)∈R23×3.\\n3.2 M OTION TOKENIZER\\nTo treat the motion as a foreign language and train with language models, we first need a motion'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='tokenizer, which can be realized by VQ-V AE (Oord et al., 2017). The motion VQ-V AE consists of\\na fully convolutional encoder Eand decoder D. The fully convolutional design enables processing\\nmotions with arbitrary lengths. The encoder embeds raw motion representation to latent features\\nfm=E(M), where fm∈RT/r×c,M∈RT×279.ris the down-sample rate.\\nThen, codebooks are learned to quantize the motion latent features. We use three techniques in the\\nquantization process, which are 1) exponential moving average (EMA), 2) codebook reset (Dhariwal'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='et al., 2020), 3) product quantization (Jegou et al., 2010; Lucas et al., 2022). The first two techniques\\nincrease the usage rate of codebooks. Product quantization increases the codebook expressiveness\\nby decomposing the latent space into a Cartesian product of sub-spaces with lower dimensions.\\nSpecifically, the latent feature fmis split equally into Ntrucks {fm\\nn}N\\nn=1, which are quantized\\nseparately by Ncodebooks {Zn}N\\nn=1. Each codebook with Kentries is defined as Zn={zi}K\\ni=1,\\nwhere zi∈Rc/N. The quantization process for feature fm\\ntnat frame tand trunk nis formulated as\\nitn=Q(fm'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='tnat frame tand trunk nis formulated as\\nitn=Q(fm\\ntn) = arg min\\nzi∈Zn∥fm\\ntn−zi∥2. (1)\\nThe resulting indices itnare flattened and used as motion token sequences W={[(in)N\\nn=1]t}T/r\\nt=1,\\nwhich has the length of LW=N×(T/r). After quantization, we obtain the corresponding code-\\nbook entry for the motion latent feature ˆfm={ˆfm\\nt}T/r\\nt=1={zit}T/r\\nt=1. It is input into the decoder D\\nto decode raw motion representation ˆM=D(ˆfm).\\nFor the training of VQ-V AE, two types of training losses are used. The first is the commitment loss'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 3}, page_content='Lc=∥fm−ˆfm∥2for the codebook learning. The second is motion reconstruction loss Lr, which\\nconsists of raw representation loss Lm, joint position loss Lj, rotation velocity loss Lv, which are\\ndefined as\\nLr=λmLm+λjLj+λvLv=λm∥M−ˆM∥1+λj∥FK(M)−FK(ˆM)∥1 (2)\\n+λv∥Rrv\\n1:T−1−(Rr\\n1:T−1)−1Rr\\n2:T∥1+λv∥Rjv\\n1:T−1−(Rj\\n1:T−1)−1Rj\\n2:T∥1. (3)\\n4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='Technical Report\\n“<s>Perform ... based on the given ... Input CLIP embeddings: <CLIP_Placeholder>. Input three-points: <TP_Placeholder>”\\n1\\n27313\\n2729\\n373\\n278\\n10567\\n24492\\n29925\\n8297\\n10567\\n2211\\n29899\\n...\\n...\\nCLIP Encoder\\nText Tokenizer\\nText Tokenizer\\nLinear Layer\\nLM Embedding\\nLM Embedding\\nTP Encoder\\nConcatenate\\nFigure 3: Details of Multi-Modal Instruction Tuning. Different modalities are encoded separately.\\nTheir features are concatenated in the order of the instruction template and input into the transformer\\nlayers of the language model.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='layers of the language model.\\nWe define the smoothed L1 loss as ∥ · ∥ 1. In summary, the training loss of the motion VQ-V AE is\\nLvq=λcLc+λrLr, where λ∗are manually adjusted weights.\\n3.3 M OTION PRE-TRAINING\\nAs discussed before, we build the motion learning framework on a pre-trained language model.\\nHowever, the pre-trained language models only model the distribution of natural languages. There-\\nfore, to empower them to generate motions, we perform motion pre-training to learn motion distri-\\nbutions. The motion pre-training is conducted similarly to language model pre-training.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='Before we can start training the language model, two modifications to the model are needed. Firstly,\\nsince the pre-trained language model only contains embeddings for text tokens, we expand the em-\\nbeddings in accordance with the motion codebook size. Secondly, the output shape of the language\\nmodel head is also expanded for the same reason. The language model is ready for motion pre-\\ntraining after the above preparations. Using the motion tokenizer described above, motion represen-\\ntations Mcan be encoded into a sequence of motion tokens W={wi}LW\\ni=1. They are fed into the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='i=1. They are fed into the\\nlanguage model to learn the motion token distribution by conducting the classic next-token predic-\\ntion (Radford et al., 2019). The loss function of this stage Lpreis formulated as\\nLpre=−LWX\\ni=2P(wi|w1...wi−1; Θ), (4)\\nwhere we maximize the log-likelihood of the next-token probability given the previous token inputs\\nand network parameter Θ.\\nAfter the training of this stage, as a by-product, we obtain an unconditional motion generator. Given\\na leading motion sequence as the prompt, it can autoregressively sample an arbitrary length of rea-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='sonable human motion that continues the given motion. More importantly, the language model\\nlearns the distribution of human motions and has the ability of sampling plausible human motions,\\nwhich lays a solid foundation for the next stage.\\n3.4 M ULTI -MODAL INSTRUCTION TUNING\\nInspired by recent advancements in LLMs (Achiam et al., 2023; cha, 2022; Zheng et al., 2023),\\nto squeeze the power out of generative pre-training models, instruction tuning is adopted to guide\\nmodels with instructions to perform specific tasks. The instruction template usually consists of 1)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='instructions that specify which tasks to perform; 2) inputs of the task; 3) outputs. We also envision\\nour model accepting multi-modal sensor data as inputs. However, even with motion pre-training,\\nthe model only accepts text or motion tokens as inputs. It is not practical or necessary to design\\ntokenizers and perform pre-training for all the involved modalities. Therefore, we draw inspiration\\nfrom vision language models (Liu et al., 2023b;a), where they directly map vision data to LLM\\nfeature space to enable visual question answering.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 4}, page_content='feature space to enable visual question answering.\\nSpecifically, we consider two input modalities other than motion and natural languages, which are\\negocentric videos and motion sensor inputs. Motion sensor inputs can be three-points (head and\\nwrists) 6-DoF poses or one-point (only head) 6-DoF poses. Both are encoded with positions, veloc-\\nity, rotation and angular velocity. Below, we use three-points as examples. We unify both motion\\ntracking and motion understanding using the following templates.\\n5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='Technical Report\\nTask: Motion Tracking\\nInstruction: Perform motion tracking based on\\nthe given three-points and CLIP embeddings.\\nInput: Input CLIP embeddings:\\n<CLIP Placeholder> . Input three-\\npoints feature: <TPPlaceholder>\\nOutput: <Motion Placeholder>Task: Motion Understanding\\nInstruction: Describe the human motion based on\\nthe given three-points and CLIP embeddings.\\nInput: Input CLIP embeddings:\\n<CLIP Placeholder> . Input three-points\\nfeature: <TPPlaceholder>\\nOutput: <Narration Placeholder>\\nThe encoded three-points 6-DoF poses would replace <TPPlaceholder> .'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='<CLIP Placeholder> is the placeholder for egocentric video features. Motion tokens\\nare filled in <Motion Placeholder> .<Narration Placeholder> is the placeholder for\\ncorresponding motion narration. A detailed illustration of how we organize different modalities\\nof data is shown in Fig. 3. Texts are tokenized and translated to feature vectors through LM\\nembedding. Egocentric videos are first encoded by CLIP image encoder (Radford et al., 2021) per\\nframe, which are further projected by linear layers to the language model feature space. Similarly,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='motion sensor data, e.g., sequences of three-points 6-DoF poses, is encoded by a fully convolutional\\nencoder. Lastly, all the encoded features are concatenated and input into the transformer layers of\\nthe language model.\\nFor the training of motion understanding, to better learn the joint distribution of motion and natural\\nlanguages, we also include two auxiliary tasks in the joint instruction training, which are motion-\\nto-text and text-to-motion generation. They are also defined with the templates similar to the above'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='ones. In summary, we train the four tasks jointly as the last step. The loss function is the same\\nnext-token prediction loss, as defined in Eq. 4.\\n4 E XPERIMENTS\\n4.1 E XPERIMENTAL SETUP\\nDataset. We use the Nymeria dataset Ma et al. (2024) to train and validate our method. The dataset\\nprovides a)full body motions, captured by the Xsens Mocap system (Roetenberg et al., 2009),\\nb)egocentric videos, captured by Aria glasses (Somasundaram et al., 2023), and c)narrations of\\nmotions written by human annotators. Three-points 6-DoF poses are taken from ground truth joints.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='For motion tracking, the training set consists of 147.89h of data and the test set has 41.93h of data.\\nFor motion understanding, the training set has 16673 segments, each lasting for 3-5seconds, adding\\nup to 15.77h. The test set consists of 7468 segments, 6.76h of data.\\nTraining Details. Motion VQ-V AE has two codebooks, each having 8192 entries and code dimen-\\nsion of 64. The down-sample rate is r= 4. For motion tracking, all experiments are conducted with\\nwindow size of 60frames, which is 1second. Random rotation augmentation is applied on motions.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='We choose to use GPT2-Medium (Radford et al., 2019) as the language backbone.\\nEvaluation Protocols. For motion tracking, we calculate joint position errors (for full, upper and\\nlower body), joint angle errors (for full body and root joint). For motion understanding, the outputs\\nare natural languages. Therefore, we adopt NLP metrics, including BERT (Zhang et al., 2019),\\nBLEU (Papineni et al., 2002), and ROUGE (Lin, 2004) scores.\\n4.2 M OTION TRACKING\\nQuantitative Results. We report quantitative results of motion tracking in Tab. 1. All methods'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='are evaluated with batch inference, meaning that every 60frames are inferenced independently.\\nWe evaluate several different input combinations of three modalities, which are three-points 6-DoF\\nposes (“3pts”), one-point 6-DoF poses (“1pt”) and egocentric videos (“Vid”). For the 3pts-only and\\n1pt-only settings, EgoLM achieves comparable performance with baseline methods. This show the\\neffectiveness of using language models to perform precise motion tracking tasks. Moreover, we\\nalso use egocentric videos to provide environment contexts for motion tracking. For three-points'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 5}, page_content='tracking, the additional modality brings 10mm improvement in full body joints error. For the one-\\npoint tracking, adding egocentric videos improves joints error by 20mm. It shows the effectiveness\\nof using egocentric videos as context information for disambiguation of the ill-posed problem.\\n6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 6}, page_content='Technical Report\\nTable 1: Quantitative Results of Motion Tracking. “Full”, “Upper”, “Lower” are joint position\\nerrors in mm. “J.A.”, “Root” are joint angle errors for full body and root joint in degree.†We\\ndirectly replace three-points with one-point to train AvatarPoser.\\nMethodInput ModalityFull Upper Lower J.A. Root3pts 1pt Vid.\\nAvatarPoser (Jiang et al., 2022) ✓ 85.89 52.78 165.18 12.41 14.78\\nBodiffusion (Castillo et al., 2023) ✓ 79.80 52.79 152.68 12.74 13.09\\nOurs ✓ 83.88 54.06 148.37 13.31 14.13\\nOurs ✓ ✓ 73.38 49.67 124.58 12.48 13.23'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 6}, page_content='Ours ✓ ✓ 73.38 49.67 124.58 12.48 13.23\\nAvatarPoser†(Jiang et al., 2022) ✓ 129.23 94.19 192.34 16.55 21.60\\nEgoEgo (Li et al., 2023) ✓ 132.16 100.02 190.32 18.90 21.80\\nOurs ✓ 127.45 97.87 174.92 16.97 20.57\\nOurs ✓ ✓ 106.95 83.73 141.26 14.67 19.04\\nAvatarPoserBoDiffusion\\nOurs\\nGT\\n0mm200mmEgocentricVideo\\nFigure 4: Qualitative Results of Three-Points Motion Tracking. Skeletons are color-coded by the\\njoint position errors. Baseline methods only use three-points as inputs. Ours uses three-points and\\negocentric videos as inputs.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 6}, page_content='egocentric videos as inputs.\\nQualitative Results. Three-points motion tracking results and comparisons are shown in Fig. 4.\\nDue to the ambiguity of three-points, AvatarPoser mistakenly generates standing poses for squatting\\nsequences (right example). BoDiffusion, for its generative nature, can sample correct results in\\nsome cases, e.g., the squatting example. But it also suffers from the ambiguity issue, as shown in\\nthe bending down sequence (left example). They show the importance of considering contexts in'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 6}, page_content='the motion tracking task for the purpose of disambiguation. Our full model can reliably perform\\nthree-points body tracking for the shown challenging cases.\\nOne-point motion tracking results are shown in Fig. 5. It is a more challenging task especially\\nfor upper body. As shown in the left example, the upper body motions generated by EgoEgo are\\ncompletely different from the ground truth. In the right example, EgoEgo wrongly generates sitting\\nposes for standing frames and standing poses for sitting frames, which is caused by the ambiguity'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 6}, page_content='problem. Egocentric videos in this task not only help to eliminate the ambiguity, but also provide\\nsome clues about the hand position. In the left example, when hands are visible in the frames, our\\nmodel captures this information through CLIP embeddings and generates correct arm movements.\\n7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='Technical Report\\nEgoEgoOursEgocentricVideo0mm200mmGT\\nFigure 5: Qualitative Results of One-Point Motion Tracking. Skeletons are color-coded by joint\\nposition errors. EgoEgo only uses one-point as inputs. Ours includes egocentric videos as inputs.\\n4.3 M OTION UNDERSTANDING\\nQuantitative Results. We report quantitative results of motion understanding in Tab. 2. For\\nthis task, we tested three input modalities, i.e., three-points (“3pts”), motions, and egocentric\\nvideos (“Vid”). Different combinations of these modalities are evaluated. We first test and com-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='pare with two motion understanding methods that only take motion as inputs, TM2T (Guo et al.,\\n2022b) and MotionGPT (Jiang et al., 2024). TM2T trains language generation from scratch, which\\nexplains its poor performance. MotionGPT uses a pre-trained T5 model (Raffel et al., 2020).\\nEgoLM(M2T&T2M) achieves the best performance for the scalability advantage brought by the\\ndecoder-only architecture.\\nUsing motion as inputs requires precise motion tracking, which is not always available. So, we\\nexplored using sensor inputs instead. We tested two variants: three-points-only (TP2T) and egocen-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='tric videos only (V2T). The TP2T variant showed a noticeable drop in performance compared to the\\nmotion-only version, as three-points provide limited information about body motion. In contrast, the\\nV2T variant outperformed the motion-only version because egocentric videos capture environmen-\\ntal context relevant to our motion narrations. This highlights the importance of egocentric videos in\\nunderstanding motion.\\nWe then test our proposed setup of combing three-points and egocentric videos for motion under-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='standing. There are three ways of achieving this setup. The first one is to combine two existing\\nsetups: 1) three-points motion tracking and 2) motion-to-text generation (TPV2M +MV2T). The\\nperformance of this variant slightly drops compared with MV2T variant, due to error accumulation.\\nThe second way is directly training three-points plus egocentric video to text generation (TPV2T)\\nwith the proposed multi-modal instruction tuning. It is better than only using egocentric videos or\\nmotions. However, it still falls behind MV2T variant for the missing lower body information. To'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='solve that, we propose to also include three-points motion tracking in training to actively establish\\nthe connection between three-points and motion narrations. Joint training improves motion under-\\nstanding from three-points plus egocentric video, which proves the effectiveness of using motion as\\na bridge between different modalities.\\nQualitative Results. We show four examples of motion understanding in Fig. 6. TM2T and Mo-\\ntionGPT use full body motions as inputs. Ours is the full version with three-points and egocentric'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 7}, page_content='videos as inputs. TM2T’s language generation part is trained from scratch. Therefore, it often makes\\nmistakes about motions and even generates texts that does not make sense. MotionGPT can generate\\nreasonable descriptions for the motions. In the lower left example, just from the motions, “removing\\na piece of clothing from the hanger” is a reasonable answer. However, our target motion narration\\n8'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='Technical Report\\nTable 2: Quantitative Results of Motion Understanding. Different input modality combinations\\nare tested. All metrics are higher the better.\\nMethodInput ModalityBert↑Bleu@1 ↑Bleu@4 ↑RougeL ↑3pts Motion Vid.\\nTM2T (Guo et al., 2022b) ✓ 11.08 40.11 8.99 30.70\\nMotionGPT (Jiang et al., 2024) ✓ 14.09 42.22 10.31 32.33\\nOurs (M2T&T2M) ✓ 15.90 42.68 11.06 33.71\\nOurs (TP2T) ✓ 11.94 41.70 9.85 31.47\\nOurs (V2T) ✓ 16.62 43.03 11.34 33.13\\nOurs (TPV2M +MV2T) ✓ ✓ 19.97 45.41 12.81 35.04\\nOurs (TPV2T) ✓ ✓ 18.38 44.55 12.12 33.80\\nOurs (Joint Training) ✓ ✓ 19.40 45.45 12.72 34.82'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='Ours (Joint Training) ✓ ✓ 19.40 45.45 12.72 34.82\\nOurs: The human is standing in the bedroomto fold the piece of clothing. The human is folding the piece of clothing with his left and right hand. The person is resting his left and right foot on the floor.\\nOurs:The person bends down as she plankson the floor. The human extends both of her arms on the floor to support her body. The person extends both of her legs while tiptoeing both of her feet.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='Ours: The person is standing still in the living room while talking to her peer. The human lifts both of his arms and then moves both hands in circular motion as she gesticulates. The human rests both of his feet on the ground.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content=\"Ours: The person is standing by the refrigerator while putting the pack of food inside the freezer. The human puts the pack of food inside the freezer with her right hand as her left hand holds the refrigerator door. The human is standing with both feed fixed on the floor.MotionGPT: The person is standing still in front of the sofawhile holding a piece of clothing. The human's left arm is bent and raised upward with his left hand holding a piece of clothing. The human is standing with both legs apart and both feet resting on the floor.TM2T:The person is sitting at the table as he lays her\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='person is sitting at the table as he lays her body on the sofa then leans backwards while talking and looking at her colleague. The person is resting both of her arms on her lap, lifts and bends both of her arms as she sits down on the sofa. The person is sitting on the sofa with both legs bent and slightly spread apart.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='MotionGPT: The human bends down while kneeling in the living area. The person extends both of her arms on the floor to support her body. The human extends both of her legs on the ground.TM2T: The person lowers her thigh as she lays down on the floor while kneeling on the floor. The person extends both her arms as she moves her right elbow on the floor to support her body.MotionGPT: The person is standing straight at the living room … The human has both arms naturally hanging at her sides then she bends, extends and raises her right arm and throws the object on the living room with her right'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='arm and throws the object on the living room with her right hand. … The human has both feet fixed on the floor with both legs stretched upright then she slightly bends and spreads both of her legs widely apart.TM2T:The person is standing still in front of the cabinet while making a hanger. The person bends and raises her left hand then lays the hanger on her side of her chest then spreads both arms on her side below her chest. The person stands with both legs stretched upright and both feet fixed on the floor.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='MotionGPT:The person stands in front of the cabinet to remove the clothes from the hanger. the human raises both of his arms to remove a piece of clothing from the hanger. the human stands with both feet fixed on the floor.TM2T: the person stands up straight as she holds the pillow and place them on the table. the person then arrange the pillow in the middle of the room with her right hand and places it on the table, while her left arm is slightly bent in front as she holds and arrange the pillow in the direction of the table.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='Figure 6: Qualitative Results of Motion Understanding. We use green to highlight correct parts\\nand red for mistakes.\\nis highly related to environments. TM2T and MotionGPT fail to generate correct narrations for the\\nlack of vision signals. For our model, even though we do not directly use motions as inputs, EgoLM\\njointly model the distributions of different modalities and can generate correct narrations according\\nto different scenarios.\\n4.4 A BLATION STUDY\\nWindow Size of Motion Tracking. As shown in Tab. 3, we increase the window size for three-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 8}, page_content='points motion tracking from 60to120frames, which brings an improvement of 4.2mm in joint\\nposition errors. This is reasonable since increasing the window size brings more contexts, which\\nhelps the disambiguation. If we further include egocentric videos in the inputs, the improvement\\nof increasing window size is not as large. Moreover, using 60frames plus egocentric video shows\\nbetter performance than only using 120frames. This indicates that the context of egocentric video\\nmight be more effective than increasing window size.\\n9'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='Technical Report\\nTable 3: Ablation Study\\non Window Size for Motion\\nTracking.\\nWin Vid Full Upper Lower J.A.\\n60 83.88 54.06 148.37 13.31\\n120 79.61 52.66 138.87 13.01\\n60✓ 73.38 49.67 124.58 12.48\\n120✓ 72.76 49.20 123.09 12.52Table 4: Ablation Study on Recon-\\nstruction Results of Motion VQ-\\nV AE. [ mm]\\nPQ CB Dim MPJPE PA-MPJPE ACCEL\\n1 2048 512 51.60 37.55 1.09\\n2 2048 512 39.63 29.77 0.71\\n2 16384 256 39.13 29.78 1.08\\n2 16384 64 34.49 26.83 0.67Table 5: Ablation on\\nthe LM size. Medium:\\n345M; Large: 1.5B\\nGPT-2 Size Medium Large\\nBert↑ 18.38 19.56\\nBleu@1 ↑ 44.55 44.48\\nBleu@4 ↑ 12.12 12.49'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='Bert↑ 18.38 19.56\\nBleu@1 ↑ 44.55 44.48\\nBleu@4 ↑ 12.12 12.49\\nRougeL ↑ 33.80 35.21'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='Bleu@4 ↑ 12.12 12.49\\nRougeL ↑ 33.80 35.21\\nInput Prompt:The human leans forward and then turns right while walking towards the kitchen sink. The person holds and close the kitchen drawer with her left hand while the right arm rest beside her. The person bends her both legs and then steps backward.Input Prompt:The person walks toward the kitchen gas range and then grabs the fork while her left arm rest beside her. The person is walking forward to kitchen gas range with her both feet and then steps sideward with her right and left footrespectively.b) Motion Prediction Results'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='a) Text-to-Motion Generation Results\\nFigure 7: More Analysis on EgoLM. a) Qualitative results of text-to-motion generation. b)Quali-\\ntative results of motion prediction.\\nMotion VQ-VAE. Ablation studies on motion VQ-V AE are reported in Tab. 4. “PQ” is the number\\nof codebooks. “CB” is the total number of entries in codebooks. The first two lines shows that\\nlarge improvements can be achieved by simply using product quantization. Moreover, increasing\\nthe number of codes and decreasing code dimensions bring further improvement.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='Larger Language Model. We use GPT-2 Medium (345M) to conduct most of our experiments\\nfor efficiency. To examine the potential of using larger LM, we train with GPT-2 Large (1.5B) and\\nreport performance on TPV2T in Tab. 5. The improved scores suggest EgoLM’s scalability as a\\nversatile framework.\\n4.5 M ORE APPLICATIONS\\nText-to-Motion Generation. As part of our joint training, EgoLM is capable of generating motions\\nfrom texts, as shown in Fig. 7 a). Even with long prompts separately describing upper body and\\nlower body, our model is able to generate motions that match the inputs.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='Motion Prediction. As a by-product of the motion pre-training, EgoLM can function as a motion\\npredictor. As shown in Fig. 7 b), given motion prompts (the red skeleton in the left), subsequent\\nmotions can be randomly sampled. We show three different samples in different colors.\\n5 D ISCUSSION\\nWe propose EgoLM, a multi-modal language model for egocentric motion tracking and understand-\\ning. A three-steps paradigm, including motion tokenization, motion pre-training and multi-modal\\ninstruction tuning, is proposed to facilitate the training. In contrast to previous works, the proposed'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='framework unifies the egocentric motion tasks with a language model, and incorporates multi-modal\\nsensor data as context information, which is proven effective for both tasks.\\nLimitations. Firstly, our motion tokenizer is a VQ-V AE, which carries reconstruction errors. It\\nsets an upper bound for motion tracking. Moreover, for the motion tracking training, the loss is\\ncalculated on discrete motion tokens, instead of raw motion representations, which might also harm\\nthe performance of motion tracking. Secondly, for motion understanding, since each egocentric'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 9}, page_content='video frame is compressed by the CLIP encoder to a one-dimensional vector, it is hard for models\\nto precisely name the object that the person is interacting with. Moreover, as is commonly observed\\nin language models (Ji et al., 2023), EgoLM also suffers from the hallucination problem.\\nPotential Societal Impact. While contextual AI offers opportunities for efficiency improvement\\nand societal advancement, the collection and analysis of human data could lead to privacy issues for\\nboth users and people around.\\n10'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='Technical Report\\nREFERENCES\\nNov 2022. URL https://openai.com/blog/chatgpt .\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\\nman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical\\nreport. arXiv preprint arXiv:2303.08774 , 2023.\\nFederica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J.\\nBlack. Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image.\\nInComputer Vision – ECCV 2016 , Lecture Notes in Computer Science. Springer International'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='Publishing, October 2016.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.\\nAngela Castillo, Maria Escobar, Guillaume Jeanneret, Albert Pumarola, Pablo Arbel ´aez, Ali Thabet,\\nand Artsiom Sanakoyeu. Bodiffusion: Diffusing sparse observations for full-body human motion\\nsynthesis. arXiv preprint arXiv:2304.11118 , 2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='synthesis. arXiv preprint arXiv:2304.11118 , 2023.\\nYi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and\\nXihui Liu. Egoplan-bench: Benchmarking egocentric embodied planning with multimodal large\\nlanguage models. arXiv preprint arXiv:2312.06722 , 2023.\\nDima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos\\nKazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scal-\\ning egocentric vision: The epic-kitchens dataset. In European Conference on Computer Vision\\n(ECCV) , 2018.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='(ECCV) , 2018.\\nDima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos\\nKazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. The\\nepic-kitchens dataset: Collection, challenges and baselines. IEEE Transactions on Pattern Anal-\\nysis and Machine Intelligence (TPAMI) , 43(11):4125–4141, 2021. doi: 10.1109/TPAMI.2020.\\n2991965.\\nDima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evange-\\nlos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. Interna-\\ntional Journal of Computer Vision (IJCV) , 130:33–55, 2022. URL https://doi.org/10.\\n1007/s11263-021-01531-2 .\\nKanakamedala Deepika, Veeranki Tilekya, Jatroth Mamatha, and T Subetha. Jollity chatbot-a con-\\ntextual ai assistant. In 2020 Third International Conference on Smart Systems and Inventive\\nTechnology (ICSSIT) , pp. 1196–1200. IEEE, 2020.\\nAna Garcia Del Molino, Cheston Tan, Joo-Hwee Lim, and Ah-Hwee Tan. Summarization of ego-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='centric videos: A comprehensive survey. IEEE Transactions on Human-Machine Systems , 47(1):\\n65–76, 2016.\\nPrafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever.\\nJukebox: A generative model for music. arXiv preprint arXiv:2005.00341 , 2020.\\nYuming Du, Robin Kips, Albert Pumarola, Sebastian Starke, Ali Thabet, and Artsiom Sanakoyeu.\\nAvatars grow legs: Generating smooth human motion from sparse tracking inputs with diffusion\\nmodel. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\\npp. 481–490, 2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 10}, page_content='pp. 481–490, 2023.\\nMaria Escobar, Laura Daza, Cristina Gonz ´alez, Jordi Pont-Tuset, and Pablo Arbel ´aez. Video\\nswin transformers for egocentric video understanding@ ego4d challenges 2022. arXiv preprint\\narXiv:2207.11329 , 2022.\\n11'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='Technical Report\\nKristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Gird-\\nhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in\\n3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pp. 18995–19012, 2022.\\nChuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and\\nLi Cheng. Action2motion: Conditioned generation of 3d human motions. In Proceedings of the\\n28th ACM International Conference on Multimedia , pp. 2021–2029, 2020.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating\\ndiverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pp. 5152–5161, June 2022a.\\nChuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for\\nthe reciprocal generation of 3d human motions and texts. In European Conference on Computer\\nVision , pp. 580–597. Springer, 2022b.\\nThorsten Hempel, Ahmed A Abdelrahman, and Ayoub Al-Hamadi. 6d rotation representation for'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='unconstrained head pose estimation. In 2022 IEEE International Conference on Image Processing\\n(ICIP) , pp. 2496–2500. IEEE, 2022.\\nCatalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3. 6m: Large scale\\ndatasets and predictive methods for 3d human sensing in natural environments. IEEE transactions\\non pattern analysis and machine intelligence , 36(7):1325–1339, 2013.\\nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor\\nsearch. IEEE transactions on pattern analysis and machine intelligence , 33(1):117–128, 2010.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM\\nComputing Surveys , 55(12):1–38, 2023.\\nBaoxiong Jia, Ting Lei, Song-Chun Zhu, and Siyuan Huang. Egotaskqa: Understanding human\\ntasks in egocentric videos. Advances in Neural Information Processing Systems , 35:3343–3360,\\n2022.\\nBiao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as\\na foreign language. Advances in Neural Information Processing Systems , 36, 2024.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='Jiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa Laich, Patrick Snape, and Christian\\nHolz. Avatarposer: Articulated full-body pose tracking from sparse motion sensing. In European\\nConference on Computer Vision , pp. 443–460. Springer, 2022.\\nJiaxi Jiang, Paul Streli, Manuel Meier, Andreas Fender, and Christian Holz. Egoposer: Robust\\nreal-time ego-body pose estimation in large scenes. arXiv preprint arXiv:2308.06493 , 2023.\\nAngjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='human shape and pose. In Computer Vision and Pattern Recognition (CVPR) , 2018.\\nHildegard Kuehne, Hueihan Jhuang, Est ´ıbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a\\nlarge video database for human motion recognition. In 2011 International conference on computer\\nvision , pp. 2556–2563. IEEE, 2011.\\nJiaman Li, Karen Liu, and Jiajun Wu. Ego-body pose estimation via ego-head pose estimation.\\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.\\n17142–17151, 2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 11}, page_content='17142–17151, 2023.\\nYin Li, Zhefan Ye, and James M Rehg. Delving into egocentric actions. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition , pp. 287–295, 2015.\\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\\nbranches out , pp. 74–81, 2004.\\n12'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='Technical Report\\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\\ntuning, 2023a.\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS ,\\n2023b.\\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl:\\nA skinned multi-person linear model. In Seminal Graphics Papers: Pushing the Boundaries,\\nVolume 2 , pp. 851–866. 2023.\\nThomas Lucas, Fabien Baradel, Philippe Weinzaepfel, and Gr ´egory Rogez. Posegpt: Quantization-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='based 3d human motion generation and forecasting. In European Conference on Computer Vision ,\\npp. 417–435. Springer, 2022.\\nLingni Ma, Yuting Ye, Fangzhou Hong, Vladimir Guzov, Yifeng Jiang, Rowan Postyeni, Luis\\nPesqueira, Alexander Gamino, Vijay Baiyya, Hyo Jin Kim, et al. Nymeria: A massive collec-\\ntion of multimodal egocentric daily motion in the wild. arXiv preprint arXiv:2406.09905 , 2024.\\nJulieta Martinez, Rayat Hossain, Javier Romero, and James J. Little. A simple yet effective baseline\\nfor 3d human pose estimation. In ICCV , 2017.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='for 3d human pose estimation. In ICCV , 2017.\\nNicholas Milef, Shinjiro Sueda, and N Khademi Kalantari. Variational pose prediction with dynamic\\nsample selection from sparse tracking signals. In Computer Graphics Forum , volume 42, pp. 359–\\n369. Wiley Online Library, 2023.\\nVimal Mollyn, Riku Arakawa, Mayank Goel, Chris Harrison, and Karan Ahuja. Imuposer: Full-\\nbody pose estimation using imus in phones, watches, and earbuds. In Proceedings of the 2023\\nCHI Conference on Human Factors in Computing Systems , pp. 1–12, 2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='Tushar Nagarajan, Santhosh Kumar Ramakrishnan, Ruta Desai, James Hillis, and Kristen Grauman.\\nEgoenv: Human-centric environment representations from egocentric video. Advances in Neural\\nInformation Processing Systems , 36, 2024.\\nAaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn-\\ning. arXiv preprint arXiv:1711.00937 , 2017.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='for Computational Linguistics , pp. 311–318, 2002.\\nDario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3d human pose es-\\ntimation in video with temporal convolutions and semi-supervised training. In Conference on\\nComputer Vision and Pattern Recognition (CVPR) , 2019.\\nMathis Petrovich, Michael J Black, and G ¨ul Varol. Action-conditioned 3d human motion synthesis\\nwith transformer vae. In Proceedings of the IEEE/CVF International Conference on Computer\\nVision , pp. 10985–10995, 2021.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='Vision , pp. 10985–10995, 2021.\\nChiara Plizzari, Gabriele Goletto, Antonino Furnari, Siddhant Bansal, Francesco Ragusa, Gio-\\nvanni Maria Farinella, Dima Damen, and Tatiana Tommasi. An outlook into the future of egocen-\\ntric vision. arXiv preprint arXiv:2308.07123 , 2023.\\nJose Luis Ponton, Haoran Yun, Andreas Aristidou, Carlos Andujar, and Nuria Pelechano. Sparse-\\nposer: Real-time full-body motion reconstruction from sparse data. ACM Transactions on Graph-\\nics, 43(1):1–14, 2023.\\nAbhinanda R. Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 12}, page_content='and Michael J. Black. BABEL: Bodies, action and behavior with english labels. In Proceedings\\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , pp. 722–731, June 2021.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\\nmodels are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\\n13'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='Technical Report\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\\nmodels from natural language supervision. In International conference on machine learning , pp.\\n8748–8763. PMLR, 2021.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\\ntransformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='Ivan Rodin, Antonino Furnari, Dimitrios Mavroeidis, and Giovanni Maria Farinella. Predicting the\\nfuture from first person (egocentric) vision: A survey. Computer Vision and Image Understanding ,\\n211:103252, 2021.\\nDaniel Roetenberg, Henk Luinge, and Per Slycke. Xsens mvn: Full 6dof human motion tracking\\nusing miniature inertial sensors. Xsens Motion Technol. BV Tech. Rep. , 3, 01 2009.\\nKiran Somasundaram, Jing Dong, Huixuan Tang, Julian Straub, Mingfei Yan, Michael Goesele,\\nJakob Julian Engel, Renzo De Nardi, and Richard Newcombe. Project aria: A new tool for'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='egocentric multi-modal ai research. arXiv preprint arXiv:2308.13561 , 2023.\\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions\\nclasses from videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.\\nShuhan Tan, Tushar Nagarajan, and Kristen Grauman. Egodistill: Egocentric head motion distilla-\\ntion for efficient video understanding. Advances in Neural Information Processing Systems , 36,\\n2024.\\nGuy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano.\\nHuman motion diffusion model. arXiv preprint arXiv:2209.14916 , 2022.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='Alexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep neural net-\\nworks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\\n1653–1660, 2014.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee\\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\\nefficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023a.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023b.\\nDu Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spa-\\ntiotemporal features with 3d convolutional networks. In Proceedings of the IEEE International\\nConference on Computer Vision (ICCV) , December 2015.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='tion processing systems , 30, 2017.\\nTom Vercauteren, Mathias Unberath, Nicolas Padoy, and Nassir Navab. Cai4cai: the rise of contex-\\ntual artificial intelligence in computer-assisted interventions. Proceedings of the IEEE , 108(1):\\n198–214, 2019.\\nLimin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool.\\nTemporal segment networks: Towards good practices for deep action recognition. In European\\nconference on computer vision , pp. 20–36. Springer, 2016.\\nJilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, and Weidi Xie. Retrieval-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 13}, page_content='augmented egocentric video captioning. arXiv preprint arXiv:2401.00789 , 2024.\\nZihui Xue, Yale Song, Kristen Grauman, and Lorenzo Torresani. Egocentric video task translation.\\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.\\n2310–2320, 2023.\\n14'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 14}, page_content='Technical Report\\nSijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for\\nskeleton-based action recognition. In Proceedings of the AAAI conference on artificial intelli-\\ngence , volume 32, 2018.\\nXinyu Yi, Yuxiao Zhou, Marc Habermann, Vladislav Golyanik, Shaohua Pan, Christian Theobalt,\\nand Feng Xu. Egolocate: Real-time motion capture, localization, and mapping with sparse body-\\nmounted sensors. arXiv preprint arXiv:2305.01599 , 2023.\\nRyo Yonetani, Kris M Kitani, and Yoichi Sato. Recognizing micro-actions and reactions from'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 14}, page_content='paired egocentric videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pp. 2629–2638, 2016.\\nHang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language\\nmodel for video understanding. arXiv preprint arXiv:2306.02858 , 2023a.\\nJianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao,\\nHongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual descriptions with\\ndiscrete representations. arXiv preprint arXiv:2301.06052 , 2023b.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 14}, page_content='Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei\\nLiu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE Transac-\\ntions on Pattern Analysis and Machine Intelligence , 2024.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluat-\\ning text generation with bert. arXiv preprint arXiv:1904.09675 , 2019.\\nYaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu,\\nand Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. arXiv'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 14}, page_content='preprint arXiv:2306.10900 , 2023c.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\\nZixiang Zhou, Yu Wan, and Baoyuan Wang. Avatargpt: All-in-one framework for motion under-\\nstanding, planning, generation and beyond. arXiv preprint arXiv:2311.16468 , 2023.\\n15'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='Technical Report\\nSUPPLEMENTARY\\nWe provide more implementation details (Sec. A) and qualitative results (Sec. B) in this supple-\\nmentary material. To better showcase our results, We also provide videos in our project page\\nhttps://hongfz16.github.io/projects/EgoLM .\\nEgoLM\\n𝑡−1𝑡1𝑡−1𝑡𝑡−2\\n𝑡−1𝑡1………EgoLM\\n𝑡𝑡+1𝑡−1𝑡𝑡+1𝑡−1𝑡−1𝑡……b) Auto-regressive motionInference for next step of 𝑡+1𝑡+1…a) InferenceInitializationMotion Tokens\\nFigure 8: Online Motion Tracking Inference. For the new time step of t+1with new data coming'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='in, last motion tokens are combined with the new input tokens to decode the next motion token t+1.\\nA I MPLEMENTATION DETAILS\\nA.1 A UTO-REGRESSIVE INFERENCE FOR MOTION TRACKING\\nAt inference time, motion understanding is the same as the language model inference. For motion\\ntracking, it usually requires online inference over a long period. With a language model, which is an\\nauto-regressive model, it is straight-forward to perform online motion tracking. As shown in Fig. 8,\\nfirstly, an initialization over the first tframes of data is required. When the new data frame t+ 1'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='comes in, the input conditions are updated accordingly. Then, it is not necessary to predict all the\\nmotion tokens from frame 2to frame t+ 1. We take the previously generated motion tokens from\\nframe 2to frame tas inputs and prompt the network to generate one more token for frame t+ 1.\\nA.2 E VALUATION METRICS\\nFor motion tracking, we use joint position errors and joint angle errors to evaluate the performance.\\nSpecifically, for the joint position errors, we first align ground truth skeletons and generated skele-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='tons by the head positions only by translation. Then full body, upper body and lower body joint po-\\nsition errors are calculated separately. Joint angle errors are calculated on full body and root joints.\\nFor the evaluation of motion VQ-V AE in main paper Tab. 4, we apply widely adopted metrics for\\nmotion regression, i.e., Mean Per-Joint Position Error (MPJPE) (Ionescu et al., 2013), Procrustes-\\nAligned (PA-)MPJPE (Kanazawa et al., 2018), and joint position acceleration (ACCL) error. For the\\nmotion understanding, we use standard NLP metrics, please kindly refer to corresponding papers'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='for more details.\\nB M ORE QUALITATIVE RESULTS\\nB.1 T HREE -POINTS MOTION TRACKING\\nWe show four more visual examples of three-points motion tracking in Fig. 9, Fig. 10 and Fig. 11.\\nAvatarPoser (Jiang et al., 2022) and BoDiffusion (Castillo et al., 2023) are solid baselines that per-\\nform well on easy walking cases, e.g., upper example in Fig. 10. For the workout sequence, i.e.,\\nlower example in Fig. 11, even only given three points of upper body, the distribution of lower body\\nmotion can be collapsed and generate reasonable motions that matches the ground truth. In Fig. 11,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 15}, page_content='we demonstrate the effectiveness of including egocentric videos as inputs. Without any environment\\ncontext, AvatarPoser and BoDiffusion often fail to distinguish standing and sitting down. We do not\\nassume the knowledge of the head height over the floor, meaning that the three-points positions are\\nnormalized to the local coordinates of the first frame. Therefore, it is hard for baseline methods to\\ndisambiguate certain scenarios. We propose to introduce contexts using egocentric videos, which\\ncontains rich information about the environment and how the person is interacting with it. There-\\n16'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 16}, page_content='Technical Report\\nAvatarPoserBoDiffusionOurs\\nGTEgocentricVideo0mm200mm\\nFigure 9: Qualitative Results of Three-Points Motion Tracking. Skeletons are color-coded by\\njoint position errors.\\nfore, our model can generate the most accurate motions by utilizing these information. For more\\nvisualization of three-points motion tracking, please kindly refer to our supplementary videos.\\n17'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 17}, page_content='Technical Report\\nAvatarPoser\\nBoDiffusion\\nOurs\\nGT\\nEgocentricVideo\\nAvatarPoser\\nBoDiffusion\\nOurs\\nGT\\nEgocentricVideo0mm200mm\\nFigure 10: Qualitative Results of Three-Points Motion Tracking. Skeletons are color-coded by\\njoint position errors.\\n18'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 18}, page_content='Technical Report\\nAvatarPoser\\nBoDiffusion\\nOurs\\nGT\\nEgocentricVideo\\nAvatarPoser\\nBoDiffusion\\nOurs\\nGT\\nEgocentricVideo0mm200mm\\nFigure 11: Qualitative Results of Three-Points Motion Tracking. Skeletons are color-coded by\\njoint position errors.\\n19'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 19}, page_content='Technical Report\\nEgoEgo\\nOurs\\nGT\\nEgocentricVideo\\nEgoEgo\\nOurs\\nGT\\nEgocentricVideo0mm200mm\\nFigure 12: Qualitative Results of One-Point Motion Tracking. Skeletons are color-coded by joint\\nposition errors.\\nB.2 O NE-POINT MOTION TRACKING\\nWe show four more examples of one-point motion tracking in Fig. 12 and Fig. 13. The introduction\\nof egocentric videos has two advantages. Firstly, similar to the case in three-points body tracking,\\nthe environment contexts in egocentric videos can disambiguate cases like standing and sitting. Sec-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 19}, page_content='ondly, specifically for one-point motion tracking, egocentric videos provide clues of hand positions.\\nAs shown in all four examples, when the person raises the arms in front of the body, hands would be\\nvisible in the egocentric videos, which helps the hand position tracking. Admittedly, high-level se-\\nmantic information provided by CLIP (Radford et al., 2021) encoders cannot accurately track hand\\npositions. Therefore, as shown in the lower example in Fig. 12, our method correctly generates arms'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 19}, page_content='moving in the air, but lacks accuracy. For more visual examples of one-point motion tracking, please\\nkindly refer to our supplementary video.\\n20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 20}, page_content='Technical Report\\nEgoEgo\\nOurs\\nGT\\nEgocentricVideo\\nEgoEgo\\nOurs\\nGT\\nEgocentricVideo0mm200mm\\nFigure 13: Qualitative Results of One-Point Motion Tracking. Skeletons are color-coded by joint\\nposition errors.\\n21'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 21}, page_content='Technical Report\\nSample 1Sample 2Sample 3GTEgocentricVideo\\nFigure 14: Three Random Samples of One-Point Motion Tracking with Egocentric Videos as\\nInputs. Since we use language models as our backbone, EgoLM has the ability to randomly sample\\noutputs given the same inputs. Egocentric videos provide strong clues for hand positions, leading to\\nless diversity as shown in the highlighted areas.\\nB.2.1 M ULTIPLE SAMPLES .\\nNote that EgoLM is essentially a generative model. Therefore, our model is capable of generating'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 21}, page_content='different samples with the same inputs. In Fig. 14, we show three random samplings on the same\\ninput one-point and egocentric video. When hands are not visible in the frame, i.e., the left high-\\nlighted frame, hand positions are not constrained, and therefore shows high diversity across different\\nsamples. For the other highlighted frames, hands are visible in the egocentric videos, which helps to\\ncollapse the distribution of possible positions of hands. But as discussed above, our way of encoding'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 21}, page_content='egocentric videos cannot accurately track the hand positions. Therefore, our model also shows some\\ndiversity of hand positions in these cases.\\nTo further demonstrate the diversity of our model, we also show three random samples from our\\none-point motion tracking model that does not take egocentric videos as inputs in Fig. 15. Lack of\\nany indication of the hand positions, the upper body generation is even less constrained than that of\\nthe lower body and shows high diversity across three samples.\\n22'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 22}, page_content='Technical Report\\nSample 1Sample 2Sample 3GT\\nFigure 15: Three Random Samples of One-Point Motion Tracking without Egocentric Videos\\nas Inputs. With only head poses as inputs, the generation of full body motion, especially upper\\nbody motions, is less constrained.\\n23'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content=\"TM2T: The person is sitting on a chairand leaning backward on the table while talking to her peers. The person is resting both of her arms on the table, lifts and bends her left arm as she touches the table with her left hand. The person is sitting with both legs bent and with both feet flat on the floor widely apart.MotionGPT: The person is still sitting on the chair with a hunched backwhile playing arcade and eating some chips. The person's both arms are bent forward while holding and sliding the joystick with his left hand to the left then his right hand is on top of the buttons and clicks\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content=\"then his right hand is on top of the buttons and clicks them with his right fingers. The person's both legs are still bent while sitting on the chair with both feet flat on the floor and slightly apart.V2T: The human is sittingon the sofa and leaning forward while arranging the chess pieces on the chessboard. The person has both of her arms extended forward while picking up the chess pieces with her left hand and puts down the chess piece with her right hand on the chess board. The human is sitting with both feet fixed on the floor and shoulder-width apart.Ours: The person is sitting in front\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='shoulder-width apart.Ours: The person is sitting in front of the checkerboard. The person is extending his right arm toward the checkerboard while keeping his left arm on top of his leg. The human is bending both of his knees while keeping both of his feet flat on the floor.GT:Thehumanis sitting in front of the table as he plays chess. The person is moving the knight with his right hand while his left hand remains resting on his leg. The human is bending both of his knees while keeping both of his feet flat on the floor.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='TM2T: The person still marches in place while facing his peers. The person still swings both of his hands up and down. The person still marches in place with his left foot and right foot alternately. The person still repeatedly bends both of his legs alternately. The person still marches in place with his left foot and right foot alternately.MotionGPT: The human swings his body to the right and swings back to the left while standing, hunching his back and doing some exercise in the living area with his colleagues. The human slightly swings both of his arms back and forth on his side. The'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='swings both of his arms back and forth on his side. The human raises his right leg to his waist level then stretches and lowers it while his left foot is fixed on the floor.V2T: The human is standing in the living room while watching the television. The person is resting both arms on his sides. The human has both feet fixed on the floor.Ours: The person is swaying her body side to side while exercising in the living area. The person repeatedly swings and bends both of her arms in front of her then lowers it down on her side. The person repeatedly raises both of her feet in front of her then'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='repeatedly raises both of her feet in front of her then lowers them down on the floor alternately.GT: The person is walking in place in front of the laptop. The human repeatedly bends both of her arm in front of her them lowers them down on her side. The human repeatedly steps both of her feet alternately.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content=\"TM2T: The person is still standing straightin front of the table while playing the board game with his peer. The person's both arms are still bentforward while both hands are still holding the edge of the knife.MotionGPT: The human still standsnear the closet. the human still holds the hanger with his left hand and his right hand holding the hanger. The person still stands with his feet slightly apart.V2T: The person is standing straight in the living areawith his colleagues while doing some exercise. The person raises both of his arms straight above his head from the back then lowers them in\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='straight above his head from the back then lowers them in front and rests them on his side. The person is standing with both feet apart and fixed on the floor.Ours: The person is standing in the living area. The human repeatedly swings both of his arms in front of himand in front of his stomach. The person is standing with both feet fixed on the floor.GT:Thehumanisstandingstillinfrontofhiscolleagues in the living room while playing charades. The person is slightly raising and lowering both of his arms to gesticulate. The human is resting both of his legs fixedtothefloor.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='TM2T: The person walks towards the cabinet then bends forward to pick up and reach for the clothes. The person extends his right arm to pick up the clothes from the cabinetthen bends his left arm to hold the clothes.MotionGPT: The person bends forwardwhile standing in the living room. The person extends her right arm to open the cabinet and extends her left arm to grab the keys on the right. The person slightly bends both of her legs then steps her right foot forward while her left foot is fixed on the floor.V2T: The human walks towards the couch and bends down while putting down the piece of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='the couch and bends down while putting down the piece of clothing. The person extends both of her arms to pick up and put downthe piece of clothing with her right hand while holding the clothes with her left hand. The human steps both of her feet forward alternately.Ours:Thehumanwalkstowardsthesofathenslightlyleans forward to put down the folded piece of clothing. The person extends her right armto put down the folded piece of clothing on the sofa, then extends her left arm to pick upanother piece of clothing on the sofa. The human is stepping both of her feet forward alternately then bends'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='is stepping both of her feet forward alternately then bends both of her legs to support her body.GT:Thepersonbendshisbodytogetanotherclothesonthesofa.Thepersonextendshisrightarmtogettheclotheswithhis right hand then raises his left arm to hold the clothes with his left hand. The person steps both feet forward towards the sofa.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='Figure 16: Qualitative Results of Motion Understanding. We use green to highlight correct parts\\nin the answers while red for wrong ones.\\nB.3 M OTION UNDERSTANDING\\nWe show eight more examples of motion understanding in Fig. 16 and Fig. 17. Similar to the\\nmain paper, we use green to highlight correct parts in the answers and red for wrong answers.\\nSimilar to the observation made in the main paper, even though TM2T (Guo et al., 2022b) and\\nMotionGPT (Jiang et al., 2024) have access to the full body motion, the generated narrations are'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 23}, page_content='reasonable but completely wrong if consider the environment context. For example, in the upper\\nright example in Fig. 17, given the simple walking sequence, both TM2T and MotionGPT can\\ncorrectly understanding that the person is walking forward. But they all give the wrong answers\\nabout the places the person is walking in. Thanks to the egocentric videos, our model successfully\\nproduces the correct description as “walking towards the beds”.\\n24'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='Technical Report'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='TM2T: The person walksout of the bedroom then turns to the leftto enter another bedroom. The person rests both her arms on her sides. the person is stepping forward with her right and left legs alternately. The person is stepping forward with her right and left legs alternately. The person then steps forward with her right feet.MotionGPT: The person walksin the garage. the person sways his hands on the sides. The human extends both legs forward alternately.V2T:  The human walks towards the bedroom. The human slightly sways her hands on her sides. The human takes four steps towards the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='hands on her sides. The human takes four steps towards the bedroom.Ours: The human walks towards the door. The human puts down her right arm and sways both hands on the side. The person extends both legs forward alternately.GT:Thepersonwalkstowardsthedoor.The person walks towards the door. The person rests his left arm on the sideand he raises his right arm while holding the hanger with his right hand. The human extends both legs forward alternately.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content=\"TM2T: The person walkstowards the doorthen leans forward as he tucks in the chairand stands in front of the door to open it. The person's right arm is swinging back and forth on his side while his left arm is bent and his left hand holding the top railings then pushes the door open with his left hand.MotionGPT: The person is walkingforward towards the shower room, pauses on the shower room and then leans forward to put down the towelon the shower curtain holder. The person is bending both of his arms and then extends his left arm forward to put down the towelon the shower curtain holder. The\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='to put down the towelon the shower curtain holder. The person is alternately stepping both of his feet forward.V2T: The person straightens up as she slightly turns to the left while walking towards the closet. The person keeps holding the clothes with her bent left arm as she lowers down and slightly raises her right arm and then she bends it back. The person steps both of her feet forward alternately.Ours: The human turns clockwise as she walks towards the closet to put the clothes on the top shelf in the bedroom. The human is holding the clothes hanger with both of her bent arms in front of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='the clothes hanger with both of her bent arms in front of her then she extends her left arm froward and grabs the clothes hangerwith her left hand. The human turns her right foot to the right, steps her left foot forward then slightly moves her right foot forward.GT:Thehumanwalkstowards the closet. The human raises his left arm to grab the clothes while he holds the hanger with his right hand. The person extends both legs forward alternately.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='TM2T: The person is standingin front of the door. the person is raising his left arm and is resting his right arm on his side. The person bends both of his legs while resting on the floor.MotionGPT: The person standsin the bedroomwhile talking to her colleague. The human is resting and bending her left arm in front while she lowers down her right hand before touching the wall with her right hand. The person stands with both feet fixed on the floor.V2T: The human is standingstraight while picking a condiment jar in the hanging cabinet. The human grabs a condiment jar with her right hand and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='The human grabs a condiment jar with her right hand and flips up the other condiment jar in front of her with right hand and then she bends and slightly lowers down her right arm. The person is standing with both feet fixed on the ground.Ours: The person is standing in front of the hanging cabinet and slightly leaning forward while picking up a condiment jar. The person is extending her right hand forward, picks up the condiment jar cover then puts it down again on the top of the hanging cabinet while resting her left arm on her side. The human is standing with both of her legs parallel to'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='The human is standing with both of her legs parallel to each other and both of her feet spread slightly apart.GT:Thepersonisstanding on tiptoes while checking inside the cupboard. The human grabs and places the bottle down on the countertop with her right hand while her left hand is resting on the countertop. The human is standing on tiptoes with both feet as she reaches inside the cupboard.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='TM2T: The person is walkingforward in the pathwaythen she slightly leans forward as she sitson the pathway. The person alternately swings both hands on her sides while both arms hang naturally at her sides.MotionGPT: The human is walkingforward while looking at the office surrounding. The human has her both arms swaying them back and forth. The human extends both legs forward alternately.V2T: The person is walking forward towards the bed. the person rests both arms on her sides. The person is extending both her legs forward alternately.Ours:The human is walking towards the bed. The person is'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='human is walking towards the bed. The person is resting both of her arms beside her. The person is extending both of her legs forward alternately.GT:Thepersonwalkstowardsthebed.Thepersonslightlyswingsbothofherarmsbackandforth.Thepersonstepsbothofher legs forward alternately.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 24}, page_content='Figure 17: Qualitative Results of Motion Understanding. We use green to highlight correct parts\\nin the answers while red for wrong ones.\\n25'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 25}, page_content='Technical Report\\nMotion Prompt #1Temperature 0.6Motion Prompt #1Temperature 1.0Motion Prompt #1Temperature 1.4Motion Prompt #2Temperature 1.0Motion Prompt #3Temperature 1.0Motion Prompt #4Temperature 1.0\\nFigure 18: Qualitative Results of Motion Prediction. The first skeletons in red are input motion\\nprompts. The following motions are randomly sampled auto-regressively from our motion pre-\\ntraining network.\\nB.4 M OTION PREDICTION\\nAs a by-product of the second stage of our training pipeline, motion pre-training, we build a motion'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18127.pdf', 'page': 25}, page_content='prediction network. Given leading motions as the prompts, our model is capable of auto-regressively\\nsample motions that complete the motion prompts. As shown in Fig. 18, the first three samples\\nshow three different samples given the same motion prompt. We can increase the intensity of the\\ngenerated motions by increasing the temperature. The last three samples show three random samples\\ngiven various motion prompts, e.g., bending forward, sitting down and standing.\\n26'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='FlowTurbo: Towards Real-time Flow-Based Image\\nGeneration with Velocity Refiner\\nWenliang Zhao∗Minglei Shi∗Xumin Yu Jie Zhou Jiwen Lu†\\nTsinghua University\\nAbstract\\nBuilding on the success of diffusion models in visual generation, flow-based models\\nreemerge as another prominent family of generative models that have achieved\\ncompetitive or better performance in terms of both visual quality and inference\\nspeed. By learning the velocity field through flow-matching, flow-based models\\ntend to produce a straighter sampling trajectory, which is advantageous during'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='the sampling process. However, unlike diffusion models for which fast samplers\\nare well-developed, efficient sampling of flow-based generative models has been\\nrarely explored. In this paper, we propose a framework called FlowTurbo to\\naccelerate the sampling of flow-based models while still enhancing the sampling\\nquality. Our primary observation is that the velocity predictor’s outputs in the\\nflow-based models will become stable during the sampling, enabling the estimation\\nof velocity via a lightweight velocity refiner. Additionally, we introduce several'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='techniques including a pseudo corrector and sample-aware compilation to further\\nreduce inference time. Since FlowTurbo does not change the multi-step sampling\\nparadigm, it can be effectively applied for various tasks such as image editing,\\ninpainting, etc. By integrating FlowTurbo into different flow-based models, we\\nobtain an acceleration ratio of 53.1% ∼58.3% on class-conditional generation and\\n29.8%∼38.5% on text-to-image generation. Notably, FlowTurbo reaches an FID\\nof 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img),'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='achieving the real-time image generation and establishing the new state-of-the-art.\\nCode is available at https://github.com/shiml20/FlowTurbo .\\n1 Introduction\\nIn recent years, diffusion models have emerged as powerful generative models, drawing considerable\\ninterest and demonstrating remarkable performance across various domains[ 10,38,30,12]. Diffusion\\nmodels utilize a denoising network, ϵθ, to learn the reverse of a diffusion process that gradually\\nadds noise to transform the data distribution into a Gaussian distribution. While the formulation of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='diffusion models enables stable training and flexible condition injection[ 30], sampling from these\\nmodels requires iterative denoising. This process necessitates multiple evaluations of the denoising\\nnetwork, thereby increasing computational costs. To address this, several techniques such as fast\\ndiffusion samplers[ 22,18,42] and efficient distillation[ 31,37] have been proposed to reduce the\\nsampling steps of diffusion models.\\nAlongside the research on diffusion models, flow-based models[ 5,19,17] have garnered increasing'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 0}, page_content='attention due to their versatility in modeling data distributions. Flow is defined as a probability path\\nthat connects two distributions and can be efficiently modeled by learning a neural network to estimate\\nthe conditional velocity field through a neural network vθvia flow matching [ 17]. Encompassing\\n∗Equal contribution.†Corresponding author.\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2409.18128v1  [cs.CV]  26 Sep 2024'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='4 8 12 16 20\\nSampling Steps0.000.180.350.530.70Normalized Value of CurvatureSiT\\nDiT\\nSD3-Medium\\nFLUX.1-dev\\nOpen-Sora(a) Comparison of curvatures of different models.\\n4 8 12 16 200.000.010.020.03\\nSiT(b) SiT\\n4 8 12 16 200.000.010.020.03\\nSD3-Medium (c) SD3-Medium\\n4 8 12 16 200.000.010.020.03\\nFLUX.1-dev\\n(d) FLUX.1-dev\\n4 8 12 16 200.000.010.020.03\\nOpen-Sora (e) Open-Sora\\nFigure 1: Visualization of the curvatures of the sampling trajectories of different models. We\\ncompare the curvatures of the model predictions of a standard diffusion model (DiT [ 28]) and several'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='flow-based models (SiT [ 24], SD3-Medium [ 8], FLUX.1-dev [ 14], and Open-Sora [ 43]) during the\\nsampling. We observe that the vθin flow-based models is much more stable than ϵof diffusion\\nmodels during the sampling, which motivates us to seek a more lightweight estimation model to\\nreduce the sampling costs of flow-based generative models.\\nthe standard diffusion process as a special case, flow-based generative models support more flexible\\nchoices of probability paths. Recent work has favored a simple linear interpolant path[ 20,24,8],'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='which corresponds to the optimal transport from the Gaussian distribution to the data distribution.\\nThis linear connection between data and noise results in a more efficient sampling process for flow-\\nbased models. However, unlike diffusion models, which benefit from numerous efficient sampling\\nmethods, current samplers for flow-based models primarily rely on traditional numerical methods\\nsuch as Euler’s method and Heun’s method [ 24]. These traditional methods, while functional, fail to\\nfully exploit the unique properties of flow-based generative models, thereby limiting the potential for'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='faster and more efficient sampling.\\nIn this paper, we propose FlowTurbo, a framework designed to accelerate the generation process\\nof flow-based generative models. FlowTurbo is motivated by comparing the training objectives of\\ndiffusion and flow-based generative models, as well as analyzing how the prediction results ϵθand\\nvθvary over time. Our observation, illustrated in Figure 1, indicates that the velocity predictions\\nof a flow-based model remain relatively stable during sampling, in contrast to the more variable'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='predictions of ϵθin diffusion models. This stability allows us to regress the offset of the velocity at\\neach sampling step using a lightweight velocity refiner, which contains only 5%of the parameters\\nof the original velocity prediction model. During the sampling process, we can replace the original\\nvelocity prediction model with our lightweight refiner at specific steps to reduce computational costs.\\nAs a step towards real-time image generation, we propose two useful techniques called pseudo\\ncorrector and sample-aware compilation to further improve the sampling speed. Specifically, the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='pseudo corrector method modifies the updating rule in Heun’s method by reusing the velocity\\nprediction of the previous sampling step, which will reduce the number of model evaluations at each\\nstep by half while keeping the original convergence order. The sample-aware compilation integrates\\nthe model evaluations, the sampling steps as well as the classifier-free guidance [ 11] together and\\ncompile them into a static graph, which can bring extra speedup compared with standard model-level\\ncompilation. Since each sample block is independent, we can still adjust the number of inference'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='steps and sampling configurations flexibly.\\nOur FlowTurbo framework is fundamentally different from previous one-step distillation methods\\nfor diffusion models [ 20,40,32], which require generating millions of noise-image pairs offline and\\nconducting distillation over hundreds of GPU days. In contrast, FlowTurbo’s velocity refiner can be\\nefficiently trained on pure images in less than 6 hours. Moreover, one-step distillation-based methods\\nare limited to image generation and disable most of the functionalities of the original base model.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 1}, page_content='Conversely, FlowTurbo preserves the multi-step sampling paradigm, allowing it to be effectively\\napplied to various tasks such as image editing, inpainting, and more.\\nWe perform extensive experiments to evaluate our method. By applying FlowTurbo to different\\nflow-based models, we obtain an acceleration ratio of 53.1% ∼58.3% on class-conditional generation\\n2'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='and 29.8% ∼38.5% on text-to-image generation. Notably, FlowTurbo attains an FID score of 2.12\\non ImageNet with 100 (ms / img) and FID of score 3.93 with 38 (ms / img), thereby enabling real-\\ntime image generation and establishes the new state-of-the-art. Additionally, we present qualitative\\ncomparisons demonstrating how FlowTurbo generates superior images with higher throughput and\\nhow it can be seamlessly integrated into various applications such as image editing, inpainting, etc.\\nWe believe our FlowTurbo can serve as a general framework to accelerate flow-based generative'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='models and will see wider use as these models continue to grow [24, 20, 8, 9].\\n2 Related Work\\nDiffusion and flow-based models. Diffusion models [ 10,38] are a family of generative models that\\nhave become the de-facto method for high-quality generation. The diffusion process gradually adds\\nnoise to transform the data distribution to a normal distribution, and the goal of diffusion models is to\\nuse a network ϵθto learn the reverse of the diffusion process via score-matching [ 10,38]. Rombach et\\nal. [30] first scales up diffusion models to large-scale text-to-image generation by performing the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='diffusion on latent space and adopting cross-attention to inject conditions. The pre-trained diffusion\\nmodels can also be easily fine-tuned to achieve generation with more diverse conditions [ 41,27] and\\nhave attracted increasing attention in the community. Flow-based generative models are different from\\ndiffusion models in both data modeling and training objectives. Flow-based models [ 20,17,8,24]\\nconsider the probability path from one distribution to another, and learn the velocity field via flow'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='matching [ 17]. By choosing the linear interpolant as the probability path which corresponds to the\\noptimal transport from the normal distribution to the data distribution, the trajectory from noise\\nto data becomes more straighter which is beneficial to the sampling. Recent work [ 24,8] have\\ndemonstrates the effectiveness and scalability of flow-based generation models. However, both\\ndiffusion and flow-based models requires multiple evaluations of the prediction model, leading to\\nlower inference speed than traditional architectures like GAN. In this work, we focus on this issue'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='and aim to accelerate flow-based generative models.\\nEfficient visual generation. Accelerating the generation of diffusion models has become an increas-\\ningly important topic. Existing methods can be roughly categorized as training-free and training-based\\nmethods. Training-free methods aim to design faster samplers that can reduce the approximation\\nerror when sampling from the diffusion SDE or ODE [ 36,22,18,42], while keeping the weights of\\ndiffusion models unchanged. Training-based methods often aim to reshape the sampling trajectory by'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='distillation from the diffusion model [ 31,40] to achieve the few-step or even one-step generation.\\nThese training-based methods usually requires multiple-round of distillation [31, 20] and expensive\\ntraining resources ( e.g.,>100 GPU days in [ 20]). Besides, the distilled one-step model no longer\\nsupports image editing due to the lack of multi-step sampling. Although there are a variety of meth-\\nods for accelerating diffusion models, there are few fast sampling methods designed for flow-based\\ngenerative models. Existing flow-based models adopt traditional numerical methods like Euler’s'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='method or Heun’s method during the inference [ 24]. In this work, we provide a framework called\\nFlowTurbo to accelerate the generation of flow-based models by learning a lightweight velocity\\nrefiner (which only requires <6 GPU hours) to regress the offset of the velocity. Together with other\\nproposed techniques, FlowTurbo addresses the previously unmet need for an efficient flow-based\\ngeneration framework, paving the way for real-time generative applications.\\n3 Method\\n3.1 Preliminaries: Diffusion and Flow-based Models'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='3 Method\\n3.1 Preliminaries: Diffusion and Flow-based Models\\nDiffusion models. Recently, diffusion models [ 10,38,35,30] have emerged as a powerful family of\\ngenerative models. The diffusion models are trained to learn the inverse of a diffusion process such\\nthat it can recover the data distribution p0(x0)from the Gaussian noise. The diffusion process can be\\nrepresented as:\\nxt=αtx0+σtϵ, t∈[0,1],ϵ∼ N(0,I), (1)\\nwhere αt, σtare the chosen noise schedule such that the marginal distribution p1(x1)∼ N(0,I).'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 2}, page_content='The optimization of diffusion models can be derived by either minimizing the ELBO of the reverse\\nprocess [ 10] or solving the reverse diffusion SDE [ 38], which would both lead to the same training\\nobjective of score-matching, i.e., to learn a noise prediction model ϵθ(xt, t)to estimate the scaled\\n3'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='score function −σt∇xlogpt(xt):\\nLDM(θ) =Et,p0(x0),p(xt|x0)h\\nλ(t)∥ϵθ(xt, t) +σt∇xlogpt(xt)∥2\\n2i\\n, (2)\\nwhere λ(t)is a time-dependent coefficient. Sampling from a diffusion model can be achieved by\\nsolving the reverse-time SDE or the corresponding diffusion ODEs [ 38], which can be efficiently\\nachieved by modern fast diffusion samplers [36, 22, 42].\\nFlow-based models. Flow-based models can be traced back to Continuous Normalizing Flows [ 5]\\n(CNF), which is a more generic modeling technique and can capture the probability paths of the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='diffusion process as well [ 17]. Training a CNF becomes more practical since the purpose of the flow\\nmatching technique [ 17], which learns the conditional velocity field of the flow. Similar to (1), we\\ncan add some constraints to the noise schedule such that α0= 1, σ0= 0andα1= 0, σ1= 1, and\\nthen define the flow as:\\nψt(·|ϵ) :x07→αtx0+σtϵ, (3)\\nIn this case, the velocity field that generates the flow ψtcan be represented as:\\nut(ψt(x0|ϵ)|ϵ) =d\\ndtψt(x0|ϵ) = ˙αtx0+ ˙σtϵ. (4)\\nThe training objective of conditional flow matching is to train a velocity prediction model vθto'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='estimate the conditional velocity field:\\nLFM(θ) =Et,p1(ϵ),p0(x0)\\r\\r\\r\\rvθ(ψt(x0|ϵ), t)−d\\ndtψt(x0|ϵ)\\r\\r\\r\\r2\\n2(5)\\nThe sampling of a flow-based model can be achieved by solving the probability flow ODE with the\\nlearned velocity\\ndxt\\ndt=vθ(xt, t),x1∼p1(x1). (6)\\nSince the formulation of the flow ψtcan be viewed as the interpolation between x0andv, it is also\\nreferred to as interpolant in some literature [ 1,24]. Among various types of interpolants, a very\\nsimple choice is linear interpolant [ 24,8], where αt= (1−t)andσt=t. In this case, the velocity'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='field becomes a straight line connecting the initial noise and the data point, which also corresponds to\\nthe optimal transport between the two distributions [ 19,17]. The effectiveness and scalability of the\\nlinear interpolant have also been proven in recent work [17, 24, 8].\\n3.2 Efficient Estimation of Velocity\\nWe consider the velocity estimation in flow-based generative models with the linear interpolant [ 24,\\n8,20]. As shown in (5), the training target of the velocity prediction model vθis exactly ϵ−x0, a'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='constant value independent of t. Our main motivation is to efficiently estimate the velocity during the\\nsampling, instead of evaluating the whole velocity prediction model vθevery time.\\nAnalyzing the stability of velocity. We start by analyzing the stability of the output value of vθ\\nalong the sampling trajectory. By comparing the training objectives of diffusion and flow-based\\nmodels (2)(5) , we know that the target of vθis independent of t. A more in-depth discussion is\\nprovided in Appendix A.3, where we show the two training objectives have different time-dependent'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='weight functions. To verify whether there are similar patterns during the sampling, we compare how\\nthe prediction results change across the sampling steps in Figure 1. Specifically, we compare the\\ncurvatures of ϵθof a diffusion model (DiT [ 28]) and the vθof flow-based models (SiT [ 8], SD3 [ 8],\\netc) during the sampling steps. For each model, we sample from 8 random noises and set the total\\nsampling steps as 20. It can be clearly observed that vθof a flow-based model is much more stable\\nthan the ϵθof a diffusion model. Therefore, We define the vθas a“stable value” . The stability of vθ'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 3}, page_content='makes it possible to obtain the velocity more efficiently rather than performing the forward pass of\\nthe whole velocity prediction network vθat every sampling step.\\nLearning a lightweight velocity refiner. Since the velocity in a flow-based model is a “stable value”,\\nwe propose to learn a lightweight refiner that can adjust the velocity with minimal computational\\ncosts. The velocity refiner takes as inputs both the current intermediate result and the velocity of the\\nprevious step, and returns the offset of velocity:\\nvti=rϕ(xti,vti−1, ti) +vti−1. (7)\\n4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='+𝐱𝑡𝑖−1=𝜓𝑡𝑖−1𝐱0𝝐=1−𝑡𝑖−1𝐱0+𝑡𝑖−1𝝐 𝐱0∼𝑝0𝐱,𝝐∼𝒩(0,𝐈)\\n𝐱𝑡𝑖−1 Velocity \\nPredictor 𝐯𝜃𝐯𝑡𝑖−1\\n×Δ𝑡𝐱𝑡𝑖Velocity \\nRefiner 𝐫𝜙𝐯𝑡𝑖−1\\u0de4𝐯𝑡𝑖+\\nℒ𝜙=𝔼\\u0de4𝐯𝑡𝑖−𝐯𝜃(𝐱𝑡𝑖,𝑡𝑖)22×Δ𝑡\\n+𝐱𝑡𝑖−1 Velocity \\nPredictor 𝐯𝜃𝐯𝑡𝑖−1\\n×Δ𝑡𝐱𝑡𝑖\\nVelocity \\nPredictor 𝐯𝜃 ത𝐱𝑡𝑖ത𝐯𝑡𝑖+×0.5+(b)Heun’s Method Sample Block (a)Learning a Lightweight V elocity Refiner\\n×Δ𝑡\\n+𝐱𝑡𝑖−1 Velocity \\nCacheത𝐯𝑡𝑖−1\\n×Δ𝑡𝐱𝑡𝑖\\nVelocity \\nPredictor 𝐯𝜃 ത𝐱𝑡𝑖ത𝐯𝑡𝑖+×0.5+\\n𝐱𝑡0∼𝒩(0,𝐈)(c)Pseudo Corrector Sample Block (d)FlowTurbo Sampling\\n𝐱𝑡0\\nSACHeun’s Method \\nSample Block\\nSACPseudo Corrector\\nSample Block\\nSACVelocity Refiner\\nSample block𝐱𝑡𝑁'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='Sample Block\\nSACVelocity Refiner\\nSample block𝐱𝑡𝑁\\nSAC: Sample -A ware Compilation×𝑁𝐻 ×𝑁𝑃 ×𝑁𝑅𝑁=𝑁𝐻+𝑁𝑃+𝑁𝑅Figure 2: Overview of FlowTurbo. (a) Motivated by the stability of the velocity predictor’s outputs during the\\nsampling, we propose to learn a lightweight velocity refiner to regress the offset of the velocity field. (b)(c) We\\npropose the pseudo corrector which leverages a velocity cache to reduce the number of model evaluations while\\nmaintaining the same convergence order as Heun’s method. (d)During sampling, we employ a combination of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='Heun’s method, the pseudo corrector, and the velocity refiner, where each sample block is processed with the\\nproposed sample-aware compilation.\\nThe velocity refiner rϕcan be designed to be very lightweight ( <5%parameters of vθ). The detailed\\narchitecture can be found in Appendix C.\\nTo learn the velocity refiner, we need to minimize the difference between the output of rϕand the\\nactual offset vti−vti−1. However, it requires multiple-step sampling to obtain an intermediate result\\nxtito make the training objective perfectly align with our target. To reduce the training cost, we'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='simulate the xtwith one-step sampling starting from xti−1, which is directly obtained by the flow\\nψti−1. The detailed procedure to compute the loss is listed as follows:\\nxti−1←ψti−1(x0|ϵ),x0∼p0(x),ϵ∼p1(x) (8)\\nvti−1←vθ(xti−1, ti−1),xti←Solver( xti−1,vti−1,∆t) (9)\\nLϕ←E∥vθ(xti, ti)−(rϕ(xti,vti−1, ti) +vti−1)∥2\\n2 (10)\\nWhere ∆t=ti−ti−1and we use a simple Euler step for the Solver to obtain xti. Once the velocity\\nrefiner is learned, we can use it to replace the original vθat some specific sampling steps. We will'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='demonstrate through experiments that adding the velocity refiner can improve the sampling quality\\nwithout introducing noticeable computational overhead.\\nCompatibility with classifier-free guidance. Classifier-free guidance [ 11] is a useful technique\\nto improve the sampling quality in conditional sampling. Let ybe the condition, the classifier-free\\nguidance for a velocity prediction model [8] can be defined as:\\nvζ(x, t|y) = (1 −ζ)vθ(x, t|∅) +ζvθ(x, t|y), (11)\\nwhere ζis the guidance scale and ∅denotes the null condition. To make our velocity refiner support'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='classifier-free guidance, we only need to make sure both the conditional prediction vθ(x, t|y)and the\\nunconditional prediction vθ(x, t|∅)appear during the training. Note that we always feed the velocity\\nprediction model vθand the velocity refiner rϕwith the same condition.\\nyγ=Iγ≤γ1·∅+Iγ>γ 1·y, γ∈ U[0,1], (12)\\nLCFG\\nϕ←E∥vθ(xti, ti|yγ)−(rϕ(xti,vti−1, ti|yγ) +vti−1)∥2\\n2, (13)\\nwhere we set the γ1= 0.1as the probability of using an unconditional velocity.\\n3.3 Towards Real-Time Image Generation\\nThe sampling costs of a flow-based model can be significantly minimized by integrating our'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 4}, page_content='lightweight velocity refiner rϕin place of the velocity prediction network vθat selected sampling\\n5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='steps. In this section, we propose two techniques to further improve the sampling speed towards\\nreal-time image generation.\\nPseudo corrector. Traditional numerical ODE solvers are usually used to sample from a probability\\nflow ODE. For example, SiT [ 8] adopt a Heun method (or improved Euler’s method) [ 15] as the ODE\\nsolver. The update rule from ti−1totican be written as:\\ndi−1←vθ(xti−1, ti−1|y), ˜xti←xti−1+ ∆tdi−1 (14)\\ndi←vθ(˜xti, ti|y), xi←xi−1+∆t\\n2[di−1+di] (15)\\nEach Heun step contains a predictor step (14) and a corrector step (15), thus includes two evaluations'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='of the velocity predictor vθ, bringing extra inference costs. Motivated by [ 42], we propose to reuse\\nthediin the next sampling step, instead of re-computing it via di←vθ(xti, ti|y)(see Figure 2\\n(b)(c) for illustration). We call this a pseudo corrector since it is different from the predictor-corrector\\nsolvers in numerical analysis. It can be proved (see Appendix B) that the pseudo corrector also enjoys\\n2-order convergence while only having one model evaluation at each step.\\nSample-aware compilation. Compiling the network into a static graph is a widely used technique'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='for model acceleration. However, all the previous work only considers network-level compilation,\\ni.e., only compiling the ϵθorvθ. We propose the sample-aware compilation which wraps both the\\nforward pass of vθorrϕand the sampling operation together (including the classifier-free guidance)\\nand performs the compilation. For example, the sample blocks illustrated in Figure 2 (b, c) are\\ncompiled into static graphs. Since each sample block is independent, we can still adjust the number\\nof inference steps and sampling configurations flexibly.\\n3.4 Discussion'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='3.4 Discussion\\nRecently, there have been more and more training-based methods [ 20,40,32] aiming to accelerate\\ndiffusion models or flow-based models through one-step distillation. Although these methods can\\nachieve faster inference, they usually require generating paired data using the pre-trained model and\\nsuffer from large training costs ( e.g., >100 GPU days in [ 40,20]). Besides, one-step methods only\\nkeep the generation ability of the original model while disabling more diverse applications such as'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='image inpainting and image editing. In contrast, our FlowTurbo aims to accelerate flow-based models\\nthrough velocity refinement, which still works in a multi-step manner and performs sampling on the\\noriginal trajectory. For example, FlowTurbo can be easily combined with existing diffusion-based\\nimage editing methods like SDEdit [25] (see Section 4.4).\\n4 Experiments\\nWe conduct extensive experiments to verify the effectiveness of FlowTurbo. Specifically, we ap-\\nply FlowTurbo to both class-conditional image generation and text-to-image generation tasks and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='demonstrate that FlowTurbo can significantly reduce the sampling costs of the flow-based generative\\nmodels. We also provide a detailed analysis of each component of FlowTurbo, as well as qualitative\\ncomparisons of different tasks.\\n4.1 Setups\\nIn our experiments, we consider two widely used benchmarks including class-conditional image gen-\\neration and text-to-image generation. For class-conditional image generation, we adopt a transformer-\\nstyle flow-based model SiT-XL [ 24] pre-trained on ImageNet 256 ×256. For text-to-image generation,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='we utilize InstaFlow [ 20] as the flow-based model, whose backbone is a U-Net similar to Stable-\\nDiffusion [ 30]. Note that we use the 2-RF model from [ 19] instead of the distilled version since\\nour FlowTurbo is designed to achieve acceleration within the multi-step sampling framework. The\\nvelocity refiner only contains 4.3% and 5% parameters of the corresponding predictor, and the detailed\\narchitecture can be found in Appendix C. During training, we randomly sample ∆t∈(0,0.12]and\\ncompute the training objectives in (13). In both tasks, we use a single NVIDIA A800 GPU to train'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 5}, page_content='the velocity refiner and find it converges within 6 hours. We use a batch size of 8 on a single A800\\nGPU to measure the latency of each method. Please refer to Appendix C for more details.\\n6'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='Table 1: Main results. We apply our FlowTurbo on SiT-XL [ 24] and the 2-RF of InstaFlow [ 20] to perform\\nclass-conditional image generation and text-to-image generation, respectively. The image quality is measured by\\nthe FID 50K ↓on ImageNet (256 ×256) and the FID 5K ↓on MS COCO 2017 (512 ×512). We use the suffix to\\nrepresent the number of Heun’s method block ( H), pseudo corrector block ( P), and the velocity refiner block\\n(R). Our results demonstrate that FlowTurbo can significantly accelerate the inference of flow-based models\\nwhile achieving better sampling quality.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='while achieving better sampling quality.\\n(a) Class-conditional Image Generation\\nMethodSample FLOPsFID↓Latency\\nConfig (G) (ms / img)\\nSiT-XL [8], ImageNet (256×256)\\nHeun’s H8 1898 3.68 89.4\\nFlowTurbo H2P4R2 957 3.63 41.6 (-53.4%)\\nHeun’s H11 2610 2.79 117.8\\nFlowTurbo H2P8R2 1431 2.69 55.2 (-53.1%)\\nHeun’s H15 3559 2.42 154.8\\nFlowTurbo H5P7R3 2274 2.22 72.5 (-53.2%)\\nHeun’s H24 5694 2.20 240.6\\nFlowTurbo H8P9R5 3457 2.12 100.3 (-58.3%)(b) Text-to-image Generation\\nMethodSample FLOPsFID↓Latency\\nConfig (G) (ms / img)\\nInstaFlow [20], MS COCO 2017 (512×512)\\nHeun’s H4 3955 32.77 104.5'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='Heun’s H4 3955 32.77 104.5\\nFlowTurbo H1P2R2 2649 32.48 68.4 (-34.5%)\\nHeun’s H5 4633 30.73 120.3\\nFlowTurbo H1P4R2 3327 30.19 84.5 (-29.8%)\\nHeun’s H8 6667 28.61 170.5\\nFlowTurbo H1P6R3 4030 28.60 104.8 (-38.5%)\\nHeun’s H10 8023 28.06 203.7\\nFlowTurbo H3P6R3 5386 27.60 137.0 (-32.7%)\\nTable 2: Comparisons with the state-of-the-arts . We compare the sampling quality and speed of different\\nmethods on ImageNet 256×256class-conditional sampling. We demonstrate that FlowTurbo can significantly'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='improve over the baseline SiT-XL [ 24] and achieves the fastest sampling (38 ms / img) and the best quality (2.12\\nFID) with different configurations.\\nModel Sample Config ParamsLatencyFID↓ IS↑ Precision ↑Recall↑(ms / img)\\nStyleGAN-XL [33] - 166M 190 2.30 265.1 0.78 0.53\\nMask-GIT [2] 8 steps 227M 120 6.18 182.1 0.80 0.51\\nADM [7] 250 steps DDIM [36] 554M 2553 10.94 101.0 0.69 0.63\\nADM-G [7] 250 steps DDIM [36] 608M 4764 4.59 186.7 0.83 0.53\\nLDM-4-G [30] 250 steps DDIM [36] 400M 448 3.60 247.7 0.87 0.48\\nDiT-XL [28] 250 steps DDPM [10] 675M 6914 2.27 278.2 0.83 0.57'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='SiT-XL [24] 25 steps dopri5 [15] 675M 3225 2.15 258.1 0.81 0.60\\nSiT-XL [24] 25 steps Heun’s [15] 675M 250 2.20 254.9 0.81 0.60\\nFlowTurbo (ours) H1P5R3 704M 38 3.93 223.6 0.79 0.56\\nFlowTurbo (ours) H5P7R3 704M 73 2.22 248.0 0.81 0.60\\nFlowTurbo (ours) H8P9R5 704M 100 2.12 255.6 0.81 0.60\\n4.2 Main Results\\nClass-conditional image generation. We adopt the SiT-XL [ 24] trained on ImageNet [ 6] of resolution\\nof256×256. Following common practice [ 24,30], we adopt a classifier-free guidance scale (CFG)\\nof 1.5. According to [ 24], a widely used sampling method of the flow-based model is Heun’s'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='method [ 15]. In Table 1a, we demonstrate how our FlowTurbo can achieve faster inference than\\nHeun’s method in various computational budgets. Specifically, we conduct experiments with different\\nsampling configurations (the second column of Table 1a), where we use the suffix to represent the\\nnumber of Heun’s method block ( H), pseudo corrector block ( P), and the velocity refiner block ( R).\\nNote that each Heun’s block contains two evaluations of the velocity predictor while each pseudo\\ncorrector block only contains one. We also provide the total FLOPs during the sampling and the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='inference speed of each sample configuration. In each group of comparison, we choose the sampling\\nstrategy of FlowTurbo to make the sampling quality (measured by the FID 50K ↓) similar to the\\nbaseline. Our results demonstrate that FlowTurbo can accelerate the inference by 37.2%∼43.1%,\\nwhile still achieving better sampling quality. Notably, FlowTurbo obtains 3.63 FID with a sampling\\nspeed of 41.6 ms/img, achieving real-time image generation.\\nText-to-image generation. We adopt the 2-RF model in [ 20] as our base model for text-to-image'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 6}, page_content='generation. Note that we do not adopt the distilled version in [ 20] since we focus on accelerating\\nflow-based models within the multi-step sampling paradigm. Following [ 20,26], we compute\\nthe FID 5K ↓between the generated 512×512samples and the images on MS COCO 2017 [ 16]\\n7'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='Table 3: Ablation studies. We evaluate the effectiveness of each component in FlowTurbo as well as the\\nselection of some hyper-parameters. (a)We gradually add the components of FlowTurbo to the baseline and\\nshow that FlowTurbo can achieve over 50% acceleration with better sampling quality. (b)we experiment with\\ndifferent ranges of ∆tand find ∆t∈(0.0,0.12]yields relatively good results in all the situations. (c)(d) we\\nshow how the sampling quality/speed changes with the number of velocity refiner and pseudo corrector blocks.\\n(a) Ablation of components of FlowTurbo.\\nSampleNH NP HR FID↓Latency'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='SampleNH NP HR FID↓Latency\\nConfig (ms / img)\\nbaselines7 0 0 4.42 80.0\\n8 0 0 3.68 89.4 (+11.8%)\\nA 7 0 1 2.80 80.5 (+0.7%)\\nB 2 8 2 2.69 71.6 (-10.4%)\\nC 3 3 2 3.25 57.4 (-28.2%)\\nD 2 4 2 3.63 52.7 (-34.1%)\\nE 1 5 3 3.93 48.0 (-40.0%)\\nE + Model-Level Comp. 3.93 41.0 (-48.7%)\\nE + Sample-Aware Comp. 3.93 38.3 (-52.2%)\\n(b) Ablation of ∆t.\\nSample\\nConfig∆t\\\\ FID↓\\n(0.0,0.1] (0 .0,0.12] (0 .0,0.2] [0 .06,0.12]\\nH6R2 3.58 3.55 4.48 3.36\\nH9R3 2.73 2.65 2.93 2.62\\nH12R5 2.53 2.54 2.64 2.89(c) Effects of the velocity refiner.\\nSampleFID↓Latency\\nConfig (ms / img)\\nH8 3.68 68.0\\nH7R1 2.80 61.6 (-9.4%)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='Config (ms / img)\\nH8 3.68 68.0\\nH7R1 2.80 61.6 (-9.4%)\\nH6R2 3.55 55.2 (-18.8%)\\nH5R3 7.62 49.9 (-26.5%)\\n(d) Effects of the pseudo corrector.\\nSampleFID↓Latency\\nConfig (ms / img)\\nH8 3.68 68.0\\nH7R2 2.93 62.0 (-8.8%)\\nH6P1R2 2.60 58.6 (-13.8%)\\nH5P2R2 2.66 55.2 (-18.8%)\\nH4P3R2 2.78 51.8 (-23.8%)\\nH3P4R2 2.96 48.4 (-28.8%)\\nH2P5R2 3.21 45.0 (-33.7%)\\nH1P6R2 3.59 41.6 (-38.7%)\\nvalidation set. The results are summarized in Table 1b, where we compare the sampling speed/quality\\nwith the baseline Heun’s method. Note that the notation of the sampling configuration is the same'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='as Table 1a. The results clearly demonstrate that Our FlowTurbo can also achieve significant\\nacceleration (29.8% ∼38.5%) on text-to-image generation.\\n4.3 Comparisons to State-of-the-Arts\\n40100 1000 10000\\nLatency (ms / img)123456789FID 50K\\nMask-GIT\\nADM-G\\nLDM-4-G\\nDiT-XL StyleGAN-XLSiT\\nFlowTurbo\\nFigure 3: FlowTurbo exhibits favorable\\ntrade-offs compared with SOTA methods.In Table 2, we compare our FlowTurbo with state-of-the-\\nart methods on ImageNet 256×256class-conditional\\ngeneration. We use SiT-XL [ 24] as our base model and\\napply FlowTurbo with different sampling configurations'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='apply FlowTurbo with different sampling configurations\\non it. We show that FlowTurbo with H1P5R3achieves\\nthe sampling speed of 38 (ms / img) with 3.93 FID (still\\nbetter than most methods like Mask-GIT [ 2], ADM [ 7]).\\nOn the other hand, FlowTurbo with H8P9R5archives the\\nlowest FID 2.12, outperforming all the other methods.\\nBesides, we also provide a comparison of the sampling\\nspeed/quality trade-offs of SiT (by changing the number\\nof sampling steps of Heun’s method) and FlowTurbo (by\\nchanging the sampling configurations) in 3, where the\\nresults of some other state-of-the-arts methods are also'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='results of some other state-of-the-arts methods are also\\nincluded. The comparison shows our FlowTurbo exhibits\\nfavorable sampling quality/speed trade-offs.\\n4.4 Analysis\\nAblation of components of FlowTurbo. We evaluate the effectiveness of each component of\\nFlowTurbo in Table 3a. Specifically, we start from the baseline, a 7-step Heun’s method and gradually\\nadd components of FlowTurbo. In the sample config A, we show that adding a velocity refiner\\ncan significantly improve the FID ↓(4.42→2.80), while introducing minimal computational costs'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 7}, page_content='(only +0.7%in the latency). From B to E, we adjust the ratios of Heun’s method block, the pseudo\\ncorrector block, and the velocity refiner block to achieve different trade-offs between sampling speed\\n8'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='A moving train against the background of a blue sky and epic clouds\\n A pink rectangular potion with intricate gold adornment\\nAn alien ship crashed with a sad alien sitting on the desert ground\\n A multicolored iridescent horse with unicorn horn and dragon wings(a) Results of Heun’s (2.6 s / img, left) and FlowTurbo (1.8 s / img ,right )\\nObject Removal\\nImage Editing\\nImage Inpainting (b) Extensions\\nFigure 4: Qualitative results. (a) We compared our FlowTurbo with Heun’s method on Lumina-Next-T2I [ 9].'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='With better image quality, our method requires much less sampling time ( −30.8%).(b)Since FlowTurbo\\nremains the multi-step sampling paradigm, it can be seamlessly applied to more applications such as image\\ninpainting, image editing, and object removal.\\nand quality. In the last two rows, we show that our sample-aware compilation is better than standard\\nmodel-level compilation, further increasing the sampling speed.\\nChoice of ∆t.We find the choice of ∆tduring training is crucial and affects the sampling results a'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='lot in our experiments, as shown in Table 3b. We find ∆t∈(0.0,0.1]works well for more sampling\\nsteps like H12R5, while ∆t∈[0.06,0.12]is better fore fewer sampling steps like H6R2andH9R3.\\nBesides, we find ∆t∈(0.0,0.12]yields relatively good results in all the situations.\\nEffects of velocity refiner. We evaluate the effects of the different number of velocity refiners\\nin Table 3c, and find that appropriately increasing the number of velocity refiners can improve the\\ntrade-off between sampling quality and speed. Specifically, we find H6R2can achieve better image'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='quality and generation speed than the baseline H8.\\nEffects of pseudo corrector. In Table 3d, we fix the total number of both Heun’s sample block and\\npseudo corrector block and adjust the ratio of the pseudo corrector. Our results demonstrate that\\nincreasing the number of pseudo corrector blocks can significantly improve the sampling speed while\\nintroducing neglectable performance drop ( e.g., FlowTurbo with H1P6R2performs better than H8).\\nQualitative results and extensions. We provide qualitative results of high-resolution text-to-image'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='generation by applying FlowTurbo to the newly released flow-based model Lumina-Next-T2I [ 9].\\nSince Lumina-Next-T2I adopts a heavy language model Gemma-2B [ 39] to extract text features and\\ngenerates high-resolution images ( 1024×1024 ), the inference speed of it is slower than SiT [ 24].\\nIn Figure 4a, we show that our FlowTurbo can generate images with better quality and higher\\ninference speed compared with the baseline Heun’s method. Besides, since FlowTurbo remains the\\nmulti-step sampling paradigm, it can be seamlessly applied to more applications like image inpainting,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='image editing, and object removal (Section 4.4). Please also refer to the Appendix C for the detailed\\nimplementation of various tasks.\\nLimitations and broader impact. Despite the effectiveness of FlowTurbo, our velocity refiner\\nhighly relies on the observation that the velocity is a “stable value” during the sampling. However, we\\nhave not found such a stable value in diffusion-based models yet, which might limit the application.\\nBesides, the abuse of FlowTurbo may also accelerate the generation of malicious content.\\n5 Conclusion'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='5 Conclusion\\nIn this paper, we introduce FlowTurbo, a novel framework designed to accelerate flow-based genera-\\ntive models. By leveraging the stability of the velocity predictor’s outputs, we propose a lightweight\\nvelocity refiner to adjust the velocity field offsets. This refiner comprises only about 5% of the original\\nvelocity predictor’s parameters and can be efficiently trained in under 6 GPU hours. Additionally,\\nwe have proposed a pseudo corrector that reduces the number of model evaluations while maintain-'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 8}, page_content='ing the same convergence order as the second-order Heun’s method. Furthermore, we propose a\\n9'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='sample-aware compilation technique to enhance sampling speed. Extensive experiments on various\\nflow-based generative models demonstrate FlowTurbo’s effectiveness on both class-conditional image\\ngeneration and text-to-image generation. We hope our work will inspire future efforts to accelerate\\nflow-based generative models across various application scenarios.\\nAcknowledgments\\nThis work was supported in part by the National Natural Science Foundation of China under Grant\\n62321005, Grant 624B1026, Grant 62336004, and Grant 62125603.\\nReferences'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='References\\n[1]Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying\\nframework for flows and diffusions. arXiv preprint arXiv:2303.08797 , 2023.\\n[2]Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image\\ntransformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\\npages 11315–11325, 2022.\\n[3]Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo,'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k\\ntext-to-image generation. arXiv preprint arXiv:2403.04692 , 2024.\\n[4]Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James\\nKwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic\\ntext-to-image synthesis. arXiv preprint arXiv:2310.00426 , 2023.\\n[5]Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='equations. Advances in neural information processing systems , 31, 2018.\\n[6]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In CVPR , pages 248–255. IEEE, 2009.\\n[7]Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS ,\\n34:8780–8794, 2021.\\n[8]Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi,\\nDominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='image synthesis. arXiv preprint arXiv:2403.03206 , 2024.\\n[9]Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu,\\nYuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via\\nflow-based large diffusion transformers. arXiv preprint arXiv:2405.05945 , 2024.\\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS , 33:6840–\\n6851, 2020.\\n[11] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. NeurIPS , 2021.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='[12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.\\nVideo diffusion models. arXiv preprint arXiv:2204.03458 , 2022.\\n[13] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. NeurIPS ,\\n34:21696–21707, 2021.\\n[14] Black Forest Labs. Flux: A powerful tool for text generation. https://huggingface.co/\\nblack-forest-labs/FLUX.1-dev , 2024. Accessed: 2024-09-26.\\n[15] John Denholm Lambert et al. Numerical methods for ordinary differential systems , volume 146. Wiley\\nNew York, 1991.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='New York, 1991.\\n[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV , pages 740–755. Springer,\\n2014.\\n[17] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for\\ngenerative modeling. arXiv preprint arXiv:2210.02747 , 2022.\\n[18] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on\\nmanifolds. ICLR , 2022.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 9}, page_content='manifolds. ICLR , 2022.\\n[19] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer\\ndata with rectified flow. arXiv preprint arXiv:2209.03003 , 2022.\\n10'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='[20] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-\\nquality diffusion-based text-to-image generation. In The Twelfth International Conference on Learning\\nRepresentations , 2023.\\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101 , 2017.\\n[22] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode\\nsolver for diffusion probabilistic model sampling in around 10 steps. NeurIPS , 2022.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='[23] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver\\nfor guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095 , 2022.\\n[24] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie.\\nSit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv\\npreprint arXiv:2401.08740 , 2024.\\n[25] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit:'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073 ,\\n2021.\\n[26] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim\\nSalimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition , pages 14297–14306, 2023.\\n[27] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-\\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 4296–4304, 2024.\\n[28] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision , pages 4195–4205, 2023.\\n[29] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,\\nand Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv\\npreprint arXiv:2307.01952 , 2023.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='preprint arXiv:2307.01952 , 2023.\\n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\\nimage synthesis with latent diffusion models. In CVPR , pages 10684–10695, 2022.\\n[31] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. ICLR , 2022.\\n[32] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation.\\narXiv preprint arXiv:2311.17042 , 2023.\\n[33] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='ACM SIGGRAPH 2022 conference proceedings , pages 1–10, 2022.\\n[34] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush\\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400\\nmillion image-text pairs. arXiv preprint arXiv:2111.02114 , 2021.\\n[35] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning\\nusing nonequilibrium thermodynamics. In ICML , pages 2256–2265. PMLR, 2015.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='[36] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. ICLR , 2021.\\n[37] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023.\\n[38] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\\nScore-based generative modeling through stochastic differential equations. In ICLR , 2021.\\n[39] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,\\nLaurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='gemini research and technology. arXiv preprint arXiv:2403.08295 , 2024.\\n[40] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman, and\\nTaesung Park. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828 ,\\n2023.\\n[41] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\\nmodels. In ICCV , pages 3836–3847, 2023.\\n[42] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictor-corrector'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 10}, page_content='framework for fast sampling of diffusion models. NeurIPS , 2023.\\n[43] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou,\\nTianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024.\\n11'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='A Detailed Background of Diffusion and Flow-based Models\\nIn this section, we will provide a detailed background of diffusion and flow-based models, which is\\nhelpful to understand the difference and relationship between them.\\nA.1 Diffusion Models\\nTheforward pass i.e.diffusioin pass of DPMs can be defined as a sequence of variables {xt}t∈[0,1]\\nstarting with x0, such that for any t∈[0,1],x0∈RDis a D-dimensional random variable with an\\nunknown data distribution p0(x0). the distribution of xtconditioned on x0satisfies\\np0t(xt|x0) =N(xt|αtx0, σtI) (16)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='p0t(xt|x0) =N(xt|αtx0, σtI) (16)\\nwhere αt, σt∈R+are differentiable functions of twith bounded derevatives. The choice for αt\\nandσtis referred to as the noise schedule of a DPM. Let pt(xt)denote the marginal distribution\\nofxt, DPMs choose noise schedules to ensure the marginal distribution p1(x1) =N(0,I)and the\\nsignal-to-noise-ratio (SNR) α2\\nt/σ2\\ntis strictly decreasing w.r.t. t[13]. And we have\\nxt=αtx0+σtϵ, t∈[0,1],ϵ∼ N(0,I) (17)\\nMoreover, Kingma et al. [13] prove that the following stochastic differential equation (SDE) has the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='same transition distribution q0t(xt|x0)as in (16) for any t∈[0,1]:\\ndxt=f(t)xtdt+g(t)dwt, t∈[0,1],x0∼p0(x0) (18)\\nwhere wt∈RDis the standard Wiener process , and\\nf(t) =d logαt\\ndt, g2(t) =dσ2\\nt\\ndt−2d logαt\\ndtσ2\\nt (19)\\nSong et al. [ 38] have shown that the forward process in (18) has an equivalent reverse process from\\ntime1to0under some regularity conditions, starting with the marginal distribution pT(xT):\\ndxt= [f(t)xt−g2(t)∇xlogpt(xt)]dt+g(t)d¯wt,xT∼pT(xT) (20)\\nwhere ¯wt∈RDis a standard Wiener process in the reverse time. To solve the reverse process in (20),'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='the only thing we should do is to estimate the score term ∇xlogpt(xt)at each time t. In practice,\\nDPMs train a neural network ϵθ(x, t)parameterized by θto estimate the scaled score function:\\n−σt∇xlogpt(xt). The parameter θis optimized by minimizing the following objective [ 10,38,24]\\nLDM(θ) =Et,p0(x0),p(xt|x0)\\x02\\nλ(t)∥ϵθ(xt, t) +σt∇xlogpt(xt)∥2\\n2\\x03\\n(21)\\nwhere λ(t)is a time-dependent coefficient. As ϵθ(xt, t)can alse be regarded as predicting the\\nGaussian noise added to xt, it is usually called the noise prediction model . Since the ground truth of'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='ϵθ(xt, t)is−σt∇xlogpt(xt), DPMs replace the score function in (20) by−ϵθ(xt, t)/σtand we refer\\nto it as diffusion-based generative model. DPMs define a parameterized reverse process (diffusion\\nSDE) from time 1to0, starting with x1∼p1(x1):\\ndxt=\\x14\\nf(t)xt+g2(t)\\nσtϵθ(xt, t)\\x15\\ndt+g(t)d¯wt, x 1∼ N(0,I) (22)\\nSamples can be generated from DPMs by solving the diffusion SDE in (22) with numerical solvers.\\nWhen discretizing SDEs, the step size is limited by the randomness of the Wiener process. A large\\nstep size (small number of steps) often causes non-convergence, especially in high dimensional'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 11}, page_content='spaces. For faster sampling, we can consider the associated probability flow ODE [ 38] which has the\\nsame marginla distribution at each time tas that of the SDE. Specifically, for DPMs, Song et al. [38]\\nproved that the probability flow ODE of (22) is\\ndxt\\ndt=v(xt, t) :=f(t)xt+g2(t)\\n2σtϵθ(xt, t),x1∼ N(0,I) (23)\\nSamples can be generated by solving the ODE from 1to0. Comparing with SDEs, ODEs can be\\nsolved with larger step sizes as they have no randomness. Furthermore, we can take advantage of\\nefficient numerical ODE solvers to accelerate the sampling.\\n12'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 12}, page_content='A.2 Flow-based Models\\nTo introduce flow in detail, first we construct a time-dependent vector field, u: [0,1]×RD→RD.\\nA vector field utcan be used to construct a time-dependent diffeomorphic map, called a flow,\\nϕ: [0,1]×RD→RD,defined via the ordinary differential equation (ODE):\\nd\\ndtϕt(x0) =ut(ϕt(x0)) (24)\\nϕ0(x0) =x0 (25)\\nChen et al. [5] suggested modeling the vector field utwith a neural network vθ, which in turn leads\\nto a deep parametric model of the flow ϕt, called a Continuous Normalizing Flow (CNF). It is a'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 12}, page_content='more generic modeling technique and can capture the probability paths of diffusion process as well.\\nTraining a CNF becomes more practical since the propose of the conditional flow matching (CFM)\\ntechnique [17], which learns the conditional velocity field of the flow.\\nFor generative models, similar to (17) we can add some constraints to the noise schedule such that\\nα0= 1, σ0= 0andα1= 0, σ1= 1, and then define the flow as:\\nψt(·|ϵ) :x07→αtx0+σtϵ (26)\\nThe corresponding velocity vector field which is used to construct the flow ψtcan be represented as:\\nut(ψt(x0|ϵ)|ϵ) =d'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 12}, page_content='ut(ψt(x0|ϵ)|ϵ) =d\\ndtψt(x0|ϵ) = ˙αtx0+ ˙σtϵ (27)\\nConsider the time-dependent probability density function (PDF) pt(x)ofxt=ψt(x0|ϵ) =αtx0+\\nσtϵ. Lipman et al. [17] proved that the marginal vector field utthat generates the probability path pt\\nsatisfies a Partial Differential Equation (PDE) called continuity equation (also transport equation )\\nd\\ndtpt(x) +∇x·(ut(x)pt(x)) = 0 (28)\\nUsing conditional flow matching technique v(xt, t)in(23) can be estimated parametrically as\\nvθ(xt, t)by minimizing the following objective\\nLFM(θ) =Et,p1(ϵ),p0(x0)\\r\\r\\r\\rvθ(xt, t)−d\\ndtψt(x0|ϵ)\\r\\r\\r\\r2\\n2(29)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 12}, page_content='dtψt(x0|ϵ)\\r\\r\\r\\r2\\n2(29)\\n=Et,p1(ϵ),p0(x0)∥vθ(xt, t)−˙αtx0−˙σtϵ∥2\\n2(30)\\nWe refer to (23) as aflow-based generative model. Since we have xt=ψt(x0|ϵ), the sampling of a\\nflow-based model can be achieved by solving the probability flow ODE with learned velocity\\ndxt\\ndt=vθ(xt, t), x 1∼p1(x1) (31)\\nA.3 Relationship Between Diffusion and Flow-based Models\\nThere exists a straightforward connection between v(xt, t)and the score term σt∇xlogpt(xt)\\naccording to [24].\\nv(xt, t) =˙αt\\nαtxt+\\x12\\n˙σt−˙αtσt\\nαt\\x13\\n(−σt∇xlogpt(xt)) (32)\\n13'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 13}, page_content='Algorithm 1 Heun’s Method Sampler\\nRequire: timesteps {ti}N−1\\ni=0,αt, σt,x0∼ N(0,I), velocity prediction model vθ(x, t|y)\\nfori= 0toN−1do\\n∆ti←ti+1−ti\\ndi←vθ(xi, ti|y)\\n˜xti+1←xi+ ∆tidi\\ndi+1←vθ(˜xti+1, ti+1|y)\\nxti+1←xi+∆ti\\n2[di+di+1]\\nend for\\nreturn: xN\\nAlgorithm 2 Pseudo Corrector Sampler\\nRequire: timesteps {ti}N−1\\ni=0,αt, σt,x0∼ N(0,I), velocity prediction model vθ(x, t|y)\\n∆t←t1−t0\\nfori= 0toN−1do\\n∆ti←ti+1−ti\\nifi= 0then\\ndi←vθ(xi, ti|y)\\nend if\\n˜xti+1←xi+ ∆tidi\\ndi+1←vθ(˜xti+1, ti+1|y)\\nxti+1←xi+∆ti\\n2[di+di+1]\\nend for\\nreturn: xN\\nWe can define ζt= ˙σt−˙αtσt'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 13}, page_content='2[di+di+1]\\nend for\\nreturn: xN\\nWe can define ζt= ˙σt−˙αtσt\\nαt, and we have ϵθ(xt, t)to estimate −σt∇xlogpt(xt), then derive the\\nrelationship between LDM(θ)andLFM(θ)We can plug (32) into the loss LFM(θ)in Equation (30)\\nLFM(θ) =Et,p1(ϵ),p0(x0)∥vθ(xt, t)−˙αtx0−˙σtϵ∥2\\n2(33)\\n=Et,p1(ϵ),p0(x0)\\r\\r\\r\\r˙αt\\nαtxt+ζtϵθ(xt, t)−˙αtx0−˙σtϵ\\r\\r\\r\\r2\\n2(34)\\n=Et,p1(ϵ),p0(x0)\\r\\r\\r\\r˙αtσt\\nαtϵ+ζtϵθ(xt, t)−˙αtx0−˙σtϵ\\r\\r\\r\\r2\\n2(35)\\n=Et,p1(ϵ),p0(x0)∥ζtϵθ(xt, t)−ζtϵ∥2\\n2(36)\\n=Et,p1(ϵ),p0(x0)h\\nζ2\\nt∥ϵθ(xt, t)−ϵ∥2\\n2i\\n(37)\\nxt=αtx0+σtϵ= Et,p0(x0),p(xt|x0)h\\nζ2\\nt∥ϵθ(xt, t) +σt∇xlogpt(xt)∥2\\n2i\\n(38)\\nRecall that\\nLDM(θ) =Et,p0(x0),p(xt|x0)h'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 13}, page_content='2i\\n(38)\\nRecall that\\nLDM(θ) =Et,p0(x0),p(xt|x0)h\\nλ(t)∥ϵθ(xt, t) +σt∇xlogpt(xt)∥2\\n2i\\n, (39)\\nwe can see that the difference of LDM(θ)andLFM(θ)during training is caused by the weighted\\nfunction, which leaving to different trajectories and properties.\\nB Proof of Convergence of Pseudo Corrector\\nIn this section, we will prove that the proposed pseudo corrector has the same local truncation error\\nand global convergence order as Heun’s method. The detailed sampling procedure of Heun’s method\\n14'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='and pseudo corrector are provided in Algorithm 1 and Algorithm 2. In this section, we use xti\\nto represent the intermediate sampling result at the titimestep, and use x∗\\nti=x(ti)to denote the\\ncorresponding ground-truth value on the trajectory. In all the proofs in this section, we omit the\\ncondition yfor simplicity.\\nB.1 Assumptions\\nAssumption B.1. The velocity predictor vθ(x, t)is Lipschitz continous of constant Lw.r.tx.\\nAssumption B.2. The velocity predictor vθ(x, t)has at least 2 derivativesd\\ndtvθ(x, t)andd2\\ndt2vθ(x, t)\\nand the derivatives are continuous.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='dt2vθ(x, t)\\nand the derivatives are continuous.\\nAssumption B.3. h= max 0≤i≤N−1hi=O(1/N), where Nis the total number of sampling steps.\\nAll the above are common in the analysis of the convergence order of fast samplers [ 22,23,42] of\\ndiffusion models.\\nB.2 Local Convergence\\nWe start by studying the local convergence and Heun’s method. Considering the updating from tito\\nti+1and assume all previous results are correct (see the definition of local convergence [ 15]). The\\nTaylor’s expansion of x∗\\nti+1attigives:\\nx∗\\nti+1=xti+hix(1)(ti) +h2\\ni\\n2x(2)(ti) +h3\\ni\\n6x(3)(ti) +O(h4). (40)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='i\\n2x(2)(ti) +h3\\ni\\n6x(3)(ti) +O(h4). (40)\\nOn the other hand, let ¯xti+1be the prediction assuming xiis correct, the updating rule of Heun’s\\nmethod shows:\\n¯xti+1=xti+hi\\n2[di+di+1] (41)\\n=xti+hi\\n2[x(1)(ti) +x(1)(ti) +hix(2)(ti) +h2\\ni\\n2x(3)(ti) +O(h3)] (42)\\n=xi+hix(1)(ti) +h2\\ni\\n2x(2)(ti) +h3\\ni\\n4x(3)(ti) +O(h4) (43)\\nTherefore, the local truncation error can be computed by:\\nTi+1=∥x∗\\nti+1−¯xti+1∥=∥ −h3\\ni\\n12x(3)(ti) +O(h4)∥ ≤C1h3, (44)\\nwhich indicates that Heun’s method has 2 order of accuracy.\\nIt is also noted that the local truncation error of the predictor step (which is the same as Euler’s'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='method) can be similarly derived by:\\n˜Ti+1=∥x∗\\nti+1−˜xti+1∥=∥h2\\n2x(2)(ti) +O(h2)∥ ≤C2h2. (45)\\nFor pseudo corrector, the analysis of local convergence is the same since we need to assume all\\nprevious results (including the xtianddi), which means the local truncation error of pseudo corrector\\nis the same as the Heun’s method.\\nB.3 Global Convergence\\nGlobal convergence for Heun’s method. When analyzing global convergence, we need to take\\ninto account both the local truncation error and the effects of the error of previous results. According\\nto the Lipschitz condition, we have:\\n∥x∗'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 14}, page_content='to the Lipschitz condition, we have:\\n∥x∗\\nti+1−˜xti+1∥ ≤(1 +hL)∥x∗\\nti−xti∥+C2h2(46)\\n15'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 15}, page_content='and\\n∥x∗\\nti+1−xti+1∥ ≤(1 +hL\\n2)∥x∗\\nti−xti∥+hL\\n2∥x∗\\nti+1−˜xti+1∥+C1h3. (47)\\nCombining the above two inequalities together, we have\\n∥x∗\\nti+1−xti+1∥ ≤(1 +hL+h2L2\\n2)∥x∗\\nti−xti∥+C3h3, (48)\\nwhere C3=hLC 2\\n2+C1. Note that ∥x∗\\nt0−xt0∥= 0 (their is no error at the beginning of the\\nsampling), it can be easily derived that\\n∥x∗\\ntN−xtN∥ ≤C3h2\\nL+hL2\\n2((1 + hL+h2L2\\n2)N−1)≤C4h2(eC5−1) = C6h2. (49)\\nTherefore, we have proven that Heun’s method have 2 order of global convergence.\\nGlobal convergence for pseudo corrector. The only difference between pseudo corrector and'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 15}, page_content='Heun’s method is how diis obtained. Pseudo corrector reuse the difrom the last sampling step\\nrather than re-compute it as in Heun’s method. As a result, diused in pseudo corrector is computed\\non˜xtirather than xti, which will lead to another error term when analyzing the global convergence.\\nConcretely, the global error of pseudo corrector can be computed by:\\n∥x∗\\nti+1−˜xti+1∥ ≤(1 +hL)∥x∗\\nti−xti∥+hL∥˜xti−xti∥+C2h2(50)\\n∥x∗\\nti+1−xti+1∥ ≤(1 +hL\\n2)∥x∗\\nti−˜xti∥+hL\\n2∥x∗\\nti+1−˜xti+1∥+hL\\n2∥xti−˜xti∥+C1h3.(51)\\nFor the sake of simplicity, let ˜∆i=∥x∗\\nti−˜xti∥and∆i=∥x∗'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 15}, page_content='For the sake of simplicity, let ˜∆i=∥x∗\\nti−˜xti∥and∆i=∥x∗\\nti−xti∥. Therefore, the above formulas\\nbecomes:\\n˜∆i+1≤∆i+hL˜∆i+C2h2(52)\\n∆i+1≤(1 +hL\\n2)∆i+hL\\n2(1 +hL\\n2)˜∆i+C4h3. (53)\\nBy calculating (52) ×hL+(53) we have:\\n∆i+1+hL˜∆i+1≤(1 +hL\\n2)∆i+hL\\n2(1 +hL\\n2)˜∆i+C4h3+hL∆i+h2L2˜∆i+C2Lh3\\n= (1 +3\\n2hL)\"\\n∆i+hL\\n2+5\\n4h2L2\\n1 +3\\n2hL˜∆i#\\n+C7h3\\n≤(1 +3\\n2hL)(∆i+hL˜∆i) +C7h3.(54)\\nNote that ∆0+hL˜∆0= 0. Let∆′\\ni= ∆ i+hL˜∆i, we have\\n∆′\\ni≤(1 +3\\n2hL)∆′\\ni+C7h3. (55)\\nSimilar to the derivation of (49), we can derive that\\n∆′\\ni≤C8h2((1 +3\\n2hL)N−1)≤C9h2, (56)\\nwhich indicates that\\n∆N≤C9h2, hL ˜∆i+1≤C9h2. (57)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 15}, page_content='which indicates that\\n∆N≤C9h2, hL ˜∆i+1≤C9h2. (57)\\nTherefore we have ∆N≤C9h2, and thus the global convergence of pseudo corrector is 2-order.\\n16'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='Table 4: Ablation of the number of the velocity refiners. We change the number of velocity refiners and\\ncompare the sampling quality of each configuration. We find there exists a optimal number of velocity refiners\\nto achieve the lowest FID.\\nMethod Sample Config Ratio of Refiner FID ↓ Latency (ms / img)\\nSiT-XL [8], ImageNet (256×256)\\nFlowTurbo H7P10R6 0.26 2.19 100.7\\nFlowTurbo H7P10R5 0.23 2.12 100.3\\nFlowTurbo H7P10R4 0.19 2.18 99.9\\nFlowTurbo H7P10R3 0.15 2.15 99.6\\nFlowTurbo H5P10R6 0.29 2.25 87.2\\nFlowTurbo H5P10R5 0.25 2.20 86.8\\nFlowTurbo H5P10R4 0.21 2.21 86.4'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='FlowTurbo H5P10R4 0.21 2.21 86.4\\nFlowTurbo H5P10R3 0.17 2.22 86.0\\nC Implementation Details\\nClass-conditional image generation. We use the SiT-XL-2[ 24] as our base model to perform the\\nexperiments on class-conditional image generation. We use a single block of SiT-XL-2 as the Velocity\\nRefiner. We double the input channel from 4 to 8 to take the previous velocity as input. The resulting\\nvelocity refiner only contains 29M parameters, about 4.3% of the original SiT-XL-2(675M). We use\\nImageNet-1K [ 6]2to train our velocity model. We used AdamW [ 21] optimizer for all models. We'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='use a constant learning rate of 5×10−5and a batch size of 18 on a single A800 GPU. We used a\\nrandom horizontal flip with a probability of 0.5 in data augmentation. We did not tune the learning\\nrates, decay/warm-up schedules, AdamW parameters, or use any extra data augmentation during\\ntraining. Our velocity refiner (for SiT-XL-2) trains at approximately 4.44 steps/sec on an A800 GPU,\\nand converges in 30,000 steps, which takes about 2 hours.\\nText-to-image generation. We use the 2-RF in InstaFlow [ 20] as our base model to perform the'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='experiments on text-to-image generation. Since the architecture of the original velocity predictor\\nin [20] is a U-Net [ 30], we cannot directly use a single block of it as the velocity refiner as we do\\nfor SiT [ 24]. Instead, we simply reduce the number of channels in each block from [320, 640, 1280,\\n1280] to [160, 160, 320, 320] and reduce the number of layers in each block from 2 to 1. We also\\ndouble the input channel from 4 to 8 to take the previous velocity as input. The resulting velocity\\nrefiner only contains 43.5M parameters, about 5% of the original U-Net (860M). We use a subset'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='of LAION [ 34]3containing only 50K images to train our velocity model. We use AdamW [ 21]\\noptimizer with a learning rate of 2e-5 and weight decay of 0.0. We adopt a batch size of 16 and set\\nthe warming-up steps as 100. We also use a gradient clipping of 0.01 to stabilize training. We train\\nour model on a single A800 GPU for 10K iterations, which takes about 5.5 hours.\\nImplementation of extension tasks. We have demonstrated our FlowTurbo is also suitable for\\nextension tasks due to the multi-step nature of our framework in Section 4.4. For image inpainting, we'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='adopt the inpainting pipeline in diffusion models4, where we merge the noise latent and the generated\\nlatent at a specific timestep by the input mask. For object removal, we first use a Grounded-SAM5\\nto generate the mask and perform similar image inpainting pipeline. For image editing, we adopt\\nthe SDEdit [ 25] which first adds noise to the original image and use it as an intermediate result to\\ncontinue the sampling.\\nD More Analysis\\nIn this section, we provide more analysis through both quantitative results and qualitative results.\\n2License: Custom (research, non-commercial)'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 16}, page_content='2License: Custom (research, non-commercial)\\n3License: Creative Common CC-BY 4.0\\n4https://huggingface.co/docs/diffusers/en/using-diffusers/inpaint\\n5https://github.com/IDEA-Research/Grounded-Segment-Anything\\n17'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='Table 5: Comparisons with state-of-the-art methods on text-to-image generation. We compare\\nour FlowTurbo with state-of-the-art diffusion models (15 steps DPM-Solver++ [ 23]) and show our\\nFlowTurbo enjoys favorable trade-offs between sampling quality and speed.\\nMethod Sample Config FLOPs (G) Latency (ms / img) FID ↓\\nSD2.1 [30] 15 steps DPM++ [23] 11427 286.0 33.03\\nSDXL [29] 15 steps DPM++ [23] 24266 427.2 29.46\\nPixArt- α[4] 15 steps DPM++ [23] 17523 366.8 37.96\\nPixArt- σ[3] 15 steps DPM++ [23] 17957 365.9 33.62\\nFlowTurbo H1P6R3 4030 104.8 28.60\\nFlowTurbo H3P6R3 5386 137.0 27.60'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='FlowTurbo H3P6R3 5386 137.0 27.60\\nD.1 More Quantitative Results\\nAblation of the number of the velocity refiners. In Table 4, we investigate how to choose the\\nnumber of velocity refiners to get a better sampling quality. We adopt two basic configurations of\\nH7R10andH5R10, and vary the number of velocity refiners from 3 to 6. We find that the FID will\\nfirst decrease and then increase when NRbecomes larger, and there exists an optimal NR= 5where\\nwe reach the lowest FID. These results indicate that we can always tune this hyper-parameter to\\nexpect a better result.'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='expect a better result.\\nMore comparisons on text-to-image generation. In Table Table 5, we compare the sampling\\nquality and speed of FLowTurbo with state-of-the-art diffusion models on text-to-image generation.\\nFor all the diffusion models, we adopt a 15-step DPM-Solver++ [ 23] as the default sampler. The\\nFLOPs reported also take the multi-step sampling into account. Our results show that our FlowTurbo\\ncan achieve the lowest FID and inference latency.\\nD.2 More Qualitative Results\\nTo better illustrate the sampling quality of our FlowTurbo, we provide more qualitative results on'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='both class-conditional image generation and text-to-image generation.\\nClass-conditional image generation. We use SiT-XL [ 24] as our flow-based model for class-\\nconditional image generation. In Figure 5, we provide random samples from FlowTurbo of the\\nsample config H8P9R5, which inference at 100 ms/img. We also demonstrate the sampling quality\\ntrade-offs in Figure 6, we compare the sampling quality of two different configurations H1P5R3(38\\nms / img) and H8P9R5(100 ms / img). We generate the images from the same initial noise for better'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='comparisons. Our result demonstrates that our FlowTurbo can achieve real-time image generation,\\nand the sampling quality can be further improved with more computational budgets.\\nText-to-image generation. We adopt Lumina-Next-T2I [ 9] to achieve text-to-image generation.\\nWe compare the sampling quality and speed of Heun’s method and our FlowTurbo in Figure 7. We\\nfind that FlowTurbo can consistently generate images with better quality and more visual details,\\nwhile requiring less inference time.\\nE Code\\nOur code is implemented in PyTorch6. We use the codebase of [ 24] to conduct experiments. The'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 17}, page_content='code is available at https://github.com/shiml20/FlowTurbo .\\n6https://pytorch.org\\n18'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 18}, page_content='Figure 5: Random samples from FlowTurbo on ImageNet 256 ×256. We use a classifier-free\\nguidance scale of 4.0 and the sample config of H8P9R5(100 ms / img )\\n19'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 19}, page_content='(a) Sample Config H1P5R3(38 ms / img )\\n (b) Sample Config H8P9R5(100 ms / img )\\nFigure 6: Uncurated 256 ×256 samples from FlowTurbo (CFG = 4.0). For better visualization. We\\ncompare two sample configurations ( H1P5R3andH8P9R5). The same initial noise is used for both\\nsample configurations for better comparisons.\\n20'),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 20}, page_content=\"A beautiful panorama from inside a camper with beautiful beach and cliffs\\n The moon, a silver boat, sails through the sea of stars\\n A single brown bird perched on a mossy branch\\n Digital watercolor of a summer scape sunset, with flowery pastel colors\\nA legendary fruit castle in a fruit kingdom\\nInside the glass sphere, Pirate Ship in a storm with waves in the dark\\nA beautiful girl flying through a paradox with piercing eyes \\nUnderwater landscape with colorful mechanical parts, cable, wires\\nA cute kitten wearing a witch's robe and hat, holding a magic book\"),\n",
       " Document(metadata={'source': 'C:\\\\QpiAi\\\\paper_2409.18128.pdf', 'page': 20}, page_content='A tree house on a beautiful beach surrounded by mountains and waterfalls\\n A sailboat in the ocean with a moon in the sky\\nA rough linen knightess, wielding dual axes resembling two moonsFigure 7: More visual comparisons between Heun’s method (2.6 s / img, left) and our FlowTurbo\\n(1.8 s / img, right ).\\n21')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_documents(document)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
